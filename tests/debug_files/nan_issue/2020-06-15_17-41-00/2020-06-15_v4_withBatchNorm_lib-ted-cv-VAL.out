nohup: ignoring input
/home/dzubke/miniconda3/envs/awni_env36/lib/python3.6/site-packages/torch/serialization.py:425: SourceChangeWarning: source code of class 'speech.models.ctc_model_train.CTC_train' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.
  warnings.warn(msg, SourceChangeWarning)
  0%|          | 0/96139 [00:00<?, ?it/s]  0%|          | 0/96139 [00:05<?, ?it/s, avg_loss=3.65, data_time=4.06, grad_norm=6.24e+3, iter=0, loss=365, model_time=1.41]  0%|          | 1/96139 [00:05<146:14:20,  5.48s/it, avg_loss=3.65, data_time=4.06, grad_norm=6.24e+3, iter=0, loss=365, model_time=1.41]  0%|          | 1/96139 [00:09<146:14:20,  5.48s/it, avg_loss=4.2, data_time=7.87, grad_norm=99.6, iter=1, loss=58.6, model_time=2.07]     0%|          | 2/96139 [00:09<138:09:55,  5.17s/it, avg_loss=4.2, data_time=7.87, grad_norm=99.6, iter=1, loss=58.6, model_time=2.07]  0%|          | 2/96139 [00:14<138:09:55,  5.17s/it, avg_loss=5.4, data_time=11.8, grad_norm=261, iter=2, loss=124, model_time=2.79]    0%|          | 3/96139 [00:14<134:09:01,  5.02s/it, avg_loss=5.4, data_time=11.8, grad_norm=261, iter=2, loss=124, model_time=2.79]  0%|          | 3/96139 [00:20<134:09:01,  5.02s/it, avg_loss=7.78, data_time=16.6, grad_norm=1.17e+3, iter=3, loss=243, model_time=3.91]  0%|          | 4/96139 [00:20<140:54:44,  5.28s/it, avg_loss=7.78, data_time=16.6, grad_norm=1.17e+3, iter=3, loss=243, model_time=3.91]  0%|          | 4/96139 [00:28<140:54:44,  5.28s/it, avg_loss=15.4, data_time=22.6, grad_norm=1.09e+3, iter=4, loss=769, model_time=6.16]  0%|          | 5/96139 [00:28<164:41:44,  6.17s/it, avg_loss=15.4, data_time=22.6, grad_norm=1.09e+3, iter=4, loss=769, model_time=6.16]  0%|          | 5/96139 [00:37<164:41:44,  6.17s/it, avg_loss=21.7, data_time=29.1, grad_norm=7.64e+3, iter=5, loss=648, model_time=8.2]   0%|          | 6/96139 [00:37<184:16:02,  6.90s/it, avg_loss=21.7, data_time=29.1, grad_norm=7.64e+3, iter=5, loss=648, model_time=8.2]  0%|          | 6/96139 [00:44<184:16:02,  6.90s/it, avg_loss=27.9, data_time=34.3, grad_norm=879, iter=6, loss=636, model_time=10.3]     0%|          | 7/96139 [00:44<187:10:51,  7.01s/it, avg_loss=27.9, data_time=34.3, grad_norm=879, iter=6, loss=636, model_time=10.3]  0%|          | 7/96139 [00:50<187:10:51,  7.01s/it, avg_loss=29.4, data_time=38.9, grad_norm=671, iter=7, loss=177, model_time=11.1]  0%|          | 8/96139 [00:50<174:33:35,  6.54s/it, avg_loss=29.4, data_time=38.9, grad_norm=671, iter=7, loss=177, model_time=11.1]  0%|          | 8/96139 [00:55<174:33:35,  6.54s/it, avg_loss=33.8, data_time=42.9, grad_norm=403, iter=8, loss=474, model_time=12.5]  0%|          | 9/96139 [00:55<165:15:52,  6.19s/it, avg_loss=33.8, data_time=42.9, grad_norm=403, iter=8, loss=474, model_time=12.5]  0%|          | 9/96139 [01:01<165:15:52,  6.19s/it, avg_loss=37.8, data_time=47.8, grad_norm=530, iter=9, loss=434, model_time=14]    0%|          | 10/96139 [01:01<166:52:44,  6.25s/it, avg_loss=37.8, data_time=47.8, grad_norm=530, iter=9, loss=434, model_time=14]  0%|          | 10/96139 [01:07<166:52:44,  6.25s/it, avg_loss=40.9, data_time=52.6, grad_norm=984, iter=10, loss=350, model_time=15.3]  0%|          | 11/96139 [01:07<165:55:00,  6.21s/it, avg_loss=40.9, data_time=52.6, grad_norm=984, iter=10, loss=350, model_time=15.3]  0%|          | 11/96139 [01:12<165:55:00,  6.21s/it, avg_loss=41.4, data_time=56.2, grad_norm=89.1, iter=11, loss=90.5, model_time=16]  0%|          | 12/96139 [01:12<150:36:57,  5.64s/it, avg_loss=41.4, data_time=56.2, grad_norm=89.1, iter=11, loss=90.5, model_time=16]  0%|          | 12/96139 [01:19<150:36:57,  5.64s/it, avg_loss=47.8, data_time=61.8, grad_norm=3.86e+6, iter=12, loss=679, model_time=18.1]  0%|          | 13/96139 [01:19<167:14:51,  6.26s/it, avg_loss=47.8, data_time=61.8, grad_norm=3.86e+6, iter=12, loss=679, model_time=18.1]  0%|          | 13/96139 [01:25<167:14:51,  6.26s/it, avg_loss=49.9, data_time=66.5, grad_norm=349, iter=13, loss=260, model_time=19]        0%|          | 14/96139 [01:25<162:10:22,  6.07s/it, avg_loss=49.9, data_time=66.5, grad_norm=349, iter=13, loss=260, model_time=19]  0%|          | 14/96139 [01:31<162:10:22,  6.07s/it, avg_loss=51.7, data_time=71.7, grad_norm=1.85e+3, iter=14, loss=224, model_time=20]  0%|          | 15/96139 [01:31<163:00:41,  6.11s/it, avg_loss=51.7, data_time=71.7, grad_norm=1.85e+3, iter=14, loss=224, model_time=20]  0%|          | 15/96139 [01:35<163:00:41,  6.11s/it, avg_loss=52.7, data_time=75.1, grad_norm=1.35e+3, iter=15, loss=159, model_time=20.8]  0%|          | 16/96139 [01:35<147:32:26,  5.53s/it, avg_loss=52.7, data_time=75.1, grad_norm=1.35e+3, iter=15, loss=159, model_time=20.8]  0%|          | 16/96139 [01:44<147:32:26,  5.53s/it, avg_loss=57.6, data_time=81.4, grad_norm=2.47e+3, iter=16, loss=541, model_time=22.7]  0%|          | 17/96139 [01:44<168:41:28,  6.32s/it, avg_loss=57.6, data_time=81.4, grad_norm=2.47e+3, iter=16, loss=541, model_time=22.7]  0%|          | 17/96139 [01:52<168:41:28,  6.32s/it, avg_loss=61.2, data_time=88, grad_norm=399, iter=17, loss=413, model_time=24.1]        0%|          | 18/96139 [01:52<182:11:08,  6.82s/it, avg_loss=61.2, data_time=88, grad_norm=399, iter=17, loss=413, model_time=24.1]  0%|          | 18/96139 [01:58<182:11:08,  6.82s/it, avg_loss=64, data_time=93.1, grad_norm=434, iter=18, loss=343, model_time=25.4]  0%|          | 19/96139 [01:58<178:48:20,  6.70s/it, avg_loss=64, data_time=93.1, grad_norm=434, iter=18, loss=343, model_time=25.4]  0%|          | 19/96139 [02:06<178:48:20,  6.70s/it, avg_loss=68.9, data_time=99.4, grad_norm=1.19e+3, iter=19, loss=553, model_time=27.4]  0%|          | 20/96139 [02:06<191:05:32,  7.16s/it, avg_loss=68.9, data_time=99.4, grad_norm=1.19e+3, iter=19, loss=553, model_time=27.4]/home/dzubke/awni_speech/speech/speech/utils/model_debug.py:140: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).
  fig, ax1 = plt.subplots()
  0%|          | 20/96139 [02:14<191:05:32,  7.16s/it, avg_loss=73.6, data_time=105, grad_norm=1.05e+3, iter=20, loss=538, model_time=29.6]   0%|          | 21/96139 [02:14<198:41:14,  7.44s/it, avg_loss=73.6, data_time=105, grad_norm=1.05e+3, iter=20, loss=538, model_time=29.6]  0%|          | 21/96139 [02:22<198:41:14,  7.44s/it, avg_loss=76.6, data_time=111, grad_norm=1.08e+3, iter=21, loss=378, model_time=31.3]  0%|          | 22/96139 [02:22<199:02:15,  7.45s/it, avg_loss=76.6, data_time=111, grad_norm=1.08e+3, iter=21, loss=378, model_time=31.3]  0%|          | 22/96139 [02:31<199:02:15,  7.45s/it, avg_loss=81, data_time=118, grad_norm=5.31e+3, iter=22, loss=517, model_time=33.4]    0%|          | 23/96139 [02:31<209:09:22,  7.83s/it, avg_loss=81, data_time=118, grad_norm=5.31e+3, iter=22, loss=517, model_time=33.4]  0%|          | 23/96139 [02:36<209:09:22,  7.83s/it, avg_loss=81.4, data_time=122, grad_norm=459, iter=23, loss=116, model_time=34.4]    0%|          | 24/96139 [02:36<191:43:29,  7.18s/it, avg_loss=81.4, data_time=122, grad_norm=459, iter=23, loss=116, model_time=34.4]  0%|          | 24/96139 [02:41<191:43:29,  7.18s/it, avg_loss=81.5, data_time=126, grad_norm=325, iter=24, loss=93.7, model_time=35.2]  0%|          | 25/96139 [02:41<171:30:00,  6.42s/it, avg_loss=81.5, data_time=126, grad_norm=325, iter=24, loss=93.7, model_time=35.2]  0%|          | 25/96139 [02:47<171:30:00,  6.42s/it, avg_loss=83.4, data_time=131, grad_norm=264, iter=25, loss=275, model_time=36.5]   0%|          | 26/96139 [02:47<167:35:04,  6.28s/it, avg_loss=83.4, data_time=131, grad_norm=264, iter=25, loss=275, model_time=36.5]  0%|          | 26/96139 [02:53<167:35:04,  6.28s/it, avg_loss=86.4, data_time=136, grad_norm=873, iter=26, loss=381, model_time=37.8]  0%|          | 27/96139 [02:53<170:15:08,  6.38s/it, avg_loss=86.4, data_time=136, grad_norm=873, iter=26, loss=381, model_time=37.8]  0%|          | 27/96139 [03:03<170:15:08,  6.38s/it, avg_loss=90.9, data_time=143, grad_norm=2.81e+5, iter=27, loss=537, model_time=40.1]  0%|          | 28/96139 [03:03<194:21:19,  7.28s/it, avg_loss=90.9, data_time=143, grad_norm=2.81e+5, iter=27, loss=537, model_time=40.1]  0%|          | 28/96139 [03:10<194:21:19,  7.28s/it, avg_loss=95.4, data_time=149, grad_norm=1.74e+3, iter=28, loss=544, model_time=41.8]  0%|          | 29/96139 [03:10<193:09:55,  7.24s/it, avg_loss=95.4, data_time=149, grad_norm=1.74e+3, iter=28, loss=544, model_time=41.8]  0%|          | 29/96139 [03:17<193:09:55,  7.24s/it, avg_loss=100, data_time=154, grad_norm=2.11e+3, iter=29, loss=579, model_time=43.5]   0%|          | 30/96139 [03:17<191:41:43,  7.18s/it, avg_loss=100, data_time=154, grad_norm=2.11e+3, iter=29, loss=579, model_time=43.5]  0%|          | 30/96139 [03:22<191:41:43,  7.18s/it, avg_loss=101, data_time=158, grad_norm=1e+3, iter=30, loss=139, model_time=44.3]     0%|          | 31/96139 [03:22<170:26:16,  6.38s/it, avg_loss=101, data_time=158, grad_norm=1e+3, iter=30, loss=139, model_time=44.3]  0%|          | 31/96139 [03:29<170:26:16,  6.38s/it, avg_loss=104, data_time=164, grad_norm=632, iter=31, loss=467, model_time=46.2]   0%|          | 32/96139 [03:29<182:11:29,  6.82s/it, avg_loss=104, data_time=164, grad_norm=632, iter=31, loss=467, model_time=46.2]  0%|          | 32/96139 [03:36<182:11:29,  6.82s/it, avg_loss=106, data_time=169, grad_norm=2.41e+3, iter=32, loss=258, model_time=47.3]  0%|          | 33/96139 [03:36<177:09:04,  6.64s/it, avg_loss=106, data_time=169, grad_norm=2.41e+3, iter=32, loss=258, model_time=47.3]  0%|          | 33/96139 [03:45<177:09:04,  6.64s/it, avg_loss=110, data_time=176, grad_norm=1.4e+5, iter=33, loss=486, model_time=49.5]   0%|          | 34/96139 [03:45<196:30:06,  7.36s/it, avg_loss=110, data_time=176, grad_norm=1.4e+5, iter=33, loss=486, model_time=49.5]  0%|          | 34/96139 [03:53<196:30:06,  7.36s/it, avg_loss=110, data_time=181, grad_norm=748, iter=34, loss=190, model_time=53.4]     0%|          | 35/96139 [03:53<208:33:50,  7.81s/it, avg_loss=110, data_time=181, grad_norm=748, iter=34, loss=190, model_time=53.4]  0%|          | 35/96139 [03:59<208:33:50,  7.81s/it, avg_loss=111, data_time=185, grad_norm=8.52e+5, iter=35, loss=203, model_time=54.5]  0%|          | 36/96139 [03:59<193:10:03,  7.24s/it, avg_loss=111, data_time=185, grad_norm=8.52e+5, iter=35, loss=203, model_time=54.5]  0%|          | 36/96139 [04:05<193:10:03,  7.24s/it, avg_loss=114, data_time=190, grad_norm=9.32e+3, iter=36, loss=358, model_time=55.9]  0%|          | 37/96139 [04:05<179:49:10,  6.74s/it, avg_loss=114, data_time=190, grad_norm=9.32e+3, iter=36, loss=358, model_time=55.9]  0%|          | 37/96139 [04:10<179:49:10,  6.74s/it, avg_loss=114, data_time=194, grad_norm=1.49e+4, iter=37, loss=115, model_time=56.7]  0%|          | 38/96139 [04:10<168:07:58,  6.30s/it, avg_loss=114, data_time=194, grad_norm=1.49e+4, iter=37, loss=115, model_time=56.7]  0%|          | 38/96139 [04:18<168:07:58,  6.30s/it, avg_loss=118, data_time=200, grad_norm=4.26e+3, iter=38, loss=531, model_time=58.8]  0%|          | 39/96139 [04:18<180:36:43,  6.77s/it, avg_loss=118, data_time=200, grad_norm=4.26e+3, iter=38, loss=531, model_time=58.8]  0%|          | 39/96139 [04:23<180:36:43,  6.77s/it, avg_loss=118, data_time=204, grad_norm=5.32e+3, iter=39, loss=109, model_time=59.7]  0%|          | 40/96139 [04:23<169:34:14,  6.35s/it, avg_loss=118, data_time=204, grad_norm=5.32e+3, iter=39, loss=109, model_time=59.7]  0%|          | 40/96139 [04:32<169:34:14,  6.35s/it, avg_loss=122, data_time=210, grad_norm=1.43e+3, iter=40, loss=534, model_time=61.9]  0%|          | 41/96139 [04:32<183:58:59,  6.89s/it, avg_loss=122, data_time=210, grad_norm=1.43e+3, iter=40, loss=534, model_time=61.9]  0%|          | 41/96139 [04:39<183:58:59,  6.89s/it, avg_loss=122, data_time=217, grad_norm=3.52e+3, iter=41, loss=134, model_time=62.8]  0%|          | 42/96139 [04:39<188:32:18,  7.06s/it, avg_loss=122, data_time=217, grad_norm=3.52e+3, iter=41, loss=134, model_time=62.8]  0%|          | 42/96139 [04:48<188:32:18,  7.06s/it, avg_loss=129, data_time=224, grad_norm=1.14e+4, iter=42, loss=753, model_time=65.1]  0%|          | 43/96139 [04:48<207:21:35,  7.77s/it, avg_loss=129, data_time=224, grad_norm=1.14e+4, iter=42, loss=753, model_time=65.1]  0%|          | 43/96139 [04:57<207:21:35,  7.77s/it, avg_loss=132, data_time=231, grad_norm=2.14e+4, iter=43, loss=502, model_time=67.2]  0%|          | 44/96139 [04:57<215:53:52,  8.09s/it, avg_loss=132, data_time=231, grad_norm=2.14e+4, iter=43, loss=502, model_time=67.2]  0%|          | 44/96139 [05:03<215:53:52,  8.09s/it, avg_loss=133, data_time=235, grad_norm=3.48e+4, iter=44, loss=208, model_time=68.2]  0%|          | 45/96139 [05:03<193:32:06,  7.25s/it, avg_loss=133, data_time=235, grad_norm=3.48e+4, iter=44, loss=208, model_time=68.2]  0%|          | 45/96139 [05:08<193:32:06,  7.25s/it, avg_loss=133, data_time=240, grad_norm=1.91e+4, iter=45, loss=108, model_time=69.1]  0%|          | 46/96139 [05:08<180:38:42,  6.77s/it, avg_loss=133, data_time=240, grad_norm=1.91e+4, iter=45, loss=108, model_time=69.1]  0%|          | 46/96139 [05:14<180:38:42,  6.77s/it, avg_loss=133, data_time=244, grad_norm=609, iter=46, loss=131, model_time=69.9]      0%|          | 47/96139 [05:14<168:45:05,  6.32s/it, avg_loss=133, data_time=244, grad_norm=609, iter=46, loss=131, model_time=69.9]  0%|          | 47/96139 [05:22<168:45:05,  6.32s/it, avg_loss=137, data_time=251, grad_norm=4.28e+12, iter=47, loss=591, model_time=71.7]  0%|          | 48/96139 [05:22<189:03:47,  7.08s/it, avg_loss=137, data_time=251, grad_norm=4.28e+12, iter=47, loss=591, model_time=71.7]  0%|          | 48/96139 [05:28<189:03:47,  7.08s/it, avg_loss=139, data_time=256, grad_norm=1.28e+8, iter=48, loss=335, model_time=73.2]   0%|          | 49/96139 [05:28<180:47:25,  6.77s/it, avg_loss=139, data_time=256, grad_norm=1.28e+8, iter=48, loss=335, model_time=73.2]  0%|          | 49/96139 [05:34<180:47:25,  6.77s/it, avg_loss=140, data_time=260, grad_norm=6.28e+4, iter=49, loss=176, model_time=74.3]  0%|          | 50/96139 [05:34<167:49:13,  6.29s/it, avg_loss=140, data_time=260, grad_norm=6.28e+4, iter=49, loss=176, model_time=74.3]  0%|          | 50/96139 [05:37<167:49:13,  6.29s/it, avg_loss=140, data_time=262, grad_norm=8.82e+3, iter=50, loss=191, model_time=75.3]  0%|          | 51/96139 [05:37<144:47:15,  5.42s/it, avg_loss=140, data_time=262, grad_norm=8.82e+3, iter=50, loss=191, model_time=75.3]  0%|          | 51/96139 [05:46<144:47:15,  5.42s/it, avg_loss=143, data_time=269, grad_norm=1.96e+8, iter=51, loss=460, model_time=77.4]  0%|          | 52/96139 [05:46<171:19:26,  6.42s/it, avg_loss=143, data_time=269, grad_norm=1.96e+8, iter=51, loss=460, model_time=77.4]  0%|          | 52/96139 [05:51<171:19:26,  6.42s/it, avg_loss=144, data_time=273, grad_norm=1.52e+7, iter=52, loss=183, model_time=78.5]  0%|          | 53/96139 [05:51<159:42:05,  5.98s/it, avg_loss=144, data_time=273, grad_norm=1.52e+7, iter=52, loss=183, model_time=78.5]  0%|          | 53/96139 [05:55<159:42:05,  5.98s/it, avg_loss=145, data_time=276, grad_norm=3.93e+5, iter=53, loss=240, model_time=79.7]  0%|          | 54/96139 [05:55<149:30:13,  5.60s/it, avg_loss=145, data_time=276, grad_norm=3.93e+5, iter=53, loss=240, model_time=79.7]  0%|          | 54/96139 [06:04<149:30:13,  5.60s/it, avg_loss=148, data_time=283, grad_norm=2.39e+12, iter=54, loss=503, model_time=81.8]  0%|          | 55/96139 [06:04<175:29:53,  6.58s/it, avg_loss=148, data_time=283, grad_norm=2.39e+12, iter=54, loss=503, model_time=81.8]  0%|          | 55/96139 [06:11<175:29:53,  6.58s/it, avg_loss=151, data_time=288, grad_norm=8.49e+14, iter=55, loss=457, model_time=83.8]  0%|          | 56/96139 [06:11<178:47:24,  6.70s/it, avg_loss=151, data_time=288, grad_norm=8.49e+14, iter=55, loss=457, model_time=83.8]  0%|          | 56/96139 [06:18<178:47:24,  6.70s/it, avg_loss=154, data_time=293, grad_norm=6.14e+5, iter=56, loss=382, model_time=85.1]   0%|          | 57/96139 [06:18<180:13:20,  6.75s/it, avg_loss=154, data_time=293, grad_norm=6.14e+5, iter=56, loss=382, model_time=85.1]  0%|          | 57/96139 [06:24<180:13:20,  6.75s/it, avg_loss=154, data_time=298, grad_norm=2.07e+5, iter=57, loss=168, model_time=86.2]  0%|          | 58/96139 [06:24<170:49:53,  6.40s/it, avg_loss=154, data_time=298, grad_norm=2.07e+5, iter=57, loss=168, model_time=86.2]  0%|          | 58/96139 [06:32<170:49:53,  6.40s/it, avg_loss=155, data_time=304, grad_norm=8.17e+9, iter=58, loss=237, model_time=87.7]  0%|          | 59/96139 [06:32<182:15:29,  6.83s/it, avg_loss=155, data_time=304, grad_norm=8.17e+9, iter=58, loss=237, model_time=87.7]  0%|          | 59/96139 [06:37<182:15:29,  6.83s/it, avg_loss=154, data_time=308, grad_norm=2.19e+4, iter=59, loss=120, model_time=88.5]  0%|          | 60/96139 [06:37<167:24:28,  6.27s/it, avg_loss=154, data_time=308, grad_norm=2.19e+4, iter=59, loss=120, model_time=88.5]  0%|          | 60/96139 [06:44<167:24:28,  6.27s/it, avg_loss=157, data_time=314, grad_norm=1.95e+14, iter=60, loss=447, model_time=90.5]  0%|          | 61/96139 [06:44<175:03:18,  6.56s/it, avg_loss=157, data_time=314, grad_norm=1.95e+14, iter=60, loss=447, model_time=90.5]  0%|          | 61/96139 [06:50<175:03:18,  6.56s/it, avg_loss=158, data_time=319, grad_norm=2.71e+8, iter=61, loss=244, model_time=91.8]   0%|          | 62/96139 [06:50<170:47:23,  6.40s/it, avg_loss=158, data_time=319, grad_norm=2.71e+8, iter=61, loss=244, model_time=91.8]  0%|          | 62/96139 [06:56<170:47:23,  6.40s/it, avg_loss=160, data_time=324, grad_norm=1.79e+7, iter=62, loss=319, model_time=93.1]  0%|          | 63/96139 [06:56<171:21:20,  6.42s/it, avg_loss=160, data_time=324, grad_norm=1.79e+7, iter=62, loss=319, model_time=93.1]  0%|          | 63/96139 [07:01<171:21:20,  6.42s/it, avg_loss=159, data_time=328, grad_norm=1.93e+4, iter=63, loss=116, model_time=94]    0%|          | 64/96139 [07:01<161:38:57,  6.06s/it, avg_loss=159, data_time=328, grad_norm=1.93e+4, iter=63, loss=116, model_time=94]  0%|          | 64/96139 [07:06<161:38:57,  6.06s/it, avg_loss=161, data_time=331, grad_norm=7.91e+9, iter=64, loss=341, model_time=95.3]  0%|          | 65/96139 [07:06<151:49:33,  5.69s/it, avg_loss=161, data_time=331, grad_norm=7.91e+9, iter=64, loss=341, model_time=95.3]  0%|          | 65/96139 [07:15<151:49:33,  5.69s/it, avg_loss=164, data_time=338, grad_norm=1.89e+11, iter=65, loss=454, model_time=97.4]  0%|          | 66/96139 [07:15<175:19:01,  6.57s/it, avg_loss=164, data_time=338, grad_norm=1.89e+11, iter=65, loss=454, model_time=97.4]  0%|          | 66/96139 [07:20<175:19:01,  6.57s/it, avg_loss=165, data_time=342, grad_norm=6.61e+8, iter=66, loss=278, model_time=98.8]   0%|          | 67/96139 [07:20<166:16:58,  6.23s/it, avg_loss=165, data_time=342, grad_norm=6.61e+8, iter=66, loss=278, model_time=98.8]  0%|          | 67/96139 [07:25<166:16:58,  6.23s/it, avg_loss=164, data_time=346, grad_norm=5.13e+3, iter=67, loss=62.8, model_time=99.6]  0%|          | 68/96139 [07:25<151:20:21,  5.67s/it, avg_loss=164, data_time=346, grad_norm=5.13e+3, iter=67, loss=62.8, model_time=99.6]  0%|          | 68/96139 [07:28<151:20:21,  5.67s/it, avg_loss=163, data_time=348, grad_norm=2.14e+4, iter=68, loss=70, model_time=100]     0%|          | 69/96139 [07:28<131:30:12,  4.93s/it, avg_loss=163, data_time=348, grad_norm=2.14e+4, iter=68, loss=70, model_time=100]  0%|          | 69/96139 [07:32<131:30:12,  4.93s/it, avg_loss=162, data_time=352, grad_norm=30.4, iter=69, loss=26.7, model_time=101]   0%|          | 70/96139 [07:32<126:01:23,  4.72s/it, avg_loss=162, data_time=352, grad_norm=30.4, iter=69, loss=26.7, model_time=101]  0%|          | 70/96139 [07:40<126:01:23,  4.72s/it, avg_loss=166, data_time=358, grad_norm=3.56e+11, iter=70, loss=600, model_time=103]  0%|          | 71/96139 [07:40<153:45:47,  5.76s/it, avg_loss=166, data_time=358, grad_norm=3.56e+11, iter=70, loss=600, model_time=103]  0%|          | 71/96139 [07:45<153:45:47,  5.76s/it, avg_loss=168, data_time=362, grad_norm=1.14e+6, iter=71, loss=354, model_time=104]   0%|          | 72/96139 [07:45<147:29:34,  5.53s/it, avg_loss=168, data_time=362, grad_norm=1.14e+6, iter=71, loss=354, model_time=104]  0%|          | 72/96139 [07:49<147:29:34,  5.53s/it, avg_loss=167, data_time=365, grad_norm=325, iter=72, loss=93.6, model_time=105]     0%|          | 73/96139 [07:49<133:37:41,  5.01s/it, avg_loss=167, data_time=365, grad_norm=325, iter=72, loss=93.6, model_time=105]  0%|          | 73/96139 [07:53<133:37:41,  5.01s/it, avg_loss=166, data_time=368, grad_norm=9.09e+3, iter=73, loss=55.8, model_time=106]  0%|          | 74/96139 [07:53<127:03:21,  4.76s/it, avg_loss=166, data_time=368, grad_norm=9.09e+3, iter=73, loss=55.8, model_time=106]  0%|          | 74/96139 [07:58<127:03:21,  4.76s/it, avg_loss=167, data_time=372, grad_norm=1.85e+4, iter=74, loss=210, model_time=107]   0%|          | 75/96139 [07:58<129:02:03,  4.84s/it, avg_loss=167, data_time=372, grad_norm=1.85e+4, iter=74, loss=210, model_time=107]  0%|          | 75/96139 [08:03<129:02:03,  4.84s/it, avg_loss=167, data_time=376, grad_norm=2.03e+5, iter=75, loss=183, model_time=108]  0%|          | 76/96139 [08:03<129:06:53,  4.84s/it, avg_loss=167, data_time=376, grad_norm=2.03e+5, iter=75, loss=183, model_time=108]  0%|          | 76/96139 [08:08<129:06:53,  4.84s/it, avg_loss=168, data_time=380, grad_norm=2.09e+7, iter=76, loss=275, model_time=109]  0%|          | 77/96139 [08:08<133:07:35,  4.99s/it, avg_loss=168, data_time=380, grad_norm=2.09e+7, iter=76, loss=275, model_time=109]  0%|          | 77/96139 [08:14<133:07:35,  4.99s/it, avg_loss=167, data_time=385, grad_norm=4.22e+5, iter=77, loss=84.4, model_time=110]  0%|          | 78/96139 [08:14<140:06:39,  5.25s/it, avg_loss=167, data_time=385, grad_norm=4.22e+5, iter=77, loss=84.4, model_time=110]  0%|          | 78/96139 [08:21<140:06:39,  5.25s/it, avg_loss=170, data_time=391, grad_norm=1.08e+11, iter=78, loss=442, model_time=111]  0%|          | 79/96139 [08:21<152:12:44,  5.70s/it, avg_loss=170, data_time=391, grad_norm=1.08e+11, iter=78, loss=442, model_time=111]  0%|          | 79/96139 [08:28<152:12:44,  5.70s/it, avg_loss=173, data_time=396, grad_norm=4.13e+11, iter=79, loss=506, model_time=113]  0%|          | 80/96139 [08:28<161:54:28,  6.07s/it, avg_loss=173, data_time=396, grad_norm=4.13e+11, iter=79, loss=506, model_time=113]  0%|          | 80/96139 [08:33<161:54:28,  6.07s/it, avg_loss=172, data_time=400, grad_norm=3.74e+3, iter=80, loss=91.4, model_time=114]  0%|          | 81/96139 [08:33<153:20:39,  5.75s/it, avg_loss=172, data_time=400, grad_norm=3.74e+3, iter=80, loss=91.4, model_time=114]  0%|          | 81/96139 [08:40<153:20:39,  5.75s/it, avg_loss=174, data_time=405, grad_norm=1.7e+8, iter=81, loss=328, model_time=115]    0%|          | 82/96139 [08:40<159:33:49,  5.98s/it, avg_loss=174, data_time=405, grad_norm=1.7e+8, iter=81, loss=328, model_time=115]  0%|          | 82/96139 [08:47<159:33:49,  5.98s/it, avg_loss=176, data_time=410, grad_norm=4.71e+11, iter=82, loss=357, model_time=117]  0%|          | 83/96139 [08:47<167:21:00,  6.27s/it, avg_loss=176, data_time=410, grad_norm=4.71e+11, iter=82, loss=357, model_time=117]  0%|          | 83/96139 [08:52<167:21:00,  6.27s/it, avg_loss=177, data_time=414, grad_norm=6.89e+9, iter=83, loss=327, model_time=118]   0%|          | 84/96139 [08:52<161:29:41,  6.05s/it, avg_loss=177, data_time=414, grad_norm=6.89e+9, iter=83, loss=327, model_time=118]sys:1: RuntimeWarning: Traceback of forward call that caused the error:
  File "train.py", line 302, in <module>
    run(config)
  File "train.py", line 214, in run
    run_state = run_epoch(model, optimizer, train_ldr, logger, *run_state)
  File "train.py", line 55, in run_epoch
    loss = model.loss(temp_batch)
  File "/home/dzubke/awni_speech/speech/speech/models/ctc_model_train.py", line 53, in loss
    out, rnn_args = self.forward_impl(x)
  File "/home/dzubke/awni_speech/speech/speech/models/ctc_model_train.py", line 45, in forward_impl
    x, rnn_args = self.encode(x, rnn_args)
  File "/home/dzubke/awni_speech/speech/speech/models/model.py", line 84, in encode
    x = self.conv(x)
  File "/home/dzubke/miniconda3/envs/awni_env36/lib/python3.6/site-packages/torch/nn/modules/module.py", line 477, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/dzubke/miniconda3/envs/awni_env36/lib/python3.6/site-packages/torch/nn/modules/container.py", line 91, in forward
    input = module(input)
  File "/home/dzubke/miniconda3/envs/awni_env36/lib/python3.6/site-packages/torch/nn/modules/module.py", line 477, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/dzubke/miniconda3/envs/awni_env36/lib/python3.6/site-packages/torch/nn/modules/batchnorm.py", line 66, in forward
    exponential_average_factor, self.eps)
  File "/home/dzubke/miniconda3/envs/awni_env36/lib/python3.6/site-packages/torch/nn/functional.py", line 1254, in batch_norm
    training, momentum, eps, torch.backends.cudnn.enabled

Succesfully loaded weights from trained model
====== Model, loaders, optimimzer created =======
model: CTC_train(
  (conv): Sequential(
    (0): Conv2d(1, 32, kernel_size=(11, 41), stride=(1, 2), padding=(0, 20))
    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): Dropout(p=0.4)
    (4): Conv2d(32, 32, kernel_size=(11, 21), stride=(1, 2), padding=(0, 10))
    (5): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): ReLU()
    (7): Dropout(p=0.4)
    (8): Conv2d(32, 96, kernel_size=(11, 21), stride=(1, 1), padding=(0, 10))
    (9): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (10): ReLU()
    (11): Dropout(p=0.4)
  )
  (rnn): LSTM(6240, 512, num_layers=5, batch_first=True, dropout=0.4)
  (fc): LinearND(
    (fc): Linear(in_features=512, out_features=40, bias=True)
  )
)
preproc: Showing limited attributes as not all new attributes are supported

_input_dim: 257
start_and_end: False
int_to_char: {0: 's', 1: 't', 2: 'd', 3: 'r', 4: 'iy', 5: 'th', 6: 'aa', 7: 'v', 8: 'ay', 9: 'er', 10: 'oy', 11: 'p', 12: 'uh', 13: 'hh', 14: 'ch', 15: 'w', 16: 'ao', 17: 'jh', 18: 'l', 19: 'ow', 20: 'uw', 21: 'sh', 22: 'dh', 23: 'ng', 24: 'f', 25: 'z', 26: 'n', 27: 'b', 28: 'ey', 29: 'y', 30: 'ih', 31: 'aw', 32: 'g', 33: 'ae', 34: 'eh', 35: 'm', 36: 'k', 37: 'ah', 38: 'zh'}
char_to_int: {'s': 0, 't': 1, 'd': 2, 'r': 3, 'iy': 4, 'th': 5, 'aa': 6, 'v': 7, 'ay': 8, 'er': 9, 'oy': 10, 'p': 11, 'uh': 12, 'hh': 13, 'ch': 14, 'w': 15, 'ao': 16, 'jh': 17, 'l': 18, 'ow': 19, 'uw': 20, 'sh': 21, 'dh': 22, 'ng': 23, 'f': 24, 'z': 25, 'n': 26, 'b': 27, 'ey': 28, 'y': 29, 'ih': 30, 'aw': 31, 'g': 32, 'ae': 33, 'eh': 34, 'm': 35, 'k': 36, 'ah': 37, 'zh': 38}
optimizer: SGD (
Parameter Group 0
    dampening: 0.98
    initial_lr: 0.0008
    lr: 0.0008
    momentum: 0.9
    nesterov: False
    weight_decay: 0
)
learning rate: 0.0008
Traceback (most recent call last):
  File "train.py", line 302, in <module>
    run(config)
  File "train.py", line 214, in run
    run_state = run_epoch(model, optimizer, train_ldr, logger, *run_state)
  File "train.py", line 59, in run_epoch
    loss.backward()
  File "/home/dzubke/miniconda3/envs/awni_env36/lib/python3.6/site-packages/torch/tensor.py", line 93, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph)
  File "/home/dzubke/miniconda3/envs/awni_env36/lib/python3.6/site-packages/torch/autograd/__init__.py", line 90, in backward
    allow_unreachable=True)  # allow_unreachable flag
RuntimeError: Function 'CudnnBatchNormBackward' returned nan values in its 0th output.
  0%|          | 84/96139 [09:04<172:50:43,  6.48s/it, avg_loss=177, data_time=414, grad_norm=6.89e+9, iter=83, loss=327, model_time=118]