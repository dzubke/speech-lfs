nohup: ignoring input
/home/dzubke/miniconda3/envs/awni_env36/lib/python3.6/site-packages/torch/serialization.py:425: SourceChangeWarning: source code of class 'speech.models.ctc_model_train.CTC_train' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.
  warnings.warn(msg, SourceChangeWarning)
  0%|          | 0/96139 [00:00<?, ?it/s]  0%|          | 0/96139 [00:09<?, ?it/s, avg_loss=3.57, data_time=7.79, grad_norm=3.5e+3, iter=0, loss=357, model_time=2]  0%|          | 1/96139 [00:09<261:47:11,  9.80s/it, avg_loss=3.57, data_time=7.79, grad_norm=3.5e+3, iter=0, loss=357, model_time=2]  0%|          | 1/96139 [00:14<261:47:11,  9.80s/it, avg_loss=4.22, data_time=12, grad_norm=105, iter=1, loss=68.1, model_time=2.64]   0%|          | 2/96139 [00:14<221:51:01,  8.31s/it, avg_loss=4.22, data_time=12, grad_norm=105, iter=1, loss=68.1, model_time=2.64]  0%|          | 2/96139 [00:19<221:51:01,  8.31s/it, avg_loss=5.41, data_time=16.2, grad_norm=755, iter=2, loss=123, model_time=3.41]  0%|          | 3/96139 [00:19<195:32:09,  7.32s/it, avg_loss=5.41, data_time=16.2, grad_norm=755, iter=2, loss=123, model_time=3.41]  0%|          | 3/96139 [00:26<195:32:09,  7.32s/it, avg_loss=7.77, data_time=22.1, grad_norm=3.05e+3, iter=3, loss=242, model_time=4.55]  0%|          | 4/96139 [00:26<193:13:04,  7.24s/it, avg_loss=7.77, data_time=22.1, grad_norm=3.05e+3, iter=3, loss=242, model_time=4.55]  0%|          | 4/96139 [00:37<193:13:04,  7.24s/it, avg_loss=15, data_time=30.4, grad_norm=1.69e+4, iter=4, loss=734, model_time=6.87]    0%|          | 5/96139 [00:37<219:49:57,  8.23s/it, avg_loss=15, data_time=30.4, grad_norm=1.69e+4, iter=4, loss=734, model_time=6.87]  0%|          | 5/96139 [00:45<219:49:57,  8.23s/it, avg_loss=21.2, data_time=36.8, grad_norm=9.72e+3, iter=5, loss=635, model_time=8.77]  0%|          | 6/96139 [00:45<220:49:03,  8.27s/it, avg_loss=21.2, data_time=36.8, grad_norm=9.72e+3, iter=5, loss=635, model_time=8.77]  0%|          | 6/96139 [00:53<220:49:03,  8.27s/it, avg_loss=27.5, data_time=43, grad_norm=4.42e+4, iter=6, loss=643, model_time=10.7]    0%|          | 7/96139 [00:53<219:38:25,  8.23s/it, avg_loss=27.5, data_time=43, grad_norm=4.42e+4, iter=6, loss=643, model_time=10.7]  0%|          | 7/96139 [00:59<219:38:25,  8.23s/it, avg_loss=28.8, data_time=47.6, grad_norm=1.95e+3, iter=7, loss=163, model_time=11.6]  0%|          | 8/96139 [00:59<197:00:13,  7.38s/it, avg_loss=28.8, data_time=47.6, grad_norm=1.95e+3, iter=7, loss=163, model_time=11.6]  0%|          | 8/96139 [01:05<197:00:13,  7.38s/it, avg_loss=33.4, data_time=52.5, grad_norm=1.8e+3, iter=8, loss=486, model_time=13]     0%|          | 9/96139 [01:05<189:28:12,  7.10s/it, avg_loss=33.4, data_time=52.5, grad_norm=1.8e+3, iter=8, loss=486, model_time=13]  0%|          | 9/96139 [01:12<189:28:12,  7.10s/it, avg_loss=37.5, data_time=57.9, grad_norm=9.1e+3, iter=9, loss=451, model_time=14.4]  0%|          | 10/96139 [01:12<186:23:37,  6.98s/it, avg_loss=37.5, data_time=57.9, grad_norm=9.1e+3, iter=9, loss=451, model_time=14.4]  0%|          | 10/96139 [01:17<186:23:37,  6.98s/it, avg_loss=40.6, data_time=61.9, grad_norm=2.31e+3, iter=10, loss=346, model_time=15.7]  0%|          | 11/96139 [01:17<173:42:59,  6.51s/it, avg_loss=40.6, data_time=61.9, grad_norm=2.31e+3, iter=10, loss=346, model_time=15.7]  0%|          | 11/96139 [01:21<173:42:59,  6.51s/it, avg_loss=41.1, data_time=65.3, grad_norm=237, iter=11, loss=85.2, model_time=16.4]     0%|          | 12/96139 [01:21<154:07:29,  5.77s/it, avg_loss=41.1, data_time=65.3, grad_norm=237, iter=11, loss=85.2, model_time=16.4]  0%|          | 12/96139 [01:29<154:07:29,  5.77s/it, avg_loss=46.8, data_time=71.3, grad_norm=2.78e+5, iter=12, loss=612, model_time=18.5]  0%|          | 13/96139 [01:29<172:37:41,  6.47s/it, avg_loss=46.8, data_time=71.3, grad_norm=2.78e+5, iter=12, loss=612, model_time=18.5]  0%|          | 13/96139 [01:35<172:37:41,  6.47s/it, avg_loss=48.7, data_time=76.2, grad_norm=3.19e+4, iter=13, loss=236, model_time=19.5]  0%|          | 14/96139 [01:35<167:51:44,  6.29s/it, avg_loss=48.7, data_time=76.2, grad_norm=3.19e+4, iter=13, loss=236, model_time=19.5]  0%|          | 14/96139 [01:41<167:51:44,  6.29s/it, avg_loss=50.4, data_time=80.8, grad_norm=477, iter=14, loss=217, model_time=20.5]      0%|          | 15/96139 [01:41<162:26:12,  6.08s/it, avg_loss=50.4, data_time=80.8, grad_norm=477, iter=14, loss=217, model_time=20.5]  0%|          | 15/96139 [01:45<162:26:12,  6.08s/it, avg_loss=51.5, data_time=84.4, grad_norm=437, iter=15, loss=161, model_time=21.3]  0%|          | 16/96139 [01:45<148:29:12,  5.56s/it, avg_loss=51.5, data_time=84.4, grad_norm=437, iter=15, loss=161, model_time=21.3]  0%|          | 16/96139 [01:53<148:29:12,  5.56s/it, avg_loss=56.2, data_time=90.5, grad_norm=3.64e+5, iter=16, loss=527, model_time=23.2]  0%|          | 17/96139 [01:53<168:49:25,  6.32s/it, avg_loss=56.2, data_time=90.5, grad_norm=3.64e+5, iter=16, loss=527, model_time=23.2]  0%|          | 17/96139 [02:00<168:49:25,  6.32s/it, avg_loss=59.6, data_time=95.9, grad_norm=4.04e+3, iter=17, loss=398, model_time=24.7]  0%|          | 18/96139 [02:00<173:22:34,  6.49s/it, avg_loss=59.6, data_time=95.9, grad_norm=4.04e+3, iter=17, loss=398, model_time=24.7]  0%|          | 18/96139 [02:06<173:22:34,  6.49s/it, avg_loss=62.4, data_time=101, grad_norm=1.57e+4, iter=18, loss=332, model_time=26.1]   0%|          | 19/96139 [02:06<169:27:43,  6.35s/it, avg_loss=62.4, data_time=101, grad_norm=1.57e+4, iter=18, loss=332, model_time=26.1]  0%|          | 19/96139 [02:15<169:27:43,  6.35s/it, avg_loss=67.3, data_time=107, grad_norm=2.22e+4, iter=19, loss=555, model_time=28.1]  0%|          | 20/96139 [02:15<185:59:23,  6.97s/it, avg_loss=67.3, data_time=107, grad_norm=2.22e+4, iter=19, loss=555, model_time=28.1]/home/dzubke/awni_speech/speech/speech/utils/model_debug.py:140: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).
  fig, ax1 = plt.subplots()
  0%|          | 20/96139 [02:23<185:59:23,  6.97s/it, avg_loss=72, data_time=113, grad_norm=1.94e+3, iter=20, loss=535, model_time=30.1]    0%|          | 21/96139 [02:23<196:47:15,  7.37s/it, avg_loss=72, data_time=113, grad_norm=1.94e+3, iter=20, loss=535, model_time=30.1]  0%|          | 21/96139 [02:31<196:47:15,  7.37s/it, avg_loss=75.2, data_time=119, grad_norm=6.61e+4, iter=21, loss=391, model_time=31.8]  0%|          | 22/96139 [02:31<199:57:56,  7.49s/it, avg_loss=75.2, data_time=119, grad_norm=6.61e+4, iter=21, loss=391, model_time=31.8]  0%|          | 22/96139 [02:39<199:57:56,  7.49s/it, avg_loss=79.8, data_time=125, grad_norm=6.43e+3, iter=22, loss=542, model_time=34]    0%|          | 23/96139 [02:39<206:29:50,  7.73s/it, avg_loss=79.8, data_time=125, grad_norm=6.43e+3, iter=22, loss=542, model_time=34]  0%|          | 23/96139 [02:43<206:29:50,  7.73s/it, avg_loss=80.2, data_time=129, grad_norm=1.42e+4, iter=23, loss=115, model_time=35]  0%|          | 24/96139 [02:43<181:02:47,  6.78s/it, avg_loss=80.2, data_time=129, grad_norm=1.42e+4, iter=23, loss=115, model_time=35]  0%|          | 24/96139 [02:48<181:02:47,  6.78s/it, avg_loss=80.3, data_time=133, grad_norm=1.02e+3, iter=24, loss=97, model_time=35.8]  0%|          | 25/96139 [02:48<164:31:36,  6.16s/it, avg_loss=80.3, data_time=133, grad_norm=1.02e+3, iter=24, loss=97, model_time=35.8]  0%|          | 25/96139 [02:55<164:31:36,  6.16s/it, avg_loss=82.4, data_time=138, grad_norm=977, iter=25, loss=287, model_time=37.1]     0%|          | 26/96139 [02:55<166:24:29,  6.23s/it, avg_loss=82.4, data_time=138, grad_norm=977, iter=25, loss=287, model_time=37.1]  0%|          | 26/96139 [03:01<166:24:29,  6.23s/it, avg_loss=85.4, data_time=143, grad_norm=1.54e+3, iter=26, loss=376, model_time=38.4]  0%|          | 27/96139 [03:01<164:34:26,  6.16s/it, avg_loss=85.4, data_time=143, grad_norm=1.54e+3, iter=26, loss=376, model_time=38.4]  0%|          | 27/96139 [03:09<164:34:26,  6.16s/it, avg_loss=90.4, data_time=148, grad_norm=2.18e+3, iter=27, loss=589, model_time=40.7]  0%|          | 28/96139 [03:09<178:44:18,  6.69s/it, avg_loss=90.4, data_time=148, grad_norm=2.18e+3, iter=27, loss=589, model_time=40.7]  0%|          | 28/96139 [03:17<178:44:18,  6.69s/it, avg_loss=95, data_time=155, grad_norm=696, iter=28, loss=548, model_time=42.5]        0%|          | 29/96139 [03:17<194:56:01,  7.30s/it, avg_loss=95, data_time=155, grad_norm=696, iter=28, loss=548, model_time=42.5]  0%|          | 29/96139 [03:26<194:56:01,  7.30s/it, avg_loss=99.7, data_time=162, grad_norm=599, iter=29, loss=565, model_time=44.3]  0%|          | 30/96139 [03:26<205:51:16,  7.71s/it, avg_loss=99.7, data_time=162, grad_norm=599, iter=29, loss=565, model_time=44.3]  0%|          | 30/96139 [03:31<205:51:16,  7.71s/it, avg_loss=100, data_time=167, grad_norm=250, iter=30, loss=133, model_time=45.1]   0%|          | 31/96139 [03:31<186:02:49,  6.97s/it, avg_loss=100, data_time=167, grad_norm=250, iter=30, loss=133, model_time=45.1]  0%|          | 31/96139 [03:40<186:02:49,  6.97s/it, avg_loss=104, data_time=173, grad_norm=1e+3, iter=31, loss=477, model_time=47]   0%|          | 32/96139 [03:40<199:01:33,  7.46s/it, avg_loss=104, data_time=173, grad_norm=1e+3, iter=31, loss=477, model_time=47]  0%|          | 32/96139 [03:45<199:01:33,  7.46s/it, avg_loss=105, data_time=177, grad_norm=337, iter=32, loss=264, model_time=48.2]  0%|          | 33/96139 [03:45<181:49:32,  6.81s/it, avg_loss=105, data_time=177, grad_norm=337, iter=32, loss=264, model_time=48.2]  0%|          | 33/96139 [03:54<181:49:32,  6.81s/it, avg_loss=109, data_time=184, grad_norm=1.03e+4, iter=33, loss=485, model_time=50.3]  0%|          | 34/96139 [03:54<200:08:17,  7.50s/it, avg_loss=109, data_time=184, grad_norm=1.03e+4, iter=33, loss=485, model_time=50.3]  0%|          | 34/96139 [04:02<200:08:17,  7.50s/it, avg_loss=110, data_time=189, grad_norm=313, iter=34, loss=194, model_time=54.1]      0%|          | 35/96139 [04:02<204:17:30,  7.65s/it, avg_loss=110, data_time=189, grad_norm=313, iter=34, loss=194, model_time=54.1]  0%|          | 35/96139 [04:07<204:17:30,  7.65s/it, avg_loss=111, data_time=192, grad_norm=3.62e+4, iter=35, loss=207, model_time=55.3]  0%|          | 36/96139 [04:07<183:59:48,  6.89s/it, avg_loss=111, data_time=192, grad_norm=3.62e+4, iter=35, loss=207, model_time=55.3]  0%|          | 36/96139 [04:15<183:59:48,  6.89s/it, avg_loss=114, data_time=198, grad_norm=513, iter=36, loss=377, model_time=56.7]      0%|          | 37/96139 [04:15<187:06:12,  7.01s/it, avg_loss=114, data_time=198, grad_norm=513, iter=36, loss=377, model_time=56.7]  0%|          | 37/96139 [04:19<187:06:12,  7.01s/it, avg_loss=114, data_time=202, grad_norm=1.42e+3, iter=37, loss=110, model_time=57.5]  0%|          | 38/96139 [04:19<167:50:06,  6.29s/it, avg_loss=114, data_time=202, grad_norm=1.42e+3, iter=37, loss=110, model_time=57.5]  0%|          | 38/96139 [04:27<167:50:06,  6.29s/it, avg_loss=118, data_time=208, grad_norm=4.72e+3, iter=38, loss=562, model_time=59.6]  0%|          | 39/96139 [04:27<183:32:32,  6.88s/it, avg_loss=118, data_time=208, grad_norm=4.72e+3, iter=38, loss=562, model_time=59.6]  0%|          | 39/96139 [04:33<183:32:32,  6.88s/it, avg_loss=118, data_time=213, grad_norm=416, iter=39, loss=106, model_time=60.5]      0%|          | 40/96139 [04:33<170:04:18,  6.37s/it, avg_loss=118, data_time=213, grad_norm=416, iter=39, loss=106, model_time=60.5]  0%|          | 40/96139 [04:42<170:04:18,  6.37s/it, avg_loss=122, data_time=220, grad_norm=4.62e+4, iter=40, loss=544, model_time=62.8]  0%|          | 41/96139 [04:42<194:06:24,  7.27s/it, avg_loss=122, data_time=220, grad_norm=4.62e+4, iter=40, loss=544, model_time=62.8]  0%|          | 41/96139 [04:47<194:06:24,  7.27s/it, avg_loss=122, data_time=224, grad_norm=1.39e+5, iter=41, loss=136, model_time=63.7]  0%|          | 42/96139 [04:47<177:33:51,  6.65s/it, avg_loss=122, data_time=224, grad_norm=1.39e+5, iter=41, loss=136, model_time=63.7]  0%|          | 42/96139 [04:55<177:33:51,  6.65s/it, avg_loss=129, data_time=230, grad_norm=1.93e+4, iter=42, loss=764, model_time=65.9]  0%|          | 43/96139 [04:56<190:53:10,  7.15s/it, avg_loss=129, data_time=230, grad_norm=1.93e+4, iter=42, loss=764, model_time=65.9]  0%|          | 43/96139 [05:04<190:53:10,  7.15s/it, avg_loss=133, data_time=237, grad_norm=6.45e+4, iter=43, loss=518, model_time=68]    0%|          | 44/96139 [05:04<204:23:27,  7.66s/it, avg_loss=133, data_time=237, grad_norm=6.45e+4, iter=43, loss=518, model_time=68]  0%|          | 44/96139 [05:09<204:23:27,  7.66s/it, avg_loss=134, data_time=241, grad_norm=3.92e+5, iter=44, loss=225, model_time=69]  0%|          | 45/96139 [05:09<181:36:52,  6.80s/it, avg_loss=134, data_time=241, grad_norm=3.92e+5, iter=44, loss=225, model_time=69]  0%|          | 45/96139 [05:14<181:36:52,  6.80s/it, avg_loss=133, data_time=245, grad_norm=6.27e+3, iter=45, loss=109, model_time=69.9]  0%|          | 46/96139 [05:14<167:54:41,  6.29s/it, avg_loss=133, data_time=245, grad_norm=6.27e+3, iter=45, loss=109, model_time=69.9]  0%|          | 46/96139 [05:20<167:54:41,  6.29s/it, avg_loss=133, data_time=249, grad_norm=5.84e+5, iter=46, loss=146, model_time=70.7]  0%|          | 47/96139 [05:20<160:07:58,  6.00s/it, avg_loss=133, data_time=249, grad_norm=5.84e+5, iter=46, loss=146, model_time=70.7]  0%|          | 47/96139 [05:27<160:07:58,  6.00s/it, avg_loss=138, data_time=255, grad_norm=7.6e+5, iter=47, loss=587, model_time=72.6]   0%|          | 48/96139 [05:27<172:51:53,  6.48s/it, avg_loss=138, data_time=255, grad_norm=7.6e+5, iter=47, loss=587, model_time=72.6]  0%|          | 48/96139 [05:33<172:51:53,  6.48s/it, avg_loss=140, data_time=260, grad_norm=8.29e+5, iter=48, loss=345, model_time=74.1]  0%|          | 49/96139 [05:33<170:07:05,  6.37s/it, avg_loss=140, data_time=260, grad_norm=8.29e+5, iter=48, loss=345, model_time=74.1]  0%|          | 49/96139 [05:38<170:07:05,  6.37s/it, avg_loss=141, data_time=263, grad_norm=1.55e+6, iter=49, loss=199, model_time=75.1]  0%|          | 50/96139 [05:38<156:44:54,  5.87s/it, avg_loss=141, data_time=263, grad_norm=1.55e+6, iter=49, loss=199, model_time=75.1]  0%|          | 50/96139 [05:45<156:44:54,  5.87s/it, avg_loss=141, data_time=269, grad_norm=1.45e+4, iter=50, loss=196, model_time=76.2]  0%|          | 51/96139 [05:45<162:43:41,  6.10s/it, avg_loss=141, data_time=269, grad_norm=1.45e+4, iter=50, loss=196, model_time=76.2]  0%|          | 51/96139 [05:53<162:43:41,  6.10s/it, avg_loss=145, data_time=276, grad_norm=1.18e+4, iter=51, loss=476, model_time=78.2]  0%|          | 52/96139 [05:53<184:15:16,  6.90s/it, avg_loss=145, data_time=276, grad_norm=1.18e+4, iter=51, loss=476, model_time=78.2]  0%|          | 52/96139 [05:59<184:15:16,  6.90s/it, avg_loss=145, data_time=281, grad_norm=2.2e+7, iter=52, loss=189, model_time=79.3]   0%|          | 53/96139 [05:59<176:31:02,  6.61s/it, avg_loss=145, data_time=281, grad_norm=2.2e+7, iter=52, loss=189, model_time=79.3]  0%|          | 53/96139 [06:05<176:31:02,  6.61s/it, avg_loss=146, data_time=285, grad_norm=2.74e+4, iter=53, loss=248, model_time=80.4]  0%|          | 54/96139 [06:05<168:18:07,  6.31s/it, avg_loss=146, data_time=285, grad_norm=2.74e+4, iter=53, loss=248, model_time=80.4]  0%|          | 54/96139 [06:14<168:18:07,  6.31s/it, avg_loss=150, data_time=292, grad_norm=1.91e+5, iter=54, loss=499, model_time=82.6]  0%|          | 55/96139 [06:14<190:29:53,  7.14s/it, avg_loss=150, data_time=292, grad_norm=1.91e+5, iter=54, loss=499, model_time=82.6]  0%|          | 55/96139 [06:24<190:29:53,  7.14s/it, avg_loss=153, data_time=300, grad_norm=9.23e+5, iter=55, loss=465, model_time=84.7]  0%|          | 56/96139 [06:24<216:32:07,  8.11s/it, avg_loss=153, data_time=300, grad_norm=9.23e+5, iter=55, loss=465, model_time=84.7]  0%|          | 56/96139 [06:30<216:32:07,  8.11s/it, avg_loss=155, data_time=305, grad_norm=1.21e+7, iter=56, loss=403, model_time=86.1]  0%|          | 57/96139 [06:30<200:28:50,  7.51s/it, avg_loss=155, data_time=305, grad_norm=1.21e+7, iter=56, loss=403, model_time=86.1]  0%|          | 57/96139 [06:35<200:28:50,  7.51s/it, avg_loss=155, data_time=309, grad_norm=4.34e+3, iter=57, loss=174, model_time=87.1]  0%|          | 58/96139 [06:35<179:28:13,  6.72s/it, avg_loss=155, data_time=309, grad_norm=4.34e+3, iter=57, loss=174, model_time=87.1]  0%|          | 58/96139 [06:42<179:28:13,  6.72s/it, avg_loss=156, data_time=314, grad_norm=1.27e+9, iter=58, loss=242, model_time=88.6]  0%|          | 59/96139 [06:42<179:44:32,  6.73s/it, avg_loss=156, data_time=314, grad_norm=1.27e+9, iter=58, loss=242, model_time=88.6]  0%|          | 59/96139 [06:47<179:44:32,  6.73s/it, avg_loss=156, data_time=318, grad_norm=6.19e+6, iter=59, loss=125, model_time=89.6]  0%|          | 60/96139 [06:47<164:24:25,  6.16s/it, avg_loss=156, data_time=318, grad_norm=6.19e+6, iter=59, loss=125, model_time=89.6]  0%|          | 60/96139 [06:55<164:24:25,  6.16s/it, avg_loss=159, data_time=324, grad_norm=3.29e+4, iter=60, loss=447, model_time=91.5]  0%|          | 61/96139 [06:55<178:03:38,  6.67s/it, avg_loss=159, data_time=324, grad_norm=3.29e+4, iter=60, loss=447, model_time=91.5]  0%|          | 61/96139 [07:02<178:03:38,  6.67s/it, avg_loss=160, data_time=329, grad_norm=4.19e+6, iter=61, loss=256, model_time=92.9]  0%|          | 62/96139 [07:02<180:21:00,  6.76s/it, avg_loss=160, data_time=329, grad_norm=4.19e+6, iter=61, loss=256, model_time=92.9]  0%|          | 62/96139 [07:08<180:21:00,  6.76s/it, avg_loss=162, data_time=334, grad_norm=1.38e+6, iter=62, loss=339, model_time=94.2]  0%|          | 63/96139 [07:08<174:49:49,  6.55s/it, avg_loss=162, data_time=334, grad_norm=1.38e+6, iter=62, loss=339, model_time=94.2]  0%|          | 63/96139 [07:13<174:49:49,  6.55s/it, avg_loss=161, data_time=338, grad_norm=6.32e+4, iter=63, loss=113, model_time=95.1]  0%|          | 64/96139 [07:13<161:39:10,  6.06s/it, avg_loss=161, data_time=338, grad_norm=6.32e+4, iter=63, loss=113, model_time=95.1]  0%|          | 64/96139 [07:19<161:39:10,  6.06s/it, avg_loss=163, data_time=343, grad_norm=1.19e+4, iter=64, loss=358, model_time=96.3]  0%|          | 65/96139 [07:19<160:17:05,  6.01s/it, avg_loss=163, data_time=343, grad_norm=1.19e+4, iter=64, loss=358, model_time=96.3]  0%|          | 65/96139 [07:27<160:17:05,  6.01s/it, avg_loss=166, data_time=349, grad_norm=1.23e+8, iter=65, loss=457, model_time=98.5]  0%|          | 66/96139 [07:27<178:33:32,  6.69s/it, avg_loss=166, data_time=349, grad_norm=1.23e+8, iter=65, loss=457, model_time=98.5]  0%|          | 66/96139 [07:34<178:33:32,  6.69s/it, avg_loss=167, data_time=354, grad_norm=8.62e+6, iter=66, loss=291, model_time=99.9]  0%|          | 67/96139 [07:34<179:14:13,  6.72s/it, avg_loss=167, data_time=354, grad_norm=8.62e+6, iter=66, loss=291, model_time=99.9]  0%|          | 67/96139 [07:39<179:14:13,  6.72s/it, avg_loss=166, data_time=359, grad_norm=2.19e+3, iter=67, loss=59.7, model_time=101]  0%|          | 68/96139 [07:39<166:24:42,  6.24s/it, avg_loss=166, data_time=359, grad_norm=2.19e+3, iter=67, loss=59.7, model_time=101]  0%|          | 68/96139 [07:45<166:24:42,  6.24s/it, avg_loss=165, data_time=364, grad_norm=5.74e+4, iter=68, loss=64.6, model_time=101]  0%|          | 69/96139 [07:45<162:04:50,  6.07s/it, avg_loss=165, data_time=364, grad_norm=5.74e+4, iter=68, loss=64.6, model_time=101]  0%|          | 69/96139 [07:48<162:04:50,  6.07s/it, avg_loss=164, data_time=367, grad_norm=41.7, iter=69, loss=27.2, model_time=102]     0%|          | 70/96139 [07:48<144:26:54,  5.41s/it, avg_loss=164, data_time=367, grad_norm=41.7, iter=69, loss=27.2, model_time=102]  0%|          | 70/96139 [07:56<144:26:54,  5.41s/it, avg_loss=168, data_time=372, grad_norm=1.04e+7, iter=70, loss=614, model_time=104]  0%|          | 71/96139 [07:56<163:26:36,  6.12s/it, avg_loss=168, data_time=372, grad_norm=1.04e+7, iter=70, loss=614, model_time=104]  0%|          | 71/96139 [08:02<163:26:36,  6.12s/it, avg_loss=170, data_time=377, grad_norm=6.28e+5, iter=71, loss=358, model_time=106]  0%|          | 72/96139 [08:02<162:47:54,  6.10s/it, avg_loss=170, data_time=377, grad_norm=6.28e+5, iter=71, loss=358, model_time=106]  0%|          | 72/96139 [08:07<162:47:54,  6.10s/it, avg_loss=170, data_time=381, grad_norm=2.05e+4, iter=72, loss=97.4, model_time=106]  0%|          | 73/96139 [08:07<153:28:22,  5.75s/it, avg_loss=170, data_time=381, grad_norm=2.05e+4, iter=72, loss=97.4, model_time=106]  0%|          | 73/96139 [08:11<153:28:22,  5.75s/it, avg_loss=168, data_time=385, grad_norm=1.82e+3, iter=73, loss=57.7, model_time=107]  0%|          | 74/96139 [08:11<141:39:15,  5.31s/it, avg_loss=168, data_time=385, grad_norm=1.82e+3, iter=73, loss=57.7, model_time=107]  0%|          | 74/96139 [08:16<141:39:15,  5.31s/it, avg_loss=169, data_time=389, grad_norm=5.02e+5, iter=74, loss=214, model_time=108]   0%|          | 75/96139 [08:16<138:07:33,  5.18s/it, avg_loss=169, data_time=389, grad_norm=5.02e+5, iter=74, loss=214, model_time=108]  0%|          | 75/96139 [08:22<138:07:33,  5.18s/it, avg_loss=169, data_time=394, grad_norm=2.26e+7, iter=75, loss=199, model_time=109]  0%|          | 76/96139 [08:22<144:34:01,  5.42s/it, avg_loss=169, data_time=394, grad_norm=2.26e+7, iter=75, loss=199, model_time=109]  0%|          | 76/96139 [08:29<144:34:01,  5.42s/it, avg_loss=170, data_time=399, grad_norm=3.86e+8, iter=76, loss=302, model_time=110]  0%|          | 77/96139 [08:29<151:33:17,  5.68s/it, avg_loss=170, data_time=399, grad_norm=3.86e+8, iter=76, loss=302, model_time=110]  0%|          | 77/96139 [08:34<151:33:17,  5.68s/it, avg_loss=170, data_time=404, grad_norm=3.65e+5, iter=77, loss=81.6, model_time=111]  0%|          | 78/96139 [08:34<149:54:06,  5.62s/it, avg_loss=170, data_time=404, grad_norm=3.65e+5, iter=77, loss=81.6, model_time=111]  0%|          | 78/96139 [08:40<149:54:06,  5.62s/it, avg_loss=172, data_time=408, grad_norm=4.77e+7, iter=78, loss=459, model_time=112]   0%|          | 79/96139 [08:40<153:48:33,  5.76s/it, avg_loss=172, data_time=408, grad_norm=4.77e+7, iter=78, loss=459, model_time=112]  0%|          | 79/96139 [08:47<153:48:33,  5.76s/it, avg_loss=176, data_time=414, grad_norm=6.44e+9, iter=79, loss=537, model_time=114]  0%|          | 80/96139 [08:47<163:22:34,  6.12s/it, avg_loss=176, data_time=414, grad_norm=6.44e+9, iter=79, loss=537, model_time=114]  0%|          | 80/96139 [08:53<163:22:34,  6.12s/it, avg_loss=175, data_time=418, grad_norm=7.22e+5, iter=80, loss=97.4, model_time=115]  0%|          | 81/96139 [08:53<158:20:07,  5.93s/it, avg_loss=175, data_time=418, grad_norm=7.22e+5, iter=80, loss=97.4, model_time=115]  0%|          | 81/96139 [08:59<158:20:07,  5.93s/it, avg_loss=177, data_time=423, grad_norm=4.79e+8, iter=81, loss=346, model_time=117]   0%|          | 82/96139 [08:59<164:53:03,  6.18s/it, avg_loss=177, data_time=423, grad_norm=4.79e+8, iter=81, loss=346, model_time=117]  0%|          | 82/96139 [09:07<164:53:03,  6.18s/it, avg_loss=179, data_time=429, grad_norm=9.33e+9, iter=82, loss=365, model_time=118]  0%|          | 83/96139 [09:07<176:53:24,  6.63s/it, avg_loss=179, data_time=429, grad_norm=9.33e+9, iter=82, loss=365, model_time=118]  0%|          | 83/96139 [09:13<176:53:24,  6.63s/it, avg_loss=181, data_time=434, grad_norm=1.69e+6, iter=83, loss=343, model_time=120]  0%|          | 84/96139 [09:13<174:44:50,  6.55s/it, avg_loss=181, data_time=434, grad_norm=1.69e+6, iter=83, loss=343, model_time=120]  0%|          | 84/96139 [09:21<174:44:50,  6.55s/it, avg_loss=185, data_time=440, grad_norm=1.39e+15, iter=84, loss=622, model_time=122]  0%|          | 85/96139 [09:21<186:20:37,  6.98s/it, avg_loss=185, data_time=440, grad_norm=1.39e+15, iter=84, loss=622, model_time=122]  0%|          | 85/96139 [09:27<186:20:37,  6.98s/it, avg_loss=185, data_time=445, grad_norm=5.08e+8, iter=85, loss=159, model_time=123]   0%|          | 86/96139 [09:27<178:55:06,  6.71s/it, avg_loss=185, data_time=445, grad_norm=5.08e+8, iter=85, loss=159, model_time=123]  0%|          | 86/96139 [09:34<178:55:06,  6.71s/it, avg_loss=186, data_time=450, grad_norm=2.13e+10, iter=86, loss=264, model_time=124]  0%|          | 87/96139 [09:34<178:04:05,  6.67s/it, avg_loss=186, data_time=450, grad_norm=2.13e+10, iter=86, loss=264, model_time=124]  0%|          | 87/96139 [09:43<178:04:05,  6.67s/it, avg_loss=184, data_time=455, grad_norm=85.8, iter=87, loss=54.3, model_time=129]     0%|          | 88/96139 [09:43<199:44:52,  7.49s/it, avg_loss=184, data_time=455, grad_norm=85.8, iter=87, loss=54.3, model_time=129]  0%|          | 88/96139 [09:50<199:44:52,  7.49s/it, avg_loss=185, data_time=461, grad_norm=2.16e+9, iter=88, loss=224, model_time=130]  0%|          | 89/96139 [09:50<193:39:45,  7.26s/it, avg_loss=185, data_time=461, grad_norm=2.16e+9, iter=88, loss=224, model_time=130]sys:1: RuntimeWarning: Traceback of forward call that caused the error:
  File "train.py", line 302, in <module>
    run(config)
  File "train.py", line 214, in run
    run_state = run_epoch(model, optimizer, train_ldr, logger, *run_state)
  File "train.py", line 55, in run_epoch
    loss = model.loss(temp_batch)
  File "/home/dzubke/awni_speech/speech/speech/models/ctc_model_train.py", line 53, in loss
    out, rnn_args = self.forward_impl(x)
  File "/home/dzubke/awni_speech/speech/speech/models/ctc_model_train.py", line 45, in forward_impl
    x, rnn_args = self.encode(x, rnn_args)
  File "/home/dzubke/awni_speech/speech/speech/models/model.py", line 84, in encode
    x = self.conv(x)
  File "/home/dzubke/miniconda3/envs/awni_env36/lib/python3.6/site-packages/torch/nn/modules/module.py", line 477, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/dzubke/miniconda3/envs/awni_env36/lib/python3.6/site-packages/torch/nn/modules/container.py", line 91, in forward
    input = module(input)
  File "/home/dzubke/miniconda3/envs/awni_env36/lib/python3.6/site-packages/torch/nn/modules/module.py", line 477, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/dzubke/miniconda3/envs/awni_env36/lib/python3.6/site-packages/torch/nn/modules/batchnorm.py", line 66, in forward
    exponential_average_factor, self.eps)
  File "/home/dzubke/miniconda3/envs/awni_env36/lib/python3.6/site-packages/torch/nn/functional.py", line 1254, in batch_norm
    training, momentum, eps, torch.backends.cudnn.enabled

Succesfully loaded weights from trained model
====== Model, loaders, optimimzer created =======
model: CTC_train(
  (conv): Sequential(
    (0): Conv2d(1, 32, kernel_size=(11, 41), stride=(1, 2), padding=(0, 20))
    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): Dropout(p=0.4)
    (4): Conv2d(32, 32, kernel_size=(11, 21), stride=(1, 2), padding=(0, 10))
    (5): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): ReLU()
    (7): Dropout(p=0.4)
    (8): Conv2d(32, 96, kernel_size=(11, 21), stride=(1, 1), padding=(0, 10))
    (9): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (10): ReLU()
    (11): Dropout(p=0.4)
  )
  (rnn): LSTM(6240, 512, num_layers=5, batch_first=True, dropout=0.4)
  (fc): LinearND(
    (fc): Linear(in_features=512, out_features=40, bias=True)
  )
)
preproc: Showing limited attributes as not all new attributes are supported

_input_dim: 257
start_and_end: False
int_to_char: {0: 'w', 1: 'er', 2: 'y', 3: 'ae', 4: 'd', 5: 'th', 6: 'v', 7: 't', 8: 'ao', 9: 'f', 10: 'ah', 11: 'z', 12: 'ey', 13: 'r', 14: 'm', 15: 'dh', 16: 'jh', 17: 'k', 18: 'sh', 19: 'uw', 20: 'aw', 21: 'n', 22: 'ch', 23: 'oy', 24: 'ow', 25: 'zh', 26: 'g', 27: 'uh', 28: 'aa', 29: 's', 30: 'hh', 31: 'iy', 32: 'l', 33: 'ih', 34: 'eh', 35: 'p', 36: 'b', 37: 'ng', 38: 'ay'}
char_to_int: {'w': 0, 'er': 1, 'y': 2, 'ae': 3, 'd': 4, 'th': 5, 'v': 6, 't': 7, 'ao': 8, 'f': 9, 'ah': 10, 'z': 11, 'ey': 12, 'r': 13, 'm': 14, 'dh': 15, 'jh': 16, 'k': 17, 'sh': 18, 'uw': 19, 'aw': 20, 'n': 21, 'ch': 22, 'oy': 23, 'ow': 24, 'zh': 25, 'g': 26, 'uh': 27, 'aa': 28, 's': 29, 'hh': 30, 'iy': 31, 'l': 32, 'ih': 33, 'eh': 34, 'p': 35, 'b': 36, 'ng': 37, 'ay': 38}
optimizer: SGD (
Parameter Group 0
    dampening: 0.98
    initial_lr: 0.0008
    lr: 0.0008
    momentum: 0.9
    nesterov: False
    weight_decay: 0
)
learning rate: 0.0008
Traceback (most recent call last):
  File "train.py", line 302, in <module>
    run(config)
  File "train.py", line 214, in run
    run_state = run_epoch(model, optimizer, train_ldr, logger, *run_state)
  File "train.py", line 59, in run_epoch
    loss.backward()
  File "/home/dzubke/miniconda3/envs/awni_env36/lib/python3.6/site-packages/torch/tensor.py", line 93, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph)
  File "/home/dzubke/miniconda3/envs/awni_env36/lib/python3.6/site-packages/torch/autograd/__init__.py", line 90, in backward
    allow_unreachable=True)  # allow_unreachable flag
RuntimeError: Function 'CudnnBatchNormBackward' returned nan values in its 0th output.
  0%|          | 89/96139 [10:04<181:11:37,  6.79s/it, avg_loss=185, data_time=461, grad_norm=2.16e+9, iter=88, loss=224, model_time=130]