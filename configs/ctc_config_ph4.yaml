seed: 2017
save_path: /home/dzubke/awni_speech/speech-lfs/examples/librispeech/models/ctc_models/null
data:
    train_set: /mnt/disks/data_disk/data/speak_test_data/2020-05-27/speak-test_2020-05-27.json #/mnt/disks/data_disk/data/lib-ted-cv-spk-swb/train_lib-ted-cv-spk250K-swb-2021-01-26.json
    dev_sets:
        speak: /mnt/disks/data_disk/data/speak_test_data/2020-05-27/speak-test_2020-05-27.json
        cv: /mnt/disks/data_disk/data/common-voice/v5.1_2020-06-22/dev.json
        libsp: /mnt/disks/data_disk/data/LibriSpeech/dev-clean.json
        ted: /mnt/disks/data_disk/data/tedlium/TEDLIUM_release-3/dev.json
    dev_set_save_reference: speak
    start_and_end: False
    num_workers: 4
logger:
    use_log: False
    log_file: logs/2021-01-06_large-mid-aug-no-fn-naren-no-amp_spk-100K-2021-01-05.log
    level: error
    debug_mode: False
preproc:
    preprocessor: log_spectrogram
    blank_idx: last               # must be either 'first' for blank=0, or 'last' for blank=vocab_size
    window_size: 32
    step_size: 16
    use_feature_normalize: False
    augment_from_normal: True
    tempo_gain_pitch_perturb: True
    tempo_gain_pitch_prob: 0.5
    tempo_range: [0.80, 1.20]
    gain_range: [-5, 5]
    pitch_range: [-250, 250]
    synthetic_gaussian_noise: True
    gauss_noise_prob: 0.5
    gauss_snr_db_range: [15, 50]
    background_noise: True
    background_noise_dir: /mnt/disks/data_disk/data/noise/feed_to_model/
    background_noise_prob: 0.5
    background_noise_range: [0.0, 0.5]
    spec_augment: True
    spec_augment_prob: 0.5
    spec_augment_policy:
        0: {W: 20, F: 30, m_F: 1, T: 18, m_T: 1}
        1: {W: 20, F: 15, m_F: 2, T: 9, m_T: 2}
        2: {W: 20, F: 10, m_F: 3, T: 6, m_T: 3}
optimizer:
    batch_size: 9
    run_state: [0, 0]
    best_so_far: 0.40
    start_epoch: 0
    epochs: 5
    learning_rate: 0.0003
    momentum: 0.9
    dampening: 0.0
    sched_gamma: 1.0
    sched_step: 1
model:
    dropout: 0.4
    load_trained: False
    trained_path: examples/librispeech/models/ctc_models/20200806/ph4/best_model_state_dict.pth
    remove_layers: []
    encoder: 
        conv:
            - [64, 11, 41, 1, 2, 0, 20]
            - [64, 11, 21, 1, 2, 0, 10]
            - [128, 11, 21, 1, 1, 0, 10]
        rnn: 
            type: LSTM
            dim: 1024
            bidirectional: False
            layers: 5
training:
    loss_name: naren
    amp: False                 # turns on Pytorch's automatic mixed-precision
    checkpoints_per_epoch: 1
    OMP_NUM_THREADS: 5        # this should be equal to:  #_CPU / #_GPU
