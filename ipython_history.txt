 1/1: import six
 3/1: import protobuf
 3/2: import cffi
 4/1: import sys
 4/2: print(sys.path)
 5/1: import cffi
 6/1: import numpy
 6/2: import sys
 6/3: print(sys.path)
 7/1: import protobuf
 7/2: import protobuf-3.4.0
 8/1: import pytest
 9/1: import tensorboard-logger
 9/2: import tensorboard
 9/3: import tensorboard_logger
11/1: import glob
11/2: wav_f = *.wav
11/3: wav_f = glob.glob(*.wav)
11/4: wav_f = glob.glob('./*.wav')
11/5: wav_f
11/6: wav_f = glob.glob('/Users/dustin/CS/consulting/firstlayerai/data/timit/*/*/*/*.wav')
11/7: wav_f
11/8: wav_f = glob.glob('/Users/dustin/CS/consulting/firstlayerai/data/timit/*/*/*.wav')
11/9: wav_f
11/10: wav_f = glob.glob('/Users/dustin/CS/consulting/firstlayerai/data/timit/*/*/*/*.WAV')
11/11: wav_f
12/1: import sys
12/2: print(sys.path)
13/1: import sys
13/2: print(sys.path)
14/1: import sys
14/2: import sys
14/3: print(sys.path)
14/4: train_path = '/Users/dustin/CS/consulting/firstlayerai/data/timit/TRAIN'
14/5: import os
14/6: pattern = os.path.join(path, "*/*/*.phn")
14/7: pattern = os.path.join(train_path, "*/*/*.phn")
14/8: import glob
14/9: files = glob.glob(pattern)
14/10: files
14/11: pattern = os.path.join(train_path, "*/*/*.PHN")
14/12: pattern = os.path.join(train_path, "*/*/*.PHN")
14/13: files = glob.glob(pattern)
14/14: files
15/1: l = [1,2,3,4]
15/2: l.drop(0)
15/3: st = 'hello'
15/4: st.upper()
15/5: st
15/6: st = st.upper()
15/7: st
16/1: import tensorflow as tf
17/1: import tensorflow as tf
17/2: from tensorflow.core.util import event_pb2
18/1: eval('string1' + 'strin2')
19/1: l1 = [[1,2,3],[4,5,6]]
19/2: [i for l in l1 for i in l]
19/3:
[range(i, i + batch_size)
                for i in range(0, 50, 5)]
19/4:
[range(i, i + 5)
                for i in range(0, 50, 5)]
19/5: m = [range(i, i + 5) for i in range(0, 50, 5)]
19/6: n = (i for b in m for i in b)
19/7: n
19/8: print(n)
19/9: [i for i in n]
19/10: import random
19/11: random.shuffle(m)
19/12: n = (i for b in m for i in b)
19/13: print(n)
19/14: [i for i in n]
19/15: x = lambda batch: zip(*batch)
19/16: lst = [i for i in n]
19/17: sampler = [i for i in n]
19/18: sampler
19/19: n = (i for b in m for i in b)
19/20: n
19/21: sampler = [i for i in n]
19/22: sampler
19/23: x(batch)
19/24: x(sampler)
19/25: l = [1,2,3]
19/26: pickle_path = "/Users/dustin/CS/consulting/firstlayerai/phoneme_classification/src/awni_speech/speech/examples/timit/models/model_pickle"
19/27: import pickle
19/28:
with open(pickle_path,'wb') as model_fn:
    pickle.dump(l, model_fn)
19/29: pickle_path = "/Users/dustin/CS/consulting/firstlayerai/phoneme_classification/src/awni_speech/speech/examples/timit/models/model_pickle/"
19/30:
with open(pickle_path,'wb') as model_fn:
    pickle.dump(l, model_fn)
19/31: pickle_path = "/Users/dustin/CS/consulting/firstlayerai/phoneme_classification/src/awni_speech/speech/examples/timit/models/model_pickle"
19/32:
with open(pickle_path,'wb') as model_fn:
    pickle.dump(l, model_fn)
19/33: pickle_path = "Users/dustin/CS/consulting/firstlayerai/phoneme_classification/src/awni_speech/speech/examples/timit/models/model_pickle/"
19/34:
with open(pickle_path,'wb') as model_fn:
    pickle.dump(l, model_fn)
19/35: pickle_path = "Users/dustin/CS/consulting/firstlayerai/phoneme_classification/src/awni_speech/speech/examples/timit/models/model_pickle/model_fit.pickle"
19/36:
with open(pickle_path,'wb') as model_fn:
    pickle.dump(l, model_fn)
19/37: pickle_path = "/Users/dustin/CS/consulting/firstlayerai/phoneme_classification/src/awni_speech/speech/examples/timit/models/model_pickle/model_fit.pickle"
19/38:
with open(pickle_path,'wb') as model_fn:
    pickle.dump(l, model_fn)
19/39:
with open(pickle_path,'rb') as model_fn:
    m = pickle.load(model_fn)
19/40: m
19/41:
if true and true: 
    print(6)
19/42:
if true and true: 
    print(6)
19/43: labels=[[4, 36, 14, 45, 34, 28, 20, 14, 45, 32, 43, 23, 38, 8, 29, 43, 3, 23, 24, 46, 29, 20, 44, 34, 20, 14, 20, 4], [4, 46, 32, 43, 39, 23, 24, 9, 34, 11, 29, 42, 27, 22, 38, 20, 35, 6, 32, 45, 2, 34, 11, 33, 20, 34, 7, 4], [4, 45, 26, 34, 7, 20, 32, 41, 15, 23, 13, 30, 17, 18, 19, 22, 36, 14, 18, 20, 24, 43, 7, 16, 42, 25, 20, 4], [4, 46, 32, 43, 3, 45, 17, 43, 40, 38, 41, 45, 17, 35, 18, 29, 46, 32, 36, 16, 43, 3, 27, 45, 43, 27, 5, 4], [4, 37, 1, 42, 32, 34, 7, 26, 30, 43, 3, 12, 15, 41, 34, 7, 32, 5, 34, 28, 45, 38, 2, 34, 39, 15, 29, 22, 4], [4, 46, 32, 43, 3, 45, 17, 43, 40, 38, 41, 45, 17, 35, 18, 29, 46, 32, 36, 16, 43, 3, 27, 45, 43, 39, 44, 9, 4], [4, 39, 35, 5, 33, 20, 34, 30, 27, 34, 11, 14, 23, 44, 5, 12, 45, 44, 23, 33, 43, 45, 16, 34, 11, 46, 33, 34, 4], [4, 28, 14, 9, 37, 44, 5, 22, 21, 45, 44, 18, 5, 19, 29, 46, 24, 16, 20, 19, 33, 5, 26, 23, 42, 44, 34, 1]]
19/44: len(labels)
19/45: len(labels[0])
19/46: len(labels[1])
19/47: len(labels[2])
19/48: len(labels[3])
19/49:
for ind in range(len(labels)-1):
    if not (len(labels[ind]) == len(labels[ind+1])):
        print("not the same")
19/50:
for ind in range(len(labels)-1):
    if not (len(labels[ind]) == len(labels[ind+1])):
        print("not the same")
        print(ind)
19/51: len(labels[4])
19/52: len(labels[5])
19/53: len(labels[6])
19/54: len(labels[7])
19/55: l = [1,2,4, 5, 3, 9, 7, 6]
20/1: import editdistance as ed
20/2: import editdistance as ed
21/1: import editdistance as ed
21/2: editdistance.eval('kitten', 'sitting')
21/3: ed.eval('kitten', 'sitting')
21/4: ed.eval('Saturday', 'Sunday')
21/5: len('kitten')
22/1: x=1
22/2: print(f"x:{x}")
23/1: l = [10, 20, 20, 10, 10, 30, 50, 10, 20]
23/2: sort_l = l.sort()
23/3: sort_l
23/4: print(sort_l)
23/5: l
23/6:
for i in len(l):
    if l[i] == l[i+1]:
        print(f"li: {l[i]}, li+1: {l[i+1]}")
        l.pop(0)
        l.pop(0)
23/7:
for i in range(len(l)):
    if l[i] == l[i+1]:
        print(f"li: {l[i]}, li+1: {l[i+1]}")
        l.pop(0)
        l.pop(0)
23/8: i = 0
23/9: count = 0
23/10:
while i < len(l): 
    if l[i] == l[i+1]:
        i += 2
        count += 1
   else:
23/12:
while i < len(l): 
    if l[i] == l[i+1]:
        i += 2
        count += 1
23/13:
while i < len(l): 
    if l[i] == l[i+1]:
        i += 2
        count += 1
    else: 
        i +=1
23/14: m = 1
23/15: m+=1
23/16: m
23/17: m
23/18:
while i < len(l)-1: 
    if l[i] == l[i+1]:
        i += 2
        count += 1
    else: 
        i +=1
23/19: count
23/20: count = 0
23/21: l
23/22: l.append(10)
23/23: l
23/24: l.extend([10,10,10])
23/25: l
23/26: l.sort()
23/27: ls
23/28: l
23/29:
while i < len(l)-1: 
    if l[i] == l[i+1]:
        i += 2
        count += 1
    else: 
        i +=1
23/30: count
23/31: count = 0
23/32: l
23/33:
while i < len(l)-1: 
    if l[i] == l[i+1]:
        i += 2
        count += 1
    else: 
        i +=1
23/34: count
23/35: i
23/36: i = 0
23/37:
while i < len(l)-1: 
    if l[i] == l[i+1]:
        i += 2
        count += 1
    else: 
        i +=1
23/38: count
23/39: l
23/40: m
23/41: m -= 1
23/42: m
24/1: ls
24/2: import loader
24/3: logspec = loader.log_specgram_from_file( '/Users/dustin/CS/consulting/firstlayerai/data/dustin_test_data/1-plane.m4a')
24/4: logspec = loader.log_specgram_from_file( '/Users/dustin/CS/consulting/firstlayerai/data/dustin_test_data/1-plane.wv')
24/5: logspec = loader.log_specgram_from_file( '/Users/dustin/CS/consulting/firstlayerai/data/dustin_test_data/2-plane.wv')
24/6: logspec = loader.log_specgram_from_file( '/Users/dustin/CS/consulting/firstlayerai/data/dustin_test_data/3-plane.wv')
24/7: logspec = loader.log_specgram_from_file( '/Users/dustin/CS/consulting/firstlayerai/data/dustin_test_data/4-plane.wv')
24/8: logspec = loader.log_specgram_from_file( '/Users/dustin/CS/consulting/firstlayerai/data/dustin_test_data/5-plane.wv')
24/9: logspec = loader.log_specgram_from_file( '/Users/dustin/CS/consulting/firstlayerai/data/dustin_test_data/6-plane.wv')
24/10: logspec = loader.log_specgram_from_file( '/Users/dustin/CS/consulting/firstlayerai/data/dustin_test_data/10-plane.wv')
24/11: logspec = loader.log_specgram_from_file( '/Users/dustin/CS/consulting/firstlayerai/data/timit/TEST/DR1/FAKS0/SA1.wv')
26/1: import loader
26/2: loader.log_specgram_from_file('/Users/dustin/CS/consulting/firstlayerai/data/dustin_test_data/10-plane.wv')
26/3: loader.log_specgram_from_file('/Users/dustin/CS/consulting/firstlayerai/data/dustin_test_data/11-plane.wv')
26/4: loader.log_specgram_from_file('/Users/dustin/CS/consulting/firstlayerai/data/dustin_test_data/1-plane.wv')
26/5: loader.log_specgram_from_file('/Users/dustin/CS/consulting/firstlayerai/data/timit/TEST/DR1/FAKS0/SA1.wv)
26/6: loader.log_specgram_from_file('/Users/dustin/CS/consulting/firstlayerai/data/timit/TEST/DR1/FAKS0/SA1.wv')
27/1: import loader
27/2: loader.log_specgram_from_file('/Users/dustin/CS/consulting/firstlayerai/data/timit/TEST/DR1/FAKS0/SA1.wv')
27/3: loader.log_specgram_from_file('/Users/dustin/CS/consulting/firstlayerai/data/dustin_test_data/1-plane.wv')
27/4: loader.log_specgram_from_file('/Users/dustin/CS/consulting/firstlayerai/data/dustin_test_data/10-plane.wv')
28/1: import loader
28/2: loader.log_specgram_from_file('/Users/dustin/CS/consulting/firstlayerai/data/timit/TEST/DR1/FAKS0/SA1.wv')
28/3: loader.log_specgram_from_file('/Users/dustin/CS/consulting/firstlayerai/data/dustin_test_data/10-plane.wv')
29/1: s = 'abcac'
29/2: s.count('a')
29/3: div, rem = 10 % 3
29/4: div, rem = divmod(10,3)
29/5: div
29/6: rem
29/7: [i for i in range(0)]
29/8: [i for i in range(1)]
29/9: [i for i in range(3-2)]
29/10: l=[1,2,3]
29/11: l*l
29/12: l*.l
29/13: l.*l
29/14: [a*b for a,b in zip(l,l)]
29/15: sum([a*b for a,b in zip(l,l)])
29/16: i = int("inf")
29/17: i
29/18: i = float("inf")
29/19: i
29/20: i = -float("inf")
29/21: i
30/1: l = [1,2,3]
30/2: m = [val,ind for ind, val in enmuerate(l)]
30/3: m = [val,ind for ind, val in enmuerate(l)]
30/4: m = enumerate(l)
30/5: m
30/6:
for ind, val in m:
    print(ind, val)
30/7: m = [val,ind for ind, val in enumerate(l)]
30/8: m = [val, ind for ind, val in enumerate(l)]
30/9: n = [i for i in l]
30/10: n
30/11: [i,j for i,j in enumerate(l)]
30/12: [i+j for i,j in enumerate(l)]
30/13: 1 !=5
30/14: l
30/15: l = [1,-2,-3]
30/16: abs(l)
30/17: sum([abs(i) for i in l])
30/18: l
30/19: [(ind,val) for ind, val in enumerate(l)]
30/20: b = sorted(l)
30/21: b
30/22:     b
30/23: q, w, e = b
30/24: q
30/25: b
30/26: b += 5
30/27: b += [5,5,5]
30/28: b
30/29: b =[1,2,3]
30/30: b
30/31: b = b .+ [5,5,5]
30/32: 5 <5
30/33: 5<=5
30/34: listOfStrings = ['Hi' , 'hello', 'at', 'this', 'there', 'from']
30/35: listOfStrings.index('Hi')
30/36: listOfStrings.index('hello')
31/1: cd utils/
31/2: ls
31/3: import score
31/4: compute_cer([["hh", "ah", "l", "ow"],["hh", "th", "hh", "eh", "l", "ow"]])
31/5: score.compute_cer([["hh", "ah", "l", "ow"],["hh", "th", "hh", "eh", "l", "ow"]])
30/37: listOfStrings = ['Hi' , 'hello', 'at', 'this', 'there', 'from', 'from']
30/38: listOfStrings.index('from')
30/39: listOfStrings.index('from')
30/40: listOfStrings.index('from')
30/41: listOfStrings.pop('from')
30/42: listOfStrings.pop(5)
30/43: listOfStrings
30/44: listOfStrings.pop(listOfStrings.index('from'))
30/45: listOfStrings
30/46: listOfStrings.pop(listOfStrings.index('from'))
30/47: 'from' in listOfStrings
30/48: 'from' not in listOfStrings
30/49: s = "Hello"
30/50: s[::-1]
30/51: l
30/52: l.insert(0,5)
30/53: l
30/54: l.insert(len(l),6)
30/55: l
30/56: l
30/57: l.count(5)
30/58: dct = {i:l.count(i) for i in l}
30/59: dct
30/60: 5 in dct
30/61: 7 in dct
30/62: dct[5] -= 1
30/63: dct
30/64: l
30/65: l.pop()
30/66: m = "Hello"
30/67: m[0]
30/68: m[0] = "J"
30/69: ls
32/1: import loader
32/2: spec = loader.log_specgram_from_file("/Users/dustin/CS/consulting/firstlayerai/data/dustin_test_data/20191203/timit_subset/SI943.wv")
32/3: import loader
32/4: spec = loader.log_specgram_from_file("/Users/dustin/CS/consulting/firstlayerai/data/dustin_test_data/20191203/timit_subset/SI943.wv")
33/1: import loader
33/2: spec = loader.log_specgram_from_file("/Users/dustin/CS/consulting/firstlayerai/data/dustin_test_data/20191203/timit_subset/SI943.wv")
33/3: [i for i in range(0,0)]
34/1: import editdistance
34/2: labels = ["dh", "ah", "r", "ey"]
34/3: preds_1 = ["dh", "mn", "r", "ey"]
34/4: import editdistance as ed
34/5: dis_1 = ed.eval(labels, preds_1)
34/6: dis_1
34/7: dis_1 = ed.eval(preds_1, labels)
34/8: dis_1
34/9: preds_2 = ["dh", "an", "r", "ey"]
34/10: dis_2 = ed.eval(preds_2, labels)
34/11: dis_2
34/12: ed.eval(['spam', 'egg'], ['spam', 'ham'])
34/13: ed.eval('egg', 'ham')
34/14: ed.eval(["dh", "ah", "r"], ["dh", "mn", "r"])
34/15: ed.eval(["dh", "ah", "r"], ["dh", "an", "r"])
34/16: ed.eval(["dh", "ah", "r"], ["ah", "dh", "r"])
34/17: ed.eval(["dh", "ah", "r"], ["ah", "dh", "r"]) # = 2
34/18: ed.eval(["dh", "ah", "r"], ["d"dh", "ah", "r"]) # = 2
34/19: ed.eval(["dh", "ah", "r"], ["d", "dh", "ah", "r"])
34/20: ed.eval(["dh", "ah", "r"], ["space", "dh", "ah", "r"])
34/21: ed.eval(["space", "dh", "ah", "r"], ["dh", "ah", "r"])
34/22: ed.eval(["dh", "ah", "r", "space"], ["dh", "ah", "r"])
34/23: ed.eval(["dh", "ah","space", "r"], ["dh", "ah", "r"])
34/24: results = (["dh", "ah","space", "r"], ["dh", "ah", "r"])
34/25:
dist = sum(editdistance.eval(label, pred)
                for label, pred in results)
34/26: [label, pred for label, pred in results]
34/27: [label+pred for label, pred in results]
34/28: results = [["dh", "ah","space", "r"], ["dh", "ah", "r"]]
34/29:
dist = sum(editdistance.eval(label, pred)
                for label, pred in results)
34/30: [label+pred for label, pred in results]
34/31: results = [(["dh", "ah","space", "r"], ["dh", "ah", "r"])]
34/32:
dist = sum(editdistance.eval(label, pred)
                for label, pred in results)
34/33: dist
34/34: total = sum(len(label) for label, _ in results)
34/35: total
34/36: print(dist/total)
34/37: ed.eval(["hh", "ah", "l", "ow"], ["hh", "th", "hh", "eh", "l", "ow"])
34/38: ed.eval( ["hh", "th", "hh", "eh", "l", "ow"], ["hh", "ah", "l", "ow"])
29/22: bool("False")
29/23: bool("false")
29/24: bool(false)
29/25: bool(0)
35/1: cd utils/
35/2: ls
35/3: cd ..
35/4: ls
35/5: import loader
35/6: arr = loader.log_specgram_from_file("/Users/dustin/CS/consulting/firstlayerai/data/dustin_test_data/20191203/20191204_subset/SI943-drz-16k-16bit.wv")
35/7: arr = loader.log_specgram_from_file("/Users/dustin/CS/consulting/firstlayerai/data/dustin_test_data/20191203/20191204_subset/SI943-drz-16k-16bit.wv")
36/1: import loader
36/2: arr = loader.log_specgram_from_file("/Users/dustin/CS/consulting/firstlayerai/data/dustin_test_data/20191203/20191204_subset/SI943-drz-16k-16bit.wv")
37/1: import loader
37/2: arr = loader.log_specgram_from_file("/Users/dustin/CS/consulting/firstlayerai/data/dustin_test_data/20191203/20191204_subset/SI943-drz-16k-16bit.wv")
37/3: import numpy as np
37/4: a1 = np.array([1,2], [3,4], [5,6])
37/5: a1 = np.array([[1,2], [3,4], [5,6]])
37/6: a1
37/7: a1.shape
37/8: a1[0]
37/9: a1[:][0]
37/10: a1[:,0]
38/1: import loader
38/2: arr = loader.log_specgram_from_file("/Users/dustin/CS/consulting/firstlayerai/data/dustin_test_data/20191203/20191204_subset/SI943-drz-16k-16bit.wv")
39/1: import loader
29/26: m
29/27: m = 5
29/28: assert m==5
29/29: assert m==4, "not 5"
29/30: assert m==5, "not 5"
29/31: assert m==5 or m==4, "not 5"
29/32: assert m==3 or m==4, "not 5"
29/33: assert m==3 or m==5, "not 5"
29/34: arr
39/2: arr
39/3: import numpy as np
39/4: arr = np.array([[1,2],[3,4],[5,6]])
39/5: arr
39/6: arr.shape
40/1: import loader
40/2: arr = loader.log_specgram_from_file("/Users/dustin/CS/consulting/firstlayerai/data/dustin_test_data/20191203/20191204_subset/SI943-drz-16k-16bit.wv", channel=0)
40/3: arr = loader.log_specgram_from_file("/Users/dustin/CS/consulting/firstlayerai/data/dustin_test_data/20191203/20191204_subset/SI943-drz-16k-16bit.wv", channel=0)
40/4: import loader
40/5: arr = loader.log_specgram_from_file("/Users/dustin/CS/consulting/firstlayerai/data/dustin_test_data/20191203/20191204_subset/SI943-drz-16k-16bit.wv", channel=0)
41/1: import loader
41/2: arr = loader.log_specgram_from_file("/Users/dustin/CS/consulting/firstlayerai/data/dustin_test_data/20191203/20191204_subset/SI943-drz-16k-16bit.wv", channel=0)
41/3: arr = loader.log_specgram_from_file("/Users/dustin/CS/consulting/firstlayerai/data/dustin_test_data/20191203/20191204_subset/SI943-drz-16k-16bit.wv", channel=0, plot=True)
41/4: import loader
41/5: arr = loader.log_specgram_from_file("/Users/dustin/CS/consulting/firstlayerai/data/dustin_test_data/20191203/20191204_subset/SI943-drz-16k-16bit.wv", channel=0, plot=True)
42/1: import loader
42/2: arr = loader.log_specgram_from_file("/Users/dustin/CS/consulting/firstlayerai/data/dustin_test_data/20191203/20191204_subset/SI943-drz-16k-16bit.wv", channel=0, plot=True)
42/3: arr = loader.log_specgram_from_file("/Users/dustin/CS/consulting/firstlayerai/data/dustin_test_data/20191203/20191204_subset/SI943-drz-16k-16bit.wv", channel=1, plot=True)
42/4: arr = loader.log_specgram_from_file("/Users/dustin/CS/consulting/firstlayerai/data/dustin_test_data/20191203/20191204_subset/SI943-drz-16k-16bit.wv", channel=0, plot=True)
42/5: arr = loader.log_specgram_from_file("/Users/dustin/CS/consulting/firstlayerai/data/dustin_test_data/20191203/20191204_subset/SI943-drz-16k-16bit.wv", channel=1, plot=True)
43/1: import loader
43/2: arr = loader.log_specgram_from_file("/Users/dustin/CS/consulting/firstlayerai/data/dustin_test_data/20191203/20191204_subset/SI943-drz-16k-16bit.wv", channel=1, plot=True)
43/3: arr = loader.log_specgram_from_file("/Users/dustin/CS/consulting/firstlayerai/data/dustin_test_data/20191203/20191204_subset/SI943-drz-16k-16bit.wv", channel=1, plot=True)
44/1: import loader
44/2: arr = loader.log_specgram_from_file("/Users/dustin/CS/consulting/firstlayerai/data/dustin_test_data/20191203/20191204_subset/SI943-drz-16k-16bit.wv", channel=1, plot=True)
44/3: arr = loader.log_specgram_from_file("/Users/dustin/CS/consulting/firstlayerai/data/dustin_test_data/20191203/20191204_subset/SI943-drz-16k-16bit.wv")
29/35: tup = (1,)
29/36: tup
29/37: tup.shape
29/38: import numpy as np
29/39: len(tup)
29/40: tup2 = (1,2)
29/41: len(tup2)
29/42: tup2 = (1,5)
29/43: len(tup2)
29/44: arr = np.array([1,2,3])
29/45: arr.shape
29/46: len(arr.shape)
29/47: import json
29/48:
with open('/Users/dustin/CS/consulting/firstlayerai/phoneme_classification/src/awni_speech/speech/predictions/20191203_predicti_dist_39.json', 'r') as score_fid:
        score_json = [json.loads(l) for l in score_fid]
29/49: score_json
29/50: len(score_json)
29/51: score_json[0]
29/52: score_json[0]['label']
29/53:
lab1 = ['sil',
 'p',
 'er',
 'sil',
 'd',
 'ah',
 'sil',
 'k',
 'sh',
 'ih',
 'n',
 'm',
 'ey',
 'f',
 'aa',
 'l',
 'f',
 'aa',
 'r',
 'sil',
 'b',
 'ah',
 'l',
 'ow',
 'ah',
 'sil',
 'k',
 's',
 'sil',
 'p',
 'ih',
 'sil',
 't',
 'ey',
 'sh',
 'n',
 'sil',
 's',
 'sil']
29/54: lab1
29/55: lab1 in [score_json[i]['label'] for i in range(len(score_json))]
29/56: lab1 in score_json
29/57: type(score_json)
29/58: type(score_json[0])
29/59:
with open('/Users/dustin/CS/consulting/firstlayerai/data/dustin_test_data/20191203/20191203_test.json', 'r') as score_fid:
        test_json = [json.loads(l) for l in score_fid]
29/60: test_json
29/61: type(test_json)
29/62: test_json[0]
45/1: import heapq
45/2: heap = []
45/3: heapq.heapify(heap)
45/4: type(heap)
45/5: True && True
45/6: True & True
45/7: True and True
45/8: False and True
45/9: False | True
45/10: False | False
45/11: True | True
45/12: l = [1,2,3,4,4,5]
45/13: l.index(4)
45/14: l.index(4)
45/15: l.index(4)
45/16: l.index(4)
45/17: dct = {'a':4, 'b':3, 'c':8}
45/18: dct.contains('a')
45/19: 'a' in dct
45/20: a = 'abc'
45/21: b = 'bcd'
45/22: b > a
45/23: int(a)
45/24: float(a)
45/25: a -b
45/26: a.digits
45/27: a.digits()
45/28: l = [1,2,3,4]
45/29: l
45/30: a = l.pop()
45/31: a
45/32: l
45/33: [i for i in range(3,6)]
45/34: l
45/35: l+l
45/36: l.+l
46/1: import loader
46/2: log_specgram_from_file('/Users/dustin/CS/consulting/firstlayerai/data/dustin_test_data/20191203/batch1/SX43.wv', plot=True)
46/3: loader.log_specgram_from_file('/Users/dustin/CS/consulting/firstlayerai/data/dustin_test_data/20191203/batch1/SX43.wv', plot=True)
46/4: loader.log_specgram_from_file(filname, plot=True)
46/5: filname = '/Users/dustin/CS/consulting/firstlayerai/data/dustin_test_data/20191203/batch2/SX43-drz-16k-16bit.wv'
46/6: loader.log_specgram_from_file(filname, plot=True)
46/7: loader.log_specgram_from_file('/Users/dustin/CS/consulting/firstlayerai/data/dustin_test_data/20191203/batch1/SX43.wv', plot=True)
46/8: file_b1 = '/Users/dustin/CS/consulting/firstlayerai/data/dustin_test_data/20191203/batch1/SX43.wv'
46/9: file_b2 = '/Users/dustin/CS/consulting/firstlayerai/data/dustin_test_data/20191203/batch2/SX43-drz-16k-16bit.wv'
46/10: file_b3 = '/Users/dustin/CS/consulting/firstlayerai/data/dustin_test_data/20191203/batch3/SX43-drz-org.wv'
46/11: file_b4 = '/Users/dustin/CS/consulting/firstlayerai/data/dustin_test_data/20191203/batch4/SX43-drz-48k-16bit.wv'
46/12: loader.log_specgram_from_file(file_b1, plot=True)
46/13: loader.log_specgram_from_file(file_b2, plot=True)
46/14: loader.log_specgram_from_file(file_b3, plot=True)
46/15: loader.log_specgram_from_file(file_b4, plot=True)
46/16: loader.log_specgram_from_file(file_b3, plot=True)
47/1: import loader
48/1: import loader
48/2: loader.check_avconv()
48/3: cd utils/
48/4: import convert
48/5: convert.check_avconv()
48/6: convert.check_ffmpeg()
48/7: loader.log_specgram_from_file('/Users/dustin/CS/consulting/firstlayerai/data/dustin_test_data/20191203/batch2/SX43-drz-16k-16bit.wav', plot=True)
49/1: import loader
49/2: spec_diff = loader.compare_log_spec_from_file('/Users/dustin/CS/consulting/firstlayerai/data/dustin_test_data/20191203/batch2/SX43-drz-16k-16bit.wav', '/Users/dustin/CS/consulting/firstlayerai/data/dustin_test_data/20191203/batch2/SX43-drz-16k-16bit.wv', plot=True)
50/1: import loader
50/2: spec_diff = loader.compare_log_spec_from_file('/Users/dustin/CS/consulting/firstlayerai/data/dustin_test_data/20191203/batch2/SX43-drz-16k-16bit.wav', '/Users/dustin/CS/consulting/firstlayerai/data/dustin_test_data/20191203/batch2/SX43-drz-16k-16bit.wv', plot=True)
51/1: import loader
51/2: spec_diff = loader.compare_log_spec_from_file('/Users/dustin/CS/consulting/firstlayerai/data/dustin_test_data/20191203/batch2/SX43-drz-16k-16bit.wav', '/Users/dustin/CS/consulting/firstlayerai/data/dustin_test_data/20191203/batch2/SX43-drz-16k-16bit.wv', plot=True)
51/3: import numpy as np
51/4: np.sum(spec_diff)
52/1: import loader
52/2: spec_diff = loader.compare_log_spec_from_file('/Users/dustin/CS/consulting/firstlayerai/data/dustin_test_data/20191203/batch2/SX43-drz-16k-16bit.wav', '/Users/dustin/CS/consulting/firstlayerai/data/dustin_test_data/20191203/batch2/SX43-drz-16k-16bit.wv', plot=True)
52/3: spec_diff = loader.compare_log_spec_from_file('/Users/dustin/CS/consulting/firstlayerai/data/dustin_test_data/20191203/batch2/SX43-drz-16k-16bit.wav', '/Users/dustin/CS/consulting/firstlayerai/data/dustin_test_data/20191203/batch2/SX43-drz-16k-16bit.wv', plot=True)
53/1: import loader
53/2: spec_diff = loader.compare_log_spec_from_file('/Users/dustin/CS/consulting/firstlayerai/data/dustin_test_data/20191203/batch2/SX43-drz-16k-16bit.wav', '/Users/dustin/CS/consulting/firstlayerai/data/dustin_test_data/20191203/batch2/SX43-drz-16k-16bit.wv', plot=True)
53/3: spec_diff = loader.compare_log_spec_from_file('/Users/dustin/CS/consulting/firstlayerai/data/dustin_test_data/20191203/batch2/SX43-drz-16k-16bit.wav', '/Users/dustin/CS/consulting/firstlayerai/data/dustin_test_data/20191203/batch2/SX43-drz-16k-16bit.wv', plot=True)
53/4: spec_diff = loader.compare_log_spec_from_file('/Users/dustin/CS/consulting/firstlayerai/data/dustin_test_data/20191203/batch3/SI943-drz-org.m4a', '/Users/dustin/CS/consulting/firstlayerai/data/dustin_test_data/20191203/batch3/SI943-drz-org.wv', plot=True)
53/5: spec_diff = loader.compare_log_spec_from_file('/Users/dustin/CS/consulting/firstlayerai/data/dustin_test_data/20191203/batch4/SI943-drz-48k-16bit.wav', '/Users/dustin/CS/consulting/firstlayerai/data/dustin_test_data/20191203/batch4/SI943-drz-48k-16bit.wv', plot=True)
53/6: spec_diff = loader.compare_log_spec_from_file('/Users/dustin/CS/consulting/firstlayerai/data/dustin_test_data/20191203/batch1/SI943.wav', '/Users/dustin/CS/consulting/firstlayerai/data/dustin_test_data/20191203/batch1/SI943.wv', plot=True)
53/7: spec_diff = loader.compare_log_spec_from_file('/Users/dustin/CS/consulting/firstlayerai/data/dustin_test_data/20191203/batch1/SX43.wav', '/Users/dustin/CS/consulting/firstlayerai/data/dustin_test_data/20191203/batch1/SX43.wv', plot=True)
54/1: import numpy as np
54/2: arr = np.array([[1,2],[3,4]])
54/3: arr
54/4: zero = np.zeros((4,4))
54/5: arr.shape[0]
54/6: zero[:arr.shape[0], :arr.shape[1]] = arr
54/7: zero
54/8: zero[:arr.shape[0], :] = arr
54/9: zero2 = np.zeros((4,2))
54/10: zero[:arr.shape[0], :] = arr
54/11: zero2[:arr.shape[0], :] = arr
54/12: zero2
54/13: zero2[:, :arr.shape[1]] = arr
54/14: np.array([[1,2,3],[4,5,6],[7,8,9]])
54/15: array = np.array([[1,2,3],[4,5,6],[7,8,9]])
54/16:
for i in array:
    print(i)
54/17: m = [((3, 47, 12, 2, 34, 10, 20, 12, 10, 22, 16, 46, 44, 2, 28, 21, 13, 12, 5, 37, 44, 18, 40, 20, 45, 15, 10, 2, 1, 8, 4, 11, 39, 3), 18.092967984526105), ((3, 18, 20, 10, 29, 30, 4, 11, 39, 10, 44, 12, 22, 31, 18, 40, 20, 11, 15, 10, 8, 31, 10, 34, 29, 0, 10, 1, 12, 39, 43), 13.787977539514454), ((3, 47, 24, 8, 29, 30, 10, 8, 12, 18, 12, 15, 16, 8, 32, 20, 32, 18, 2, 1, 10, 44, 10, 2, 39, 2, 38, 8, 21, 13, 10, 29, 30, 3), 11.639302071519387), ((3, 33, 31, 20, 12, 22, 12, 22, 18, 4, 15, 33, 10, 34, 2, 1, 46, 26, 15, 34, 12, 29, 30, 18, 21, 8, 10, 39, 25, 44, 6), 11.734846431492576)]
54/18: m
54/19: len(m)
54/20: m[0]
54/21: len(m[0])
54/22:
l = [(preproc.decode(pred), prob) 
                    for pred, prob in sample 
                    for sample in m]
54/23: l = [(pred, prob) for sample in m for pred, prob in sample]
54/24: len(m)
54/25: len(m[0])
54/26: l = [(pred, prob) for (pred, prob) in m[0]]
54/27: m[0]
54/28: len(m[0])
54/29: l = [pred for (pred, prob) in m[0]]
54/30: pred, prob = m[0]
54/31: pred
54/32: prob
54/33: l = [pred for (pred, prob) in m]
54/34: l
54/35: len(l)
54/36: m
54/37: len(m)
54/38: l = [(pred, prob) for pred, prob in m]
54/39: l
54/40: len(l)
54/41: len(l[0])
54/42: zip((1,2), (3,4), (5,6))
54/43: list(zip((1,2), (3,4), (5,6)))
54/44: l1 = [(1,2,3),(4,5,6), (7,8,9)]
54/45: list(zip(l1, l1))
54/46: list(zip(*l1, *l1))
54/47: l
54/48: l = [[((3, 47, 12, 2, 34, 10, 20, 12, 10, 22, 16, 46, 44, 2, 28, 21, 13, 12, 5, 37, 44, 18, 40, 20, 45, 15, 10, 2, 1, 8, 4, 11, 39, 3), 18.092967984526105), ((3, 47, 12, 2, 34, 10, 20, 12, 10, 22, 16, 46, 44, 2, 28, 21, 13, 12, 5, 37, 44, 18, 40, 20, 45, 15, 10, 2, 1, 8, 4, 44, 39, 3), 18.189878369457489), ((3, 47, 12, 2, 34, 10, 20, 12, 10, 22, 16, 46, 44, 2, 28, 21, 13, 12, 5, 37, 44, 18, 40, 20, 45, 15, 10, 15, 1, 8, 4, 11, 39, 3), 18.477632083054548)], [((3, 18, 20, 10, 29, 30, 4, 11, 39, 10, 44, 12, 22, 31, 18, 40, 20, 11, 15, 10, 8, 31, 10, 34, 29, 0, 10, 1, 12, 39, 43), 13.787977539514454), ((3, 18, 20, 10, 29, 30, 4, 11, 39, 10, 44, 12, 22, 31, 18, 40, 20, 11, 15, 10, 8, 31, 10, 34, 2, 0, 10, 1, 12, 39, 43), 13.839245163415821), ((3, 18, 20, 10, 29, 30, 4, 11, 39, 10, 44, 12, 34, 31, 18, 40, 20, 11, 15, 10, 8, 31, 10, 34, 29, 0, 10, 1, 12, 39, 43), 13.844712869009113)], [((3, 47, 24, 8, 29, 30, 10, 8, 12, 18, 12, 15, 16, 8, 32, 20, 32, 18, 2, 1, 10, 44, 10, 2, 39, 2, 38, 8, 21, 13, 10, 29, 30, 3), 11.639302071519387), ((3, 47, 24, 8, 29, 30, 10, 8, 12, 18, 12, 15, 16, 8, 32, 20, 32, 39, 2, 1, 10, 44, 10, 2, 39, 2, 38, 8, 21, 13, 10, 29, 30, 3), 11.712773338495337), ((3, 47, 24, 8, 29, 30, 10, 8, 12, 18, 12, 15, 31, 8, 32, 20, 32, 18, 2, 1, 10, 44, 10, 2, 39, 2, 38, 8, 21, 13, 10, 29, 30, 3), 11.80473794679933)], [((3, 33, 31, 20, 12, 22, 12, 22, 18, 4, 15, 33, 10, 34, 2, 1, 46, 26, 15, 34, 12, 29, 30, 18, 21, 8, 10, 39, 25, 44, 6), 11.734846431492576), ((3, 33, 31, 20, 12, 22, 12, 22, 18, 17, 15, 33, 10, 34, 2, 1, 46, 26, 15, 34, 12, 29, 30, 18, 21, 8, 10, 39, 25, 44, 6), 11.794023572069653), ((3, 45, 31, 20, 12, 22, 12, 22, 18, 4, 15, 33, 10, 34, 2, 1, 46, 26, 15, 34, 12, 29, 30, 18, 21, 8, 10, 39, 25, 44, 6), 11.852507090845284)]]
54/49: l
54/50: len(l)
54/51: len(l[0])
55/1: import numpy as np
55/2: n = np.array([1,2,3],[4,5,6])
55/3: n = np.array([[1,2,3],[4,5,6]])
55/4: max(n)
55/5: np.max(n)
55/6: np.min(n)
55/7: m = np.array([1,2,3,4,5,5])
55/8: m==5
55/9: np.sum(m==5)
55/10: m1 = np.array([[1],[2],[3],[4],[5],[5]])
55/11: np.sum(m==5)
55/12: np.sum(m1==5)
55/13: m1
55/14: m1.shape
55/15: np.sum(m1==5, axis=1)
55/16: np.sum(m1==5, axis=0)
55/17: np.sum(m1==5)
55/18: m1
55/19: m1
55/20: np.squeeze(m1)
55/21: np.sum(m1==5, dtype=np.int32, keepdims=None, axis=None)
55/22: np.sum(m1==5, dtype=np.int32, keepdims=0, axis=None)
55/23: np.sum(m1==5, dtype=np.int32, keepdims=0, axis=None, out=0)
55/24: np.sum(m1==5, dtype=np.int32, keepdims=0, axis=None)
56/1: from collections import deque
56/2: l = [1,2,3]
56/3: que = deque(l)
56/4: len(que)
56/5: dir(que)
57/1: import python_speech_features
58/1: import python_speech_features
58/2: python_speech_features.mffc()
58/3: python_speech_features.mfcc()
59/1: cd utils
59/2: import loader
60/1: cd utils/
60/2: import loader
61/1: cd utils
61/2: import loader
61/3: spec = loader.log_specgram_from_file('/Users/dustin/CS/consulting/firstlayerai/data/dustin_test_data/20191217/batch1/SI943.wv')
61/4: mfcc = loader.mfcc_from_file('/Users/dustin/CS/consulting/firstlayerai/data/dustin_test_data/20191217/batch1/SI943.wv')
62/1: cd utils
62/2: import loader
62/3: mfcc = loader.mfcc_from_file('/Users/dustin/CS/consulting/firstlayerai/data/dustin_test_data/20191217/batch1/SI943.wv')
62/4: spec = loader.log_specgram_from_file('/Users/dustin/CS/consulting/firstlayerai/data/dustin_test_data/20191217/batch1/SI943.wv')
62/5: print(f"mfcc shape: {mfcc.shape}")
62/6: file1 = '/Users/dustin/CS/consulting/firstlayerai/data/dustin_test_data/20191217/batch1/SI1573.wv'
62/7: spec1 = loader.log_specgram_from_file(file1)
62/8: mfcc1 = loader.mfcc_from_file(file1)
62/9: print(f"mfcc1 shape: {mfcc1.shape}")
62/10: file2 = '/Users/dustin/CS/consulting/firstlayerai/data/dustin_test_data/20191217/batch1/SI2203.wv'
62/11: spec2 = loader.log_specgram_from_file(file2)
62/12: mfcc2 = loader.mfcc_from_file(file2)
62/13: print(f"mfcc2 shape: {mfcc2.shape}")
63/1: cd utils
63/2: import loader
63/3: file = '/Users/dustin/CS/consulting/firstlayerai/data/dustin_test_data/20191217/batch1/SX43.wv'
63/4: mfcc = loader.mfcc_from_file(file)
63/5: spec = loader.log_specgram_from_file(file)
63/6: 1e-10
63/7: print(1e-10)
64/1: cd utils/
64/2: import loader
65/1: import loader
65/2: file = '/Users/dustin/CS/consulting/firstlayerai/data/dustin_test_data/20191217/batch1/SX43.wv'
65/3: mfcc = loader.mfcc_from_file(file)
66/1: cd utils/
66/2: import loader
66/3: file = '/Users/dustin/CS/consulting/firstlayerai/data/dustin_test_data/20191217/batch1/SX43.wv'
66/4: mfcc = loader.mfcc_from_file(file)
66/5: spec = loader.log_specgram_from_file(file)
66/6: mfcc
67/1: cd utils/
67/2: import loader
67/3: file = '/Users/dustin/CS/consulting/firstlayerai/data/dustin_test_data/20191217/batch1/SX43.wv'
67/4: mfcc = loader.mfcc_from_file(file)
67/5: spec = loader.log_specgram_from_file(file)
68/1: cd utils/
68/2: import loader
68/3: mfcc = loader.mfcc_from_file(file)
68/4: file = '/Users/dustin/CS/consulting/firstlayerai/data/dustin_test_data/20191217/batch1/SX43.wv'
68/5: mfcc = loader.mfcc_from_file(file)
68/6: spec = loader.log_specgram_from_file(file)
69/1: cd utils/
69/2: import loader
69/3: file = '/Users/dustin/CS/consulting/firstlayerai/data/dustin_test_data/20191217/batch1/SX43.wv'
69/4: mfcc = loader.mfcc_from_file(file)
69/5: spec = loader.log_specgram_from_file(file)
70/1: cd utils/
70/2: import loader
70/3: file = '/Users/dustin/CS/consulting/firstlayerai/data/dustin_test_data/20191217/batch1/SX43.wv'
70/4: mfcc = loader.mfcc_from_file(file)
70/5: spec = loader.log_specgram_from_file(file)
71/1: m = 5
71/2: print(5) if m <3 else print(3)
71/3: print(5) if m <3  print(3) elif m>4 else print(4)
71/4: print(5) if m <3 print(3) elif m>4 else print(4)
71/5: print(5) if m <3 elif m>4 pritn(3) else print(4)
71/6: print(5) if m <3 elif m>4 print(3) else print(4)
71/7: import this
71/8: dir()
71/9: this
71/10: ls
71/11: pwd
71/12: cd src/awni_speech/speech
71/13: ls
71/14: cd speech/
71/15: ls
71/16: import laod
71/17: import speech
71/18: pwd
71/19: cd utils
71/20: import io
71/21: model, preproc = io.load('/Users/dustin/CS/consulting/firstlayerai/phoneme_classification/src/awni_speech/speech/examples/timit/models/GCP_models/ctc_models/20191218', tag='best')
71/22: model, preproc = io.load('/Users/dustin/CS/consulting/firstlayerai/phoneme_classification/src/awni_speech/speech/examples/timit/models/GCP_models/ctc_models/20191218')
71/23: from io import load
71/24: from speech.utils.io import save, load
71/25: cd ..
71/26: from speech.utils.io import save, load
71/27: cd ..
71/28: from speech.utils.io import save, load
71/29: model, preproc = load('/Users/dustin/CS/consulting/firstlayerai/phoneme_classification/src/awni_speech/speech/examples/timit/models/GCP_models/ctc_models/20191218')
72/1: from speech.utils.io import save, load
72/2: model, preproc = load('/Users/dustin/CS/consulting/firstlayerai/phoneme_classification/src/awni_speech/speech/examples/timit/models/GCP_models/ctc_models/20191218')
72/3: cd speech/
72/4: ls
72/5: import loader
72/6: inputs, targets = preproc.preprocess("/Users/dustin/CS/consulting/firstlayerai/data/timit_datasets/ffmpeg/timit/TEST/DR1/MDAB0/SX319.wv", ["sil", "ey", "vcl", "b", "iy", "vcl", "g", "ow", "cl", "ay", "dx", "ax", "l", "iy", "ae", "m", "vcl", "b", "el", "vcl", "th", "uw", "dh", "ix", "f", "aa", "r", "m", "y", "aa", "r", "vcl", "d", "sil"])
72/7: preproc.mean.shape
72/8: mfcc1 = loader.mfcc_from_file("/Users/dustin/CS/consulting/firstlayerai/data/timit_datasets/ffmpeg/timit/TEST/DR1/MDAB0/SX319.wv")
72/9: mfcc1.shape
72/10: var1 = mfcc1 - preproc.mean
72/11: preproc.std.shape
72/12: import numpy as np
72/13: mat1 = np.ones(2,3)
72/14: mat1 = np.ones((2,3))
72/15: mat1
72/16: mat2 = np.ones(2)
72/17: mat2
72/18: mat2.shape
72/19: mat1 - mat2
72/20: mat3 = np.ones((2,1))
72/21: mat1 - mat3
72/22: model_ls, preproc_ls = model, preproc = load('/Users/dustin/CS/consulting/firstlayerai/phoneme_classification/src/awni_speech/speech/examples/timit/models/GCP_models/ctc_models/20191212')
72/23: preproc_ls.shape
72/24: preproc_ls.mean.shape
72/25: logspec1 = loader.log_specgram_from_file("/Users/dustin/CS/consulting/firstlayerai/data/timit_datasets/ffmpeg/timit/TEST/DR1/MDAB0/SX319.wv")
72/26: logspec1.shape
72/27: logspec1.shape - preproc_ls.mean
72/28: logspec1 - preproc_ls.mean
72/29: logspec1.shape
73/1: import numpy as np
73/2: import loader
73/3: cd speech/
73/4: import loader
73/5: cd ..
73/6: from speech.utils.io import save, load
73/7: mfcc1 = loader.mfcc_from_file("/Users/dustin/CS/consulting/firstlayerai/data/timit_datasets/ffmpeg/timit/TEST/DR1/MDAB0/SX319.wv")
73/8: mfcc1.shape
73/9: model, preproc = load('/Users/dustin/CS/consulting/firstlayerai/phoneme_classification/src/awni_speech/speech/examples/timit/models/GCP_models/ctc_models/20191218')
73/10: inputs, targets = preproc.preprocess("/Users/dustin/CS/consulting/firstlayerai/data/timit_datasets/ffmpeg/timit/TEST/DR1/MDAB0/SX319.wv", ["sil", "ey", "vcl", "b", "iy", "vcl", "g", "ow", "cl", "ay", "dx", "ax", "l", "iy", "ae", "m", "vcl", "b", "el", "vcl", "th", "uw", "dh", "ix", "f", "aa", "r", "m", "y", "aa", "r", "vcl", "d", "sil"])
73/11: inputs
73/12: inputs.shape
74/1: str1="fre"
74/2: str2="freerf"
74/3: str3="erf"
74/4: str1 in str2
74/5: str1 in str3
75/1: m = np.array([[1,2],[3,4]])
75/2: import numpy as np
75/3: m = np.array([[1,2],[3,4]])
75/4: n = np.array([[1,2],[4,5]])
75/5: n *. m
75/6: n.*m
75/7: n*m
75/8: np.sum(n*m)
75/9: ls
75/10: m = (0 for _ in range(4))
75/11: m
75/12: m = [0 for _ in range(4)]
75/13: m
75/14: sig = {}
75/15: sig.get('word', 0)
75/16: sig
75/17: sig.get('word')
75/18: sig
75/19: sig['word'] = sig.get('word', 0)
75/20: sig
75/21: sig['words'] = sig.get('words', 0) + 1
75/22: sig
75/23: word = 'words'
75/24: word[0:6]
75/25: sig['words'] = sig.get('words', 0) + 1
75/26: sig
75/27: sig.values()
75/28: sum(sig.values())
75/29: round(4.5)
75/30: round(4.5,1)
75/31: round(4.5,0)
75/32: round(4.6,0)
76/1: import preprocess
77/1: import preprocess
79/1: import preprocess
80/1: import preprocess
81/1: import preprocess
81/2: preprocess.load_transcripts("/Users/dustin/CS/consulting/firstlayerai/data/librispeech/dev-clean/84/121123")
82/1: import preprocess
82/2: preprocess.load_transcripts("/Users/dustin/CS/consulting/firstlayerai/data/librispeech/dev-clean/84/121123")
83/1: import preprocess
83/2: preprocess.build_json("/Users/dustin/CS/consulting/firstlayerai/data/librispeech/dev-clean/84/121123")
84/1: import preprocess
84/2: preprocess.build_json("/Users/dustin/CS/consulting/firstlayerai/data/librispeech/dev-clean/84/121123")
84/3: preprocess.convert_to_wav("/Users/dustin/CS/consulting/firstlayerai/data/librispeech/dev-clean/84/121123")
84/4: preprocess.build_json("/Users/dustin/CS/consulting/firstlayerai/data/librispeech/dev-clean/84/121123")
84/5: dur = wave.wav_duration("/Users/dustin/CS/consulting/firstlayerai/data/librispeech/dev-clean/84/121123/84-121123-0000.wav")
84/6: import wave
84/7: dur = wave.wav_duration("/Users/dustin/CS/consulting/firstlayerai/data/librispeech/dev-clean/84/121123/84-121123-0000.wav")
84/8: cd /Users/dustin/CS/consulting/firstlayerai/phoneme_classification/src/awni_speech/speech/speech/utils
84/9: import wave
84/10: dur = wave.wav_duration("/Users/dustin/CS/consulting/firstlayerai/data/librispeech/dev-clean/84/121123/84-121123-0000.wav")
84/11: dur = wave.wav_duration("/Users/dustin/CS/consulting/firstlayerai/data/librispeech/dev-clean/84/121123/84-121123-0000.wav")
84/12: import soundfile
84/13: wav_file = "/Users/dustin/CS/consulting/firstlayerai/data/librispeech/dev-clean/84/121123/84-121123-0000.wav"
84/14: audio, samp_rate = soundfile.read(wav_file, dtype='int16')
84/15:     nframes = audio.shape[0]
84/16: nframes
84/17: preprocess.build_json("/Users/dustin/CS/consulting/firstlayerai/data/librispeech/dev-clean/84/121123")
85/1: pho = "A'BODY   EY1 B AA2 D IY0"
85/2: open("librispeech-lexicon.txt", 'r') as fid
85/3: open("librispeech-lexicon.txt", 'r')
85/4: fid = open("/Users/dustin/CS/consulting/firstlayerai/phoneme_classification/src/awni_speech/speech/examples/librispeech/librispeech-lexicon.txt", 'r')
85/5: fid[0]
85/6: lines = [l for l in fid]
85/7: lines[0]
85/8: lines
85/9: len(lines)
85/10: lines[0]
85/11: lines[0].strip()
85/12: lines[0]
85/13: lines[1].strip()
85/14: lines[1]
85/15: lines[1].strip().spit()
85/16: lines[1].strip().split()
85/17: lines[0].strip().split()
85/18: lines[0].strip().lower().split()
85/19:
with open("/Users/dustin/CS/consulting/firstlayerai/data/librispeech/dev-clean/84/121123/84-121123.trans.txt", "r") as fid:
    trans = fid
85/20:
with open("/Users/dustin/CS/consulting/firstlayerai/data/librispeech/dev-clean/84/121123/84-121123.trans.txt", "r") as fid:
    trans = [l for l in fid]
85/21: trans[0]
85/22: trans[0].strip().lower().split()
85/23: trans[0].strip().lower().split()[1:]
85/24: " ".join(trans[0].strip().lower().split()[1:])
85/25: "".join(trans[0].strip().lower().split()[1:])
85/26: " ".join(trans[0].strip().lower().split()[1:])
85/27: lines[0].strip().lower().split()
85/28: trans[0].strip().lower().split()[1:]
85/29:
with open("/Users/dustin/CS/consulting/firstlayerai/data/librispeech/dev-clean/84/121123/84-121123.trans.txt", "r") as fid:
    trans = (l for l in fid)
85/30: type(trans)
85/31: trans
85/32: fid.close()
85/33:
with open("/Users/dustin/CS/consulting/firstlayerai/data/librispeech/librispeech-lexicon.txt", "r") as fid:
    lex = (l for l in fid)
85/34: lex_dict = {l[0], l[1:] for l in lex}
85/35: lex_dict = {l[0]: l[1:] for l in lex}
85/36:
with open("/Users/dustin/CS/consulting/firstlayerai/data/librispeech/librispeech-lexicon.txt", "r") as fid:
    lex = (l.strip().lower().split() for l in fid)
    lex_dict = {l[0]: l[1:] for l in lex}
85/37: lex_dict[0]
85/38: lex_dict
85/39: l
85/40: l =[]
85/41: l.extend(['ah', 'eh', 'oh'])
85/42: l
85/43: l.extend(['ah', 'eh', 'oh'])
85/44: l
85/45: l.extend(['ah', 'eh', 'oh'])
85/46: l
85/47: from collections import defaultdict
85/48: d = defaultdict()
85/49: d
85/50: d = defaultdict("unk")
85/51: d = defaultdict(str)
85/52: d[4]
85/53: d = defaultdict(lambda: "unk")
85/54: d
85/55: d[0]
85/56:
def ddict_factory(value):
    return value
85/57: d = defaultdict(ddict_factory( "unk"))
85/58: d = defaultdict(ddict_factory("unk"))
85/59: ddict_factory("unk")
85/60: d = defaultdict(ddict_factory("unk"))
85/61: d = defaultdict(lambda: "unk")
85/62: d
85/63: d = defaultdict(str)
85/64: d
85/65: d[0]
85/66: lambda: "unk"
85/67: def print_unk: return "unk"
85/68: def print_unk(): return "unk"
85/69: d1 = defaultdict(print_unk())
85/70: def print_unk(): return "unk"
85/71: d
85/72: d[1]
85/73: d[3].isEmpty()
85/74: [].isEmpty()
85/75: ls
85/76: cd examples/librispeech
85/77: import preprocess
85/78: preprocess.lexicon_to_dict()
85/79: dct = lexicon_to_dict()
85/80: dct = preprocess.lexicon_to_dict()
85/81: dct['shoplets']
86/1: import preprocess
86/2: dct = preprocess.load_lexicon_to_dict()
87/1: import preprocess
87/2: preprocess.load_lexicon_to_dict()
88/1: import preprocess
88/2: preprocess.load_lexicon_to_dict()
88/3: preprocess.lexicon_to_dict()
89/1: preprocess.lexicon_to_dict()
89/2: import preprocess
89/3: preprocess.lexicon_to_dict()
90/1: import preprocess
90/2: preprocess.lexicon_to_dict()
90/3: dd = preprocess.load_lexicon_to_dict()
90/4: dd['shoplets']
91/1: import preprocess
91/2: preprocess.lexicon_to_dict()
91/3: dd = preprocess.load_lexicon_to_dict()
91/4: dd['shoplets']
92/1: import preprocess
92/2: preprocess.lexicon_to_dict()
92/3: dd = preprocess.load_lexicon_to_dict()
92/4: dd['shoplets']
92/5: len(list(dd.keys()))
93/1: import preprocess
93/2: preprocess.lexicon_to_dict()
93/3: dd = preprocess.load_lexicon_to_dict()
93/4: dd['shoplets']
93/5: dd['shoplets']
93/6:
for key in dd.keys():
    if dd[key] == '':
        dd[key] = "unk"
93/7: dd['shoplets']
94/1: import preprocess
94/2: preprocess.lexicon_to_dict()
94/3: dd = preprocess.load_lexicon_to_dict()
94/4: dd['shoplets']
94/5: dd['1234']
95/1: import preprocess
95/2: lex_dict['123']
95/3: preprocess.lex_dict['123']
95/4: phones = preprocess.transcript_to_phonemes(['hello', 'my', 'friend'])
95/5: phones
96/1: import preprocess
96/2: phones = preprocess.transcript_to_phonemes(['hello', 'my', 'friend'])
96/3: phones
97/1: phones = ['hh', 'eh0', 'l', 'ow1', 'm', 'ay1', 'f', 'r', 'eh1', 'n', 'd']
97/2: import string
97/3:
for p in phones:
    p.rstrip(string.digits)
97/4: phones
97/5: m = []
97/6:
for p in phones:
    p.rstrip(string.digits)
    m.append(p)
97/7: phones
97/8: m
97/9: string.digits
97/10: 'eh0'.rstrip(string.digits)
97/11:
for p in phones:
    p=p.rstrip(string.digits)
    m.append(p)
97/12: phoens
97/13: phones
97/14: m
97/15: m = []
97/16:
for p in phones:
    p=p.rstrip(string.digits)
    m.append(p)
97/17: phones
97/18: m
98/1: import preprocess
98/2: preprocess.lexicon_to_dict
99/1: import preprocess
99/2: lex = preprocess.lexicon_to_dict
99/3: lex
99/4: print(lex)
99/5: print(f"there were {len(error_phones)} incorrect phones: {error_phones} ")
99/6: s = ''
99/7: s.append('s')
99/8: s.extend('s')
99/9: s+'s'
99/10: 0%2
99/11: /Users/dustin/opt/miniconda3/envs/awni_env36/bin/python /Users/dustin/CS/consulting/firstlayerai/phoneme_classification/src/awni_speech/speech/speech/loader.py
99/12: /Users/dustin/opt/miniconda3/envs/awni_env36/bin/python /Users/dustin/CS/consulting/firstlayerai/phoneme_classification/src/awni_speech/speech/speech/loader.py
99/13: 3**2
99/14: arr = [1,2,3,4,5]
99/15: print(arr)
99/16: print(str(arr))
99/17:
for i in arr:
    print(i)
99/18: print(*arr)
99/19: print(*arr, sep=',')
99/20: print(*arr, sep=', ')
99/21: print(*arr, sep='5')
99/22: arr.reverse()
99/23: arr.reverse()
99/24: arr
99/25:  print(arr.reverse())
99/26: type(arr)
99/27: rev = arr.reverse()
99/28: print(rev)
99/29: arr[::-1]
99/30: 'tom 12345'
99/31: s='tom 12345'
99/32: s.strip()
99/33: s.split()
99/34: name, num = s.strip()
99/35: name, num = s.split()
99/36: name
99/37: set(s.split())
99/38: tuple(s.split())
99/39: arr
99/40: arr[:-1]
99/41: arr.append(None)
99/42: arr
99/43: arr[:-1]
99/44: arr = arr[:-1]
99/45: arr
99/46: arr.append(6)
99/47: arr
99/48: boolean("")
99/49: bool("")
99/50: l
99/51: l
99/52: arr
99/53: x, y, z, w, t, s = *arr
99/54: *arr
99/55:
def fun_print(x):
    print(x)
99/56: fun_print(*arr)
99/57: print(*arr)
99/58: print(*arr, sep=", ")
99/59: print(*arr, sep="hellomomy")
99/60:
for i in range (5):
    if i >3: 
        match = True
    else: match = False
    print (match)
99/61:     save_distance(data, args.score_json+.json")
99/62: print('=====')
99/63: print(fmodel_dct['audio_conf'])
99/64:
def pp_dict(d, indent=0):
    pass
99/65: print(f"{10:b}")
99/66: 10:b
99/67: l = [[1,2,3],[4,5,6],[7,8,9]]
99/68: l[0,1]
99/69: l[0][1]
99/70: l[0:2][1]
99/71: l[0:2][0]
99/72: l[0:1][0]
99/73: l[0:1][1]
99/74: l
99/75: l[0:2][0]
99/76: l[0][0:2]
99/77: l[0][0:3]
99/78: l[0:3][0:3]
99/79: l[0:2][0:2]
99/80: l
99/81: l[0:2][0:2]
99/82: l[0][0:2]
99/83: l[0][0:3]
99/84: l[2][0:3]
99/85: l[1][0:3]
99/86: l[0:3][0]
99/87: l[1][0:3]
99/88: l[1][0:2]
99/89: l[1][0:3]
99/90: l[3][0:3]
99/91: l[2][0:3]
99/92: sum(l[2][0:3])
99/93: sum(l[2][0:1])
99/94: [i for i in range(4)]
99/95: l
99/96: mean(l[0])
99/97: from statistics import mean
99/98: mean(l[0])
99/99: avg = 90
99/100: assert(avg <= 100, "scores are greater than 100")
99/101: assert avg <= 100, "scores are greater than 100"
99/102: avg = 101
99/103: assert avg <= 100, "scores are greater than 100"
100/1:
from IPython.display import HTML
HTML('<center><iframe width="700" height="400" src="https://www.youtube.com/embed/tlThdr3O5Qo?rel=0&amp;controls=0&amp;showinfo=0" frameborder="0" allowfullscreen></iframe></center>')
100/2: HTML('<center><iframe width="700" height="400" src="https://www.youtube.com/embed/x7De3tCb3_A?rel=0&amp;controls=0&amp;showinfo=0" frameborder="0" allowfullscreen></iframe></center>')
100/3: !pip install lyft_dataset_sdk
100/4:
import os
import gc
import numpy as np
import pandas as pd

import json
import math
import sys
import time
from datetime import datetime
from typing import Tuple, List

import cv2
import matplotlib.pyplot as plt
import sklearn.metrics
from PIL import Image

from matplotlib.axes import Axes
from matplotlib import animation, rc
import plotly.graph_objs as go
import plotly.tools as tls
from plotly.offline import plot, init_notebook_mode
import plotly.figure_factory as ff

init_notebook_mode(connected=True)

import seaborn as sns
from pyquaternion import Quaternion
from tqdm import tqdm

from lyft_dataset_sdk.utils.map_mask import MapMask
from lyft_dataset_sdk.lyftdataset import LyftDataset
from lyft_dataset_sdk.utils.geometry_utils import view_points, box_in_image, BoxVisibility
from lyft_dataset_sdk.utils.geometry_utils import view_points, transform_matrix
from pathlib import Path

import struct
from abc import ABC, abstractmethod
from functools import reduce
from typing import Tuple, List, Dict
import copy
100/5: DATA_PATH = '../input/3d-object-detection-for-autonomous-vehicles/'
100/6:
train = pd.read_csv(DATA_PATH + 'train.csv')
sample_submission = pd.read_csv(DATA_PATH + 'sample_submission.csv')
100/7:
# Taken from https://www.kaggle.com/gaborfodor/eda-3d-object-detection-challenge

object_columns = ['sample_id', 'object_id', 'center_x', 'center_y', 'center_z',
                  'width', 'length', 'height', 'yaw', 'class_name']
objects = []
for sample_id, ps in tqdm(train.values[:]):
    object_params = ps.split()
    n_objects = len(object_params)
    for i in range(n_objects // 8):
        x, y, z, w, l, h, yaw, c = tuple(object_params[i * 8: (i + 1) * 8])
        objects.append([sample_id, i, x, y, z, w, l, h, yaw, c])
train_objects = pd.DataFrame(
    objects,
    columns = object_columns
)
100/8:
numerical_cols = ['object_id', 'center_x', 'center_y', 'center_z', 'width', 'length', 'height', 'yaw']
train_objects[numerical_cols] = np.float32(train_objects[numerical_cols].values)
100/9: train_objects.head()
100/10:
fig, ax = plt.subplots(figsize=(10, 10))
sns.distplot(train_objects['center_x'], color='darkorange', ax=ax).set_title('center_x and center_y', fontsize=16)
sns.distplot(train_objects['center_y'], color='purple', ax=ax).set_title('center_x and center_y', fontsize=16)
plt.xlabel('center_x and center_y', fontsize=15)
plt.show()
100/11:
new_train_objects = train_objects.query('class_name == "car"')
plot = sns.jointplot(x=new_train_objects['center_x'][:1000], y=new_train_objects['center_y'][:1000], kind='kde', color='blueviolet')
plot.set_axis_labels('center_x', 'center_y', fontsize=16)
plt.show()
100/12:
fig, ax = plt.subplots(figsize=(10, 10))
sns.distplot(train_objects['center_z'], color='navy', ax=ax).set_title('center_z', fontsize=16)
plt.xlabel('center_z', fontsize=15)
plt.show()
100/13:
fig, ax = plt.subplots(figsize=(10, 10))
sns.distplot(train_objects['yaw'], color='darkgreen', ax=ax).set_title('yaw', fontsize=16)
plt.xlabel('yaw', fontsize=15)
plt.show()
100/14:
fig, ax = plt.subplots(figsize=(10, 10))
sns.distplot(train_objects['width'], color='magenta', ax=ax).set_title('width', fontsize=16)
plt.xlabel('width', fontsize=15)
plt.show()
100/15:
fig, ax = plt.subplots(figsize=(10, 10))
sns.distplot(train_objects['length'], color='crimson', ax=ax).set_title('length', fontsize=16)
plt.xlabel('length', fontsize=15)
plt.show()
100/16:
fig, ax = plt.subplots(figsize=(10, 10))
sns.distplot(train_objects['height'], color='indigo', ax=ax).set_title('height', fontsize=16)
plt.xlabel('height', fontsize=15)
plt.show()
100/17:
fig, ax = plt.subplots(figsize=(10, 10))
plot = sns.countplot(y="class_name", data=train_objects.query('class_name != "motorcycle" and class_name != "emergency_vehicle" and class_name != "animal"'),
                     palette=['navy', 'darkblue', 'blue', 'dodgerblue', 'skyblue', 'lightblue']).set_title('Object Frequencies', fontsize=16)
plt.yticks(fontsize=14)
plt.xlabel("Count", fontsize=15)
plt.ylabel("Class Name", fontsize=15)
plt.show(plot)
100/18:
fig, ax = plt.subplots(figsize=(15, 15))

plot = sns.violinplot(x="class_name", y="center_x",
                      data=train_objects.query('class_name != "motorcycle" and class_name != "emergency_vehicle" and class_name != "animal"'),
                      palette='YlGnBu',
                      split=True, ax=ax).set_title('center_x (for different objects)', fontsize=16)

plt.yticks(fontsize=14)
plt.xticks(fontsize=14)
plt.xlabel("Class Name", fontsize=15)
plt.ylabel("center_x", fontsize=15)
plt.show(plot)
100/19:
fig, ax = plt.subplots(figsize=(15, 15))

plot = sns.boxplot(x="class_name", y="center_x",
                   data=train_objects.query('class_name != "motorcycle" and class_name != "emergency_vehicle" and class_name != "animal"'),
                   palette='YlGnBu', ax=ax).set_title('center_x (for different objects)', fontsize=16)

plt.yticks(fontsize=14)
plt.xticks(fontsize=14)
plt.xlabel("Class Name", fontsize=15)
plt.ylabel("center_x", fontsize=15)
plt.show(plot)
100/20:
fig, ax = plt.subplots(figsize=(15, 15))

plot = sns.violinplot(x="class_name", y="center_y",
                      data=train_objects.query('class_name != "motorcycle" and class_name != "emergency_vehicle" and class_name != "animal"'),
                      palette='YlOrRd',
                      split=True, ax=ax).set_title('center_y (for different objects)', fontsize=16)

plt.yticks(fontsize=14)
plt.xticks(fontsize=14)
plt.xlabel("Class Name", fontsize=15)
plt.ylabel("center_y", fontsize=15)
plt.show(plot)
100/21:
fig, ax = plt.subplots(figsize=(15, 15))

plot = sns.boxplot(x="class_name", y="center_y",
                   data=train_objects.query('class_name != "motorcycle" and class_name != "emergency_vehicle" and class_name != "animal"'),
                   palette='YlOrRd', ax=ax).set_title('center_y (for different objects)', fontsize=16)

plt.yticks(fontsize=14)
plt.xticks(fontsize=14)
plt.xlabel("Class Name", fontsize=15)
plt.ylabel("center_y", fontsize=15)
plt.show(plot)
100/22:
fig, ax = plt.subplots(figsize=(15, 15))

plot = sns.violinplot(x="class_name", y="center_z",
                      data=train_objects.query('class_name != "motorcycle" and class_name != "emergency_vehicle" and class_name != "animal"').query('center_z <= -5'),
                      palette='RdPu',
                      split=True, ax=ax).set_title('center_z (for different objects)', fontsize=16)

plt.yticks(fontsize=14)
plt.xticks(fontsize=14)
plt.xlabel("Class Name", fontsize=15)
plt.ylabel("center_z", fontsize=15)
plt.show(plot)
100/23:
fig, ax = plt.subplots(figsize=(15, 15))

plot = sns.boxplot(x="class_name", y="center_z",
                   data=train_objects.query('class_name != "motorcycle" and class_name != "emergency_vehicle" and class_name != "animal"').query('center_z <= -5'),
                   palette='RdPu', ax=ax).set_title('center_z (for different objects)', fontsize=16)

plt.yticks(fontsize=14)
plt.xticks(fontsize=14)
plt.xlabel("Class Name", fontsize=15)
plt.ylabel("center_z", fontsize=15)
plt.show(plot)
100/24:
fig, ax = plt.subplots(figsize=(15, 15))

plot = sns.violinplot(x="class_name", y="width",
                      data=train_objects.query('class_name != "motorcycle" and class_name != "emergency_vehicle" and class_name != "animal"'),
                      palette='YlGn',
                      split=True, ax=ax).set_title('width (for different objects)', fontsize=16)

plt.yticks(fontsize=14)
plt.xticks(fontsize=14)
plt.xlabel("Class Name", fontsize=15)
plt.ylabel("width", fontsize=15)
plt.show(plot)
100/25:
fig, ax = plt.subplots(figsize=(15, 15))

plot = sns.boxplot(x="class_name", y="width",
                   data=train_objects.query('class_name != "motorcycle" and class_name != "emergency_vehicle" and class_name != "animal"'),
                   palette='YlGn', ax=ax).set_title('width (for different objects)', fontsize=16)

plt.yticks(fontsize=14)
plt.xticks(fontsize=14)
plt.xlabel("Class Name", fontsize=15)
plt.ylabel("width", fontsize=15)
plt.show(plot)
100/26:
fig, ax = plt.subplots(figsize=(15, 15))

plot = sns.violinplot(x="class_name", y="length",
                      data=train_objects.query('class_name != "motorcycle" and class_name != "emergency_vehicle" and class_name != "animal" and length < 15'),
                      palette='Purples',
                      split=True, ax=ax).set_title('length (for different objects)', fontsize=16)

plt.yticks(fontsize=14)
plt.xticks(fontsize=14)
plt.xlabel("Class Name", fontsize=15)
plt.ylabel("length", fontsize=15)
plt.show(plot)
100/27:
fig, ax = plt.subplots(figsize=(15, 15))

plot = sns.boxplot(x="class_name", y="length",
                   data=train_objects.query('class_name != "motorcycle" and class_name != "emergency_vehicle" and class_name != "animal" and length < 15'),
                   palette='Purples', ax=ax).set_title('length (for different objects)', fontsize=16)

plt.yticks(fontsize=14)
plt.xticks(fontsize=14)
plt.xlabel("Class Name", fontsize=15)
plt.ylabel("length", fontsize=15)
plt.show(plot)
100/28:
fig, ax = plt.subplots(figsize=(15, 15))

plot = sns.violinplot(x="class_name", y="height",
                      data=train_objects.query('class_name != "motorcycle" and class_name != "emergency_vehicle" and class_name != "animal" and height < 6'),
                      palette='Reds',
                      split=True, ax=ax).set_title('height (for different objects)', fontsize=16)

plt.yticks(fontsize=14)
plt.xticks(fontsize=14)
plt.xlabel("Class Name", fontsize=15)
plt.ylabel("height", fontsize=15)
plt.show(plot)
100/29:
fig, ax = plt.subplots(figsize=(15, 15))

plot = sns.boxplot(x="class_name", y="height",
                   data=train_objects.query('class_name != "motorcycle" and class_name != "emergency_vehicle" and class_name != "animal" and height < 6'),
                   palette='Reds', ax=ax).set_title('height (for different objects)', fontsize=16)

plt.yticks(fontsize=14)
plt.xticks(fontsize=14)
plt.xlabel("Class Name", fontsize=15)
plt.ylabel("height", fontsize=15)
plt.show(plot)
100/30:
# Lyft Dataset SDK dev-kit.
# Code written by Oscar Beijbom, 2018.
# Licensed under the Creative Commons [see licence.txt]
# Modified by Vladimir Iglovikov 2019.

class PointCloud(ABC):
    """
    Abstract class for manipulating and viewing point clouds.
    Every point cloud (lidar and radar) consists of points where:
    - Dimensions 0, 1, 2 represent x, y, z coordinates.
        These are modified when the point cloud is rotated or translated.
    - All other dimensions are optional. Hence these have to be manually modified if the reference frame changes.
    """

    def __init__(self, points: np.ndarray):
        """
        Initialize a point cloud and check it has the correct dimensions.
        :param points: <np.float: d, n>. d-dimensional input point cloud matrix.
        """
        assert points.shape[0] == self.nbr_dims(), (
            "Error: Pointcloud points must have format: %d x n" % self.nbr_dims()
        )
        self.points = points

    @staticmethod
    @abstractmethod
    def nbr_dims() -> int:
        """Returns the number of dimensions.
        Returns: Number of dimensions.
        """
        pass

    @classmethod
    @abstractmethod
    def from_file(cls, file_name: str) -> "PointCloud":
        """Loads point cloud from disk.
        Args:
            file_name: Path of the pointcloud file on disk.
        Returns: PointCloud instance.
        """
        pass

    @classmethod
    def from_file_multisweep(
        cls, lyftd, sample_rec: Dict, chan: str, ref_chan: str, num_sweeps: int = 26, min_distance: float = 1.0
    ) -> Tuple["PointCloud", np.ndarray]:
        """Return a point cloud that aggregates multiple sweeps.
        As every sweep is in a different coordinate frame, we need to map the coordinates to a single reference frame.
        As every sweep has a different timestamp, we need to account for that in the transformations and timestamps.
        Args:
            lyftd: A LyftDataset instance.
            sample_rec: The current sample.
            chan: The radar channel from which we track back n sweeps to aggregate the point cloud.
            ref_chan: The reference channel of the current sample_rec that the point clouds are mapped to.
            num_sweeps: Number of sweeps to aggregated.
            min_distance: Distance below which points are discarded.
        Returns: (all_pc, all_times). The aggregated point cloud and timestamps.
        """

        # Init
        points = np.zeros((cls.nbr_dims(), 0))
        all_pc = cls(points)
        all_times = np.zeros((1, 0))

        # Get reference pose and timestamp
        ref_sd_token = sample_rec["data"][ref_chan]
        ref_sd_rec = lyftd.get("sample_data", ref_sd_token)
        ref_pose_rec = lyftd.get("ego_pose", ref_sd_rec["ego_pose_token"])
        ref_cs_rec = lyftd.get("calibrated_sensor", ref_sd_rec["calibrated_sensor_token"])
        ref_time = 1e-6 * ref_sd_rec["timestamp"]

        # Homogeneous transform from ego car frame to reference frame
        ref_from_car = transform_matrix(ref_cs_rec["translation"], Quaternion(ref_cs_rec["rotation"]), inverse=True)

        # Homogeneous transformation matrix from global to _current_ ego car frame
        car_from_global = transform_matrix(
            ref_pose_rec["translation"], Quaternion(ref_pose_rec["rotation"]), inverse=True
        )

        # Aggregate current and previous sweeps.
        sample_data_token = sample_rec["data"][chan]
        current_sd_rec = lyftd.get("sample_data", sample_data_token)
        for _ in range(num_sweeps):
            # Load up the pointcloud.
            current_pc = cls.from_file(lyftd.data_path / ('train_' + current_sd_rec["filename"]))

            # Get past pose.
            current_pose_rec = lyftd.get("ego_pose", current_sd_rec["ego_pose_token"])
            global_from_car = transform_matrix(
                current_pose_rec["translation"], Quaternion(current_pose_rec["rotation"]), inverse=False
            )

            # Homogeneous transformation matrix from sensor coordinate frame to ego car frame.
            current_cs_rec = lyftd.get("calibrated_sensor", current_sd_rec["calibrated_sensor_token"])
            car_from_current = transform_matrix(
                current_cs_rec["translation"], Quaternion(current_cs_rec["rotation"]), inverse=False
            )

            # Fuse four transformation matrices into one and perform transform.
            trans_matrix = reduce(np.dot, [ref_from_car, car_from_global, global_from_car, car_from_current])
            current_pc.transform(trans_matrix)

            # Remove close points and add timevector.
            current_pc.remove_close(min_distance)
            time_lag = ref_time - 1e-6 * current_sd_rec["timestamp"]  # positive difference
            times = time_lag * np.ones((1, current_pc.nbr_points()))
            all_times = np.hstack((all_times, times))

            # Merge with key pc.
            all_pc.points = np.hstack((all_pc.points, current_pc.points))

            # Abort if there are no previous sweeps.
            if current_sd_rec["prev"] == "":
                break
            else:
                current_sd_rec = lyftd.get("sample_data", current_sd_rec["prev"])

        return all_pc, all_times

    def nbr_points(self) -> int:
        """Returns the number of points."""
        return self.points.shape[1]

    def subsample(self, ratio: float) -> None:
        """Sub-samples the pointcloud.
        Args:
            ratio: Fraction to keep.
        """
        selected_ind = np.random.choice(np.arange(0, self.nbr_points()), size=int(self.nbr_points() * ratio))
        self.points = self.points[:, selected_ind]

    def remove_close(self, radius: float) -> None:
        """Removes point too close within a certain radius from origin.
        Args:
            radius: Radius below which points are removed.
        Returns:
        """
        x_filt = np.abs(self.points[0, :]) < radius
        y_filt = np.abs(self.points[1, :]) < radius
        not_close = np.logical_not(np.logical_and(x_filt, y_filt))
        self.points = self.points[:, not_close]

    def translate(self, x: np.ndarray) -> None:
        """Applies a translation to the point cloud.
        Args:
            x: <np.float: 3, 1>. Translation in x, y, z.
        """
        for i in range(3):
            self.points[i, :] = self.points[i, :] + x[i]

    def rotate(self, rot_matrix: np.ndarray) -> None:
        """Applies a rotation.
        Args:
            rot_matrix: <np.float: 3, 3>. Rotation matrix.
        Returns:
        """
        self.points[:3, :] = np.dot(rot_matrix, self.points[:3, :])

    def transform(self, transf_matrix: np.ndarray) -> None:
        """Applies a homogeneous transform.
        Args:
            transf_matrix: transf_matrix: <np.float: 4, 4>. Homogenous transformation matrix.
        """
        self.points[:3, :] = transf_matrix.dot(np.vstack((self.points[:3, :], np.ones(self.nbr_points()))))[:3, :]

    def render_height(
        self,
        ax: Axes,
        view: np.ndarray = np.eye(4),
        x_lim: Tuple = (-20, 20),
        y_lim: Tuple = (-20, 20),
        marker_size: float = 1,
    ) -> None:
        """Simple method that applies a transformation and then scatter plots the points colored by height (z-value).
        Args:
            ax: Axes on which to render the points.
            view: <np.float: n, n>. Defines an arbitrary projection (n <= 4).
            x_lim: (min <float>, max <float>). x range for plotting.
            y_lim: (min <float>, max <float>). y range for plotting.
            marker_size: Marker size.
        """
        self._render_helper(2, ax, view, x_lim, y_lim, marker_size)

    def render_intensity(
        self,
        ax: Axes,
        view: np.ndarray = np.eye(4),
        x_lim: Tuple = (-20, 20),
        y_lim: Tuple = (-20, 20),
        marker_size: float = 1,
    ) -> None:
        """Very simple method that applies a transformation and then scatter plots the points colored by intensity.
        Args:
            ax: Axes on which to render the points.
            view: <np.float: n, n>. Defines an arbitrary projection (n <= 4).
            x_lim: (min <float>, max <float>).
            y_lim: (min <float>, max <float>).
            marker_size: Marker size.
        Returns:
        """
        self._render_helper(3, ax, view, x_lim, y_lim, marker_size)

    def _render_helper(
        self, color_channel: int, ax: Axes, view: np.ndarray, x_lim: Tuple, y_lim: Tuple, marker_size: float
    ) -> None:
        """Helper function for rendering.
        Args:
            color_channel: Point channel to use as color.
            ax: Axes on which to render the points.
            view: <np.float: n, n>. Defines an arbitrary projection (n <= 4).
            x_lim: (min <float>, max <float>).
            y_lim: (min <float>, max <float>).
            marker_size: Marker size.
        """
        points = view_points(self.points[:3, :], view, normalize=False)
        ax.scatter(points[0, :], points[1, :], c=self.points[color_channel, :], s=marker_size)
        ax.set_xlim(x_lim[0], x_lim[1])
        ax.set_ylim(y_lim[0], y_lim[1])


class LidarPointCloud(PointCloud):
    @staticmethod
    def nbr_dims() -> int:
        """Returns the number of dimensions.
        Returns: Number of dimensions.
        """
        return 4

    @classmethod
    def from_file(cls, file_name: Path) -> "LidarPointCloud":
        """Loads LIDAR data from binary numpy format. Data is stored as (x, y, z, intensity, ring index).
        Args:
            file_name: Path of the pointcloud file on disk.
        Returns: LidarPointCloud instance (x, y, z, intensity).
        """

        assert file_name.suffix == ".bin", "Unsupported filetype {}".format(file_name)

        scan = np.fromfile(str(file_name), dtype=np.float32)
        points = scan.reshape((-1, 5))[:, : cls.nbr_dims()]
        return cls(points.T)


class RadarPointCloud(PointCloud):

    # Class-level settings for radar pointclouds, see from_file().
    invalid_states = [0]  # type: List[int]
    dynprop_states = range(7)  # type: List[int] # Use [0, 2, 6] for moving objects only.
    ambig_states = [3]  # type: List[int]

    @staticmethod
    def nbr_dims() -> int:
        """Returns the number of dimensions.
        Returns: Number of dimensions.
        """
        return 18

    @classmethod
    def from_file(
        cls,
        file_name: Path,
        invalid_states: List[int] = None,
        dynprop_states: List[int] = None,
        ambig_states: List[int] = None,
    ) -> "RadarPointCloud":
        """Loads RADAR data from a Point Cloud Data file. See details below.
        Args:
            file_name: The path of the pointcloud file.
            invalid_states: Radar states to be kept. See details below.
            dynprop_states: Radar states to be kept. Use [0, 2, 6] for moving objects only. See details below.
            ambig_states: Radar states to be kept. See details below. To keep all radar returns,
                set each state filter to range(18).
        Returns: <np.float: d, n>. Point cloud matrix with d dimensions and n points.
        Example of the header fields:
        # .PCD v0.7 - Point Cloud Data file format
        VERSION 0.7
        FIELDS x y z dyn_prop id rcs vx vy vx_comp vy_comp is_quality_valid ambig_
                                                            state x_rms y_rms invalid_state pdh0 vx_rms vy_rms
        SIZE 4 4 4 1 2 4 4 4 4 4 1 1 1 1 1 1 1 1
        TYPE F F F I I F F F F F I I I I I I I I
        COUNT 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1
        WIDTH 125
        HEIGHT 1
        VIEWPOINT 0 0 0 1 0 0 0
        POINTS 125
        DATA binary
        Below some of the fields are explained in more detail:
        x is front, y is left
        vx, vy are the velocities in m/s.
        vx_comp, vy_comp are the velocities in m/s compensated by the ego motion.
        We recommend using the compensated velocities.
        invalid_state: state of Cluster validity state.
        (Invalid states)
        0x01    invalid due to low RCS
        0x02    invalid due to near-field artefact
        0x03    invalid far range cluster because not confirmed in near range
        0x05    reserved
        0x06    invalid cluster due to high mirror probability
        0x07    Invalid cluster because outside sensor field of view
        0x0d    reserved
        0x0e    invalid cluster because it is a harmonics
        (Valid states)
        0x00    valid
        0x04    valid cluster with low RCS
        0x08    valid cluster with azimuth correction due to elevation
        0x09    valid cluster with high child probability
        0x0a    valid cluster with high probability of being a 50 deg artefact
        0x0b    valid cluster but no local maximum
        0x0c    valid cluster with high artefact probability
        0x0f    valid cluster with above 95m in near range
        0x10    valid cluster with high multi-target probability
        0x11    valid cluster with suspicious angle
        dynProp: Dynamic property of cluster to indicate if is moving or not.
        0: moving
        1: stationary
        2: oncoming
        3: stationary candidate
        4: unknown
        5: crossing stationary
        6: crossing moving
        7: stopped
        ambig_state: State of Doppler (radial velocity) ambiguity solution.
        0: invalid
        1: ambiguous
        2: staggered ramp
        3: unambiguous
        4: stationary candidates
        pdh0: False alarm probability of cluster (i.e. probability of being an artefact caused
                                                                                    by multipath or similar).
        0: invalid
        1: <25%
        2: 50%
        3: 75%
        4: 90%
        5: 99%
        6: 99.9%
        7: <=100%
        """

        assert file_name.suffix == ".pcd", "Unsupported filetype {}".format(file_name)

        meta = []
        with open(str(file_name), "rb") as f:
            for line in f:
                line = line.strip().decode("utf-8")
                meta.append(line)
                if line.startswith("DATA"):
                    break

            data_binary = f.read()

        # Get the header rows and check if they appear as expected.
        assert meta[0].startswith("#"), "First line must be comment"
        assert meta[1].startswith("VERSION"), "Second line must be VERSION"
        sizes = meta[3].split(" ")[1:]
        types = meta[4].split(" ")[1:]
        counts = meta[5].split(" ")[1:]
        width = int(meta[6].split(" ")[1])
        height = int(meta[7].split(" ")[1])
        data = meta[10].split(" ")[1]
        feature_count = len(types)
        assert width > 0
        assert len([c for c in counts if c != c]) == 0, "Error: COUNT not supported!"
        assert height == 1, "Error: height != 0 not supported!"
        assert data == "binary"

        # Lookup table for how to decode the binaries.
        unpacking_lut = {
            "F": {2: "e", 4: "f", 8: "d"},
            "I": {1: "b", 2: "h", 4: "i", 8: "q"},
            "U": {1: "B", 2: "H", 4: "I", 8: "Q"},
        }
        types_str = "".join([unpacking_lut[t][int(s)] for t, s in zip(types, sizes)])

        # Decode each point.
        offset = 0
        point_count = width
        points = []
        for i in range(point_count):
            point = []
            for p in range(feature_count):
                start_p = offset
                end_p = start_p + int(sizes[p])
                assert end_p < len(data_binary)
                point_p = struct.unpack(types_str[p], data_binary[start_p:end_p])[0]
                point.append(point_p)
                offset = end_p
            points.append(point)

        # A NaN in the first point indicates an empty pointcloud.
        point = np.array(points[0])
        if np.any(np.isnan(point)):
            return cls(np.zeros((feature_count, 0)))

        # Convert to numpy matrix.
        points = np.array(points).transpose()

        # If no parameters are provided, use default settings.
        invalid_states = cls.invalid_states if invalid_states is None else invalid_states
        dynprop_states = cls.dynprop_states if dynprop_states is None else dynprop_states
        ambig_states = cls.ambig_states if ambig_states is None else ambig_states

        # Filter points with an invalid state.
        valid = [p in invalid_states for p in points[-4, :]]
        points = points[:, valid]

        # Filter by dynProp.
        valid = [p in dynprop_states for p in points[3, :]]
        points = points[:, valid]

        # Filter by ambig_state.
        valid = [p in ambig_states for p in points[11, :]]
        points = points[:, valid]

        return cls(points)


class Box:
    """ Simple data class representing a 3d box including, label, score and velocity. """

    def __init__(
        self,
        center: List[float],
        size: List[float],
        orientation: Quaternion,
        label: int = np.nan,
        score: float = np.nan,
        velocity: Tuple = (np.nan, np.nan, np.nan),
        name: str = None,
        token: str = None,
    ):
        """
        Args:
            center: Center of box given as x, y, z.
            size: Size of box in width, length, height.
            orientation: Box orientation.
            label: Integer label, optional.
            score: Classification score, optional.
            velocity: Box velocity in x, y, z direction.
            name: Box name, optional. Can be used e.g. for denote category name.
            token: Unique string identifier from DB.
        """
        assert not np.any(np.isnan(center))
        assert not np.any(np.isnan(size))
        assert len(center) == 3
        assert len(size) == 3
        assert type(orientation) == Quaternion

        self.center = np.array(center)
        self.wlh = np.array(size)
        self.orientation = orientation
        self.label = int(label) if not np.isnan(label) else label
        self.score = float(score) if not np.isnan(score) else score
        self.velocity = np.array(velocity)
        self.name = name
        self.token = token

    def __eq__(self, other):
        center = np.allclose(self.center, other.center)
        wlh = np.allclose(self.wlh, other.wlh)
        orientation = np.allclose(self.orientation.elements, other.orientation.elements)
        label = (self.label == other.label) or (np.isnan(self.label) and np.isnan(other.label))
        score = (self.score == other.score) or (np.isnan(self.score) and np.isnan(other.score))
        vel = np.allclose(self.velocity, other.velocity) or (
            np.all(np.isnan(self.velocity)) and np.all(np.isnan(other.velocity))
        )

        return center and wlh and orientation and label and score and vel

    def __repr__(self):
        repr_str = (
            "label: {}, score: {:.2f}, xyz: [{:.2f}, {:.2f}, {:.2f}], wlh: [{:.2f}, {:.2f}, {:.2f}], "
            "rot axis: [{:.2f}, {:.2f}, {:.2f}], ang(degrees): {:.2f}, ang(rad): {:.2f}, "
            "vel: {:.2f}, {:.2f}, {:.2f}, name: {}, token: {}"
        )

        return repr_str.format(
            self.label,
            self.score,
            self.center[0],
            self.center[1],
            self.center[2],
            self.wlh[0],
            self.wlh[1],
            self.wlh[2],
            self.orientation.axis[0],
            self.orientation.axis[1],
            self.orientation.axis[2],
            self.orientation.degrees,
            self.orientation.radians,
            self.velocity[0],
            self.velocity[1],
            self.velocity[2],
            self.name,
            self.token,
        )

    @property
    def rotation_matrix(self) -> np.ndarray:
        """Return a rotation matrix.
        Returns: <np.float: 3, 3>. The box's rotation matrix.
        """
        return self.orientation.rotation_matrix

    def translate(self, x: np.ndarray) -> None:
        """Applies a translation.
        Args:
            x: <np.float: 3, 1>. Translation in x, y, z direction.
        """
        self.center += x

    def rotate(self, quaternion: Quaternion) -> None:
        """Rotates box.
        Args:
            quaternion: Rotation to apply.
        """
        self.center = np.dot(quaternion.rotation_matrix, self.center)
        self.orientation = quaternion * self.orientation
        self.velocity = np.dot(quaternion.rotation_matrix, self.velocity)

    def corners(self, wlh_factor: float = 1.0) -> np.ndarray:
        """Returns the bounding box corners.
        Args:
            wlh_factor: Multiply width, length, height by a factor to scale the box.
        Returns: First four corners are the ones facing forward.
                The last four are the ones facing backwards.
        """

        width, length, height = self.wlh * wlh_factor

        # 3D bounding box corners. (Convention: x points forward, y to the left, z up.)
        x_corners = length / 2 * np.array([1, 1, 1, 1, -1, -1, -1, -1])
        y_corners = width / 2 * np.array([1, -1, -1, 1, 1, -1, -1, 1])
        z_corners = height / 2 * np.array([1, 1, -1, -1, 1, 1, -1, -1])
        corners = np.vstack((x_corners, y_corners, z_corners))

        # Rotate
        corners = np.dot(self.orientation.rotation_matrix, corners)

        # Translate
        x, y, z = self.center
        corners[0, :] = corners[0, :] + x
        corners[1, :] = corners[1, :] + y
        corners[2, :] = corners[2, :] + z

        return corners

    def bottom_corners(self) -> np.ndarray:
        """Returns the four bottom corners.
        Returns: <np.float: 3, 4>. Bottom corners. First two face forward, last two face backwards.
        """
        return self.corners()[:, [2, 3, 7, 6]]

    def render(
        self,
        axis: Axes,
        view: np.ndarray = np.eye(3),
        normalize: bool = False,
        colors: Tuple = ("b", "r", "k"),
        linewidth: float = 2,
    ):
        """Renders the box in the provided Matplotlib axis.
        Args:
            axis: Axis onto which the box should be drawn.
            view: <np.array: 3, 3>. Define a projection in needed (e.g. for drawing projection in an image).
            normalize: Whether to normalize the remaining coordinate.
            colors: (<Matplotlib.colors>: 3). Valid Matplotlib colors (<str> or normalized RGB tuple) for front,
            back and sides.
            linewidth: Width in pixel of the box sides.
        """
        corners = view_points(self.corners(), view, normalize=normalize)[:2, :]

        def draw_rect(selected_corners, color):
            prev = selected_corners[-1]
            for corner in selected_corners:
                axis.plot([prev[0], corner[0]], [prev[1], corner[1]], color=color, linewidth=linewidth)
                prev = corner

        # Draw the sides
        for i in range(4):
            axis.plot(
                [corners.T[i][0], corners.T[i + 4][0]],
                [corners.T[i][1], corners.T[i + 4][1]],
                color=colors[2],
                linewidth=linewidth,
            )

        # Draw front (first 4 corners) and rear (last 4 corners) rectangles(3d)/lines(2d)
        draw_rect(corners.T[:4], colors[0])
        draw_rect(corners.T[4:], colors[1])

        # Draw line indicating the front
        center_bottom_forward = np.mean(corners.T[2:4], axis=0)
        center_bottom = np.mean(corners.T[[2, 3, 7, 6]], axis=0)
        axis.plot(
            [center_bottom[0], center_bottom_forward[0]],
            [center_bottom[1], center_bottom_forward[1]],
            color=colors[0],
            linewidth=linewidth,
        )

    def render_cv2(
        self,
        image: np.ndarray,
        view: np.ndarray = np.eye(3),
        normalize: bool = False,
        colors: Tuple = ((0, 0, 255), (255, 0, 0), (155, 155, 155)),
        linewidth: int = 2,
    ) -> None:
        """Renders box using OpenCV2.
        Args:
            image: <np.array: width, height, 3>. Image array. Channels are in BGR order.
            view: <np.array: 3, 3>. Define a projection if needed (e.g. for drawing projection in an image).
            normalize: Whether to normalize the remaining coordinate.
            colors: ((R, G, B), (R, G, B), (R, G, B)). Colors for front, side & rear.
            linewidth: Linewidth for plot.
        Returns:
        """
        corners = view_points(self.corners(), view, normalize=normalize)[:2, :]

        def draw_rect(selected_corners, color):
            prev = selected_corners[-1]
            for corner in selected_corners:
                cv2.line(image, (int(prev[0]), int(prev[1])), (int(corner[0]), int(corner[1])), color, linewidth)
                prev = corner

        # Draw the sides
        for i in range(4):
            cv2.line(
                image,
                (int(corners.T[i][0]), int(corners.T[i][1])),
                (int(corners.T[i + 4][0]), int(corners.T[i + 4][1])),
                colors[2][::-1],
                linewidth,
            )

        # Draw front (first 4 corners) and rear (last 4 corners) rectangles(3d)/lines(2d)
        draw_rect(corners.T[:4], colors[0][::-1])
        draw_rect(corners.T[4:], colors[1][::-1])

        # Draw line indicating the front
        center_bottom_forward = np.mean(corners.T[2:4], axis=0)
        center_bottom = np.mean(corners.T[[2, 3, 7, 6]], axis=0)
        cv2.line(
            image,
            (int(center_bottom[0]), int(center_bottom[1])),
            (int(center_bottom_forward[0]), int(center_bottom_forward[1])),
            colors[0][::-1],
            linewidth,
        )

    def copy(self) -> "Box":
        """        Create a copy of self.
        Returns: A copy.
        """
        return copy.deepcopy(self)
100/31:
# Lyft Dataset SDK dev-kit.
# Code written by Oscar Beijbom, 2018.
# Licensed under the Creative Commons [see licence.txt]
# Modified by Vladimir Iglovikov 2019.

PYTHON_VERSION = sys.version_info[0]

if not PYTHON_VERSION == 3:
    raise ValueError("LyftDataset sdk only supports Python version 3.")


class LyftDataset:
    """Database class for Lyft Dataset to help query and retrieve information from the database."""

    def __init__(self, data_path: str, json_path: str, verbose: bool = True, map_resolution: float = 0.1):
        """Loads database and creates reverse indexes and shortcuts.
        Args:
            data_path: Path to the tables and data.
            json_path: Path to the folder with json files
            verbose: Whether to print status messages during load.
            map_resolution: Resolution of maps (meters).
        """

        self.data_path = Path(data_path).expanduser().absolute()
        self.json_path = Path(json_path)

        self.table_names = [
            "category",
            "attribute",
            "visibility",
            "instance",
            "sensor",
            "calibrated_sensor",
            "ego_pose",
            "log",
            "scene",
            "sample",
            "sample_data",
            "sample_annotation",
            "map",
        ]

        start_time = time.time()

        # Explicitly assign tables to help the IDE determine valid class members.
        self.category = self.__load_table__("category")
        self.attribute = self.__load_table__("attribute")
        self.visibility = self.__load_table__("visibility")
        self.instance = self.__load_table__("instance")
        self.sensor = self.__load_table__("sensor")
        self.calibrated_sensor = self.__load_table__("calibrated_sensor")
        self.ego_pose = self.__load_table__("ego_pose")
        self.log = self.__load_table__("log")
        self.scene = self.__load_table__("scene")
        self.sample = self.__load_table__("sample")
        self.sample_data = self.__load_table__("sample_data")
        self.sample_annotation = self.__load_table__("sample_annotation")
        self.map = self.__load_table__("map")

        # Initialize map mask for each map record.
        for map_record in self.map:
            map_record["mask"] = MapMask(self.data_path / 'train_maps/map_raster_palo_alto.png', resolution=map_resolution)

        if verbose:
            for table in self.table_names:
                print("{} {},".format(len(getattr(self, table)), table))
            print("Done loading in {:.1f} seconds.\n======".format(time.time() - start_time))

        # Make reverse indexes for common lookups.
        self.__make_reverse_index__(verbose)

        # Initialize LyftDatasetExplorer class
        self.explorer = LyftDatasetExplorer(self)

    def __load_table__(self, table_name) -> dict:
        """Loads a table."""
        with open(str(self.json_path.joinpath("{}.json".format(table_name)))) as f:
            table = json.load(f)
        return table

    def __make_reverse_index__(self, verbose: bool) -> None:
        """De-normalizes database to create reverse indices for common cases.
        Args:
            verbose: Whether to print outputs.
        """

        start_time = time.time()
        if verbose:
            print("Reverse indexing ...")

        # Store the mapping from token to table index for each table.
        self._token2ind = dict()
        for table in self.table_names:
            self._token2ind[table] = dict()

            for ind, member in enumerate(getattr(self, table)):
                self._token2ind[table][member["token"]] = ind

        # Decorate (adds short-cut) sample_annotation table with for category name.
        for record in self.sample_annotation:
            inst = self.get("instance", record["instance_token"])
            record["category_name"] = self.get("category", inst["category_token"])["name"]

        # Decorate (adds short-cut) sample_data with sensor information.
        for record in self.sample_data:
            cs_record = self.get("calibrated_sensor", record["calibrated_sensor_token"])
            sensor_record = self.get("sensor", cs_record["sensor_token"])
            record["sensor_modality"] = sensor_record["modality"]
            record["channel"] = sensor_record["channel"]

        # Reverse-index samples with sample_data and annotations.
        for record in self.sample:
            record["data"] = {}
            record["anns"] = []

        for record in self.sample_data:
            if record["is_key_frame"]:
                sample_record = self.get("sample", record["sample_token"])
                sample_record["data"][record["channel"]] = record["token"]

        for ann_record in self.sample_annotation:
            sample_record = self.get("sample", ann_record["sample_token"])
            sample_record["anns"].append(ann_record["token"])

        # Add reverse indices from log records to map records.
        if "log_tokens" not in self.map[0].keys():
            raise Exception("Error: log_tokens not in map table. This code is not compatible with the teaser dataset.")
        log_to_map = dict()
        for map_record in self.map:
            for log_token in map_record["log_tokens"]:
                log_to_map[log_token] = map_record["token"]
        for log_record in self.log:
            log_record["map_token"] = log_to_map[log_record["token"]]

        if verbose:
            print("Done reverse indexing in {:.1f} seconds.\n======".format(time.time() - start_time))

    def get(self, table_name: str, token: str) -> dict:
        """Returns a record from table in constant runtime.
        Args:
            table_name: Table name.
            token: Token of the record.
        Returns: Table record.
        """

        assert table_name in self.table_names, "Table {} not found".format(table_name)

        return getattr(self, table_name)[self.getind(table_name, token)]

    def getind(self, table_name: str, token: str) -> int:
        """Returns the index of the record in a table in constant runtime.
        Args:
            table_name: Table name.
            token: The index of the record in table, table is an array.
        Returns:
        """
        return self._token2ind[table_name][token]

    def field2token(self, table_name: str, field: str, query) -> List[str]:
        """Query all records for a certain field value, and returns the tokens for the matching records.
        Runs in linear time.
        Args:
            table_name: Table name.
            field: Field name.
            query: Query to match against. Needs to type match the content of the query field.
        Returns: List of tokens for the matching records.
        """
        matches = []
        for member in getattr(self, table_name):
            if member[field] == query:
                matches.append(member["token"])
        return matches

    def get_sample_data_path(self, sample_data_token: str) -> Path:
        """Returns the path to a sample_data.
        Args:
            sample_data_token:
        Returns:
        """

        sd_record = self.get("sample_data", sample_data_token)
        return self.data_path / sd_record["filename"]

    def get_sample_data(
        self,
        sample_data_token: str,
        box_vis_level: BoxVisibility = BoxVisibility.ANY,
        selected_anntokens: List[str] = None,
        flat_vehicle_coordinates: bool = False,
    ) -> Tuple[Path, List[Box], np.array]:
        """Returns the data path as well as all annotations related to that sample_data.
        The boxes are transformed into the current sensor's coordinate frame.
        Args:
            sample_data_token: Sample_data token.
            box_vis_level: If sample_data is an image, this sets required visibility for boxes.
            selected_anntokens: If provided only return the selected annotation.
            flat_vehicle_coordinates: Instead of current sensor's coordinate frame, use vehicle frame which is
        aligned to z-plane in world
        Returns: (data_path, boxes, camera_intrinsic <np.array: 3, 3>)
        """

        # Retrieve sensor & pose records
        sd_record = self.get("sample_data", sample_data_token)
        cs_record = self.get("calibrated_sensor", sd_record["calibrated_sensor_token"])
        sensor_record = self.get("sensor", cs_record["sensor_token"])
        pose_record = self.get("ego_pose", sd_record["ego_pose_token"])

        data_path = self.get_sample_data_path(sample_data_token)

        if sensor_record["modality"] == "camera":
            cam_intrinsic = np.array(cs_record["camera_intrinsic"])
            imsize = (sd_record["width"], sd_record["height"])
        else:
            cam_intrinsic = None
            imsize = None

        # Retrieve all sample annotations and map to sensor coordinate system.
        if selected_anntokens is not None:
            boxes = list(map(self.get_box, selected_anntokens))
        else:
            boxes = self.get_boxes(sample_data_token)

        # Make list of Box objects including coord system transforms.
        box_list = []
        for box in boxes:
            if flat_vehicle_coordinates:
                # Move box to ego vehicle coord system parallel to world z plane
                ypr = Quaternion(pose_record["rotation"]).yaw_pitch_roll
                yaw = ypr[0]

                box.translate(-np.array(pose_record["translation"]))
                box.rotate(Quaternion(scalar=np.cos(yaw / 2), vector=[0, 0, np.sin(yaw / 2)]).inverse)

            else:
                # Move box to ego vehicle coord system
                box.translate(-np.array(pose_record["translation"]))
                box.rotate(Quaternion(pose_record["rotation"]).inverse)

                #  Move box to sensor coord system
                box.translate(-np.array(cs_record["translation"]))
                box.rotate(Quaternion(cs_record["rotation"]).inverse)

            if sensor_record["modality"] == "camera" and not box_in_image(
                box, cam_intrinsic, imsize, vis_level=box_vis_level
            ):
                continue

            box_list.append(box)

        return data_path, box_list, cam_intrinsic

    def get_box(self, sample_annotation_token: str) -> Box:
        """Instantiates a Box class from a sample annotation record.
        Args:
            sample_annotation_token: Unique sample_annotation identifier.
        Returns:
        """
        record = self.get("sample_annotation", sample_annotation_token)
        return Box(
            record["translation"],
            record["size"],
            Quaternion(record["rotation"]),
            name=record["category_name"],
            token=record["token"],
        )

    def get_boxes(self, sample_data_token: str) -> List[Box]:
        """Instantiates Boxes for all annotation for a particular sample_data record. If the sample_data is a
        keyframe, this returns the annotations for that sample. But if the sample_data is an intermediate
        sample_data, a linear interpolation is applied to estimate the location of the boxes at the time the
        sample_data was captured.
        Args:
            sample_data_token: Unique sample_data identifier.
        Returns:
        """

        # Retrieve sensor & pose records
        sd_record = self.get("sample_data", sample_data_token)
        curr_sample_record = self.get("sample", sd_record["sample_token"])

        if curr_sample_record["prev"] == "" or sd_record["is_key_frame"]:
            # If no previous annotations available, or if sample_data is keyframe just return the current ones.
            boxes = list(map(self.get_box, curr_sample_record["anns"]))

        else:
            prev_sample_record = self.get("sample", curr_sample_record["prev"])

            curr_ann_recs = [self.get("sample_annotation", token) for token in curr_sample_record["anns"]]
            prev_ann_recs = [self.get("sample_annotation", token) for token in prev_sample_record["anns"]]

            # Maps instance tokens to prev_ann records
            prev_inst_map = {entry["instance_token"]: entry for entry in prev_ann_recs}

            t0 = prev_sample_record["timestamp"]
            t1 = curr_sample_record["timestamp"]
            t = sd_record["timestamp"]

            # There are rare situations where the timestamps in the DB are off so ensure that t0 < t < t1.
            t = max(t0, min(t1, t))

            boxes = []
            for curr_ann_rec in curr_ann_recs:

                if curr_ann_rec["instance_token"] in prev_inst_map:
                    # If the annotated instance existed in the previous frame, interpolate center & orientation.
                    prev_ann_rec = prev_inst_map[curr_ann_rec["instance_token"]]

                    # Interpolate center.
                    center = [
                        np.interp(t, [t0, t1], [c0, c1])
                        for c0, c1 in zip(prev_ann_rec["translation"], curr_ann_rec["translation"])
                    ]

                    # Interpolate orientation.
                    rotation = Quaternion.slerp(
                        q0=Quaternion(prev_ann_rec["rotation"]),
                        q1=Quaternion(curr_ann_rec["rotation"]),
                        amount=(t - t0) / (t1 - t0),
                    )

                    box = Box(
                        center,
                        curr_ann_rec["size"],
                        rotation,
                        name=curr_ann_rec["category_name"],
                        token=curr_ann_rec["token"],
                    )
                else:
                    # If not, simply grab the current annotation.
                    box = self.get_box(curr_ann_rec["token"])

                boxes.append(box)
        return boxes

    def box_velocity(self, sample_annotation_token: str, max_time_diff: float = 1.5) -> np.ndarray:
        """Estimate the velocity for an annotation.
        If possible, we compute the centered difference between the previous and next frame.
        Otherwise we use the difference between the current and previous/next frame.
        If the velocity cannot be estimated, values are set to np.nan.
        Args:
            sample_annotation_token: Unique sample_annotation identifier.
            max_time_diff: Max allowed time diff between consecutive samples that are used to estimate velocities.
        Returns: <np.float: 3>. Velocity in x/y/z direction in m/s.
        """

        current = self.get("sample_annotation", sample_annotation_token)
        has_prev = current["prev"] != ""
        has_next = current["next"] != ""

        # Cannot estimate velocity for a single annotation.
        if not has_prev and not has_next:
            return np.array([np.nan, np.nan, np.nan])

        if has_prev:
            first = self.get("sample_annotation", current["prev"])
        else:
            first = current

        if has_next:
            last = self.get("sample_annotation", current["next"])
        else:
            last = current

        pos_last = np.array(last["translation"])
        pos_first = np.array(first["translation"])
        pos_diff = pos_last - pos_first

        time_last = 1e-6 * self.get("sample", last["sample_token"])["timestamp"]
        time_first = 1e-6 * self.get("sample", first["sample_token"])["timestamp"]
        time_diff = time_last - time_first

        if has_next and has_prev:
            # If doing centered difference, allow for up to double the max_time_diff.
            max_time_diff *= 2

        if time_diff > max_time_diff:
            # If time_diff is too big, don't return an estimate.
            return np.array([np.nan, np.nan, np.nan])
        else:
            return pos_diff / time_diff

    def list_categories(self) -> None:
        self.explorer.list_categories()

    def list_attributes(self) -> None:
        self.explorer.list_attributes()

    def list_scenes(self) -> None:
        self.explorer.list_scenes()

    def list_sample(self, sample_token: str) -> None:
        self.explorer.list_sample(sample_token)

    def render_pointcloud_in_image(
        self,
        sample_token: str,
        dot_size: int = 5,
        pointsensor_channel: str = "LIDAR_TOP",
        camera_channel: str = "CAM_FRONT",
        out_path: str = None,
    ) -> None:
        self.explorer.render_pointcloud_in_image(
            sample_token,
            dot_size,
            pointsensor_channel=pointsensor_channel,
            camera_channel=camera_channel,
            out_path=out_path,
        )

    def render_sample(
        self,
        sample_token: str,
        box_vis_level: BoxVisibility = BoxVisibility.ANY,
        nsweeps: int = 1,
        out_path: str = None,
    ) -> None:
        self.explorer.render_sample(sample_token, box_vis_level, nsweeps=nsweeps, out_path=out_path)

    def render_sample_data(
        self,
        sample_data_token: str,
        with_anns: bool = True,
        box_vis_level: BoxVisibility = BoxVisibility.ANY,
        axes_limit: float = 40,
        ax: Axes = None,
        nsweeps: int = 1,
        out_path: str = None,
        underlay_map: bool = False,
    ) -> None:
        return self.explorer.render_sample_data(
            sample_data_token,
            with_anns,
            box_vis_level,
            axes_limit,
            ax,
            num_sweeps=nsweeps,
            out_path=out_path,
            underlay_map=underlay_map,
        )

    def render_annotation(
        self,
        sample_annotation_token: str,
        margin: float = 10,
        view: np.ndarray = np.eye(4),
        box_vis_level: BoxVisibility = BoxVisibility.ANY,
        out_path: str = None,
    ) -> None:
        self.explorer.render_annotation(sample_annotation_token, margin, view, box_vis_level, out_path)

    def render_instance(self, instance_token: str, out_path: str = None) -> None:
        self.explorer.render_instance(instance_token, out_path=out_path)

    def render_scene(self, scene_token: str, freq: float = 10, imwidth: int = 640, out_path: str = None) -> None:
        self.explorer.render_scene(scene_token, freq, image_width=imwidth, out_path=out_path)

    def render_scene_channel(
        self,
        scene_token: str,
        channel: str = "CAM_FRONT",
        freq: float = 10,
        imsize: Tuple[float, float] = (640, 360),
        out_path: str = None,
    ) -> None:
        self.explorer.render_scene_channel(
            scene_token=scene_token, channel=channel, freq=freq, image_size=imsize, out_path=out_path
        )

    def render_egoposes_on_map(self, log_location: str, scene_tokens: List = None, out_path: str = None) -> None:
        self.explorer.render_egoposes_on_map(log_location, scene_tokens, out_path=out_path)
100/32:
class LyftDatasetExplorer:
    """Helper class to list and visualize Lyft Dataset data. These are meant to serve as tutorials and templates for
    working with the data."""

    def __init__(self, lyftd: LyftDataset):
        self.lyftd = lyftd

    @staticmethod
    def get_color(category_name: str) -> Tuple[int, int, int]:
        """Provides the default colors based on the category names.
        This method works for the general Lyft Dataset categories, as well as the Lyft Dataset detection categories.
        Args:
            category_name:
        Returns:
        """
        if "bicycle" in category_name or "motorcycle" in category_name:
            return 255, 61, 99  # Red
        elif "vehicle" in category_name or category_name in ["bus", "car", "construction_vehicle", "trailer", "truck"]:
            return 255, 158, 0  # Orange
        elif "pedestrian" in category_name:
            return 0, 0, 230  # Blue
        elif "cone" in category_name or "barrier" in category_name:
            return 0, 0, 0  # Black
        else:
            return 255, 0, 255  # Magenta

    def list_categories(self) -> None:
        """Print categories, counts and stats."""

        print("Category stats")

        # Add all annotations
        categories = dict()
        for record in self.lyftd.sample_annotation:
            if record["category_name"] not in categories:
                categories[record["category_name"]] = []
            categories[record["category_name"]].append(record["size"] + [record["size"][1] / record["size"][0]])

        # Print stats
        for name, stats in sorted(categories.items()):
            stats = np.array(stats)
            print(
                "{:27} n={:5}, width={:5.2f}\u00B1{:.2f}, len={:5.2f}\u00B1{:.2f}, height={:5.2f}\u00B1{:.2f}, "
                "lw_aspect={:5.2f}\u00B1{:.2f}".format(
                    name[:27],
                    stats.shape[0],
                    np.mean(stats[:, 0]),
                    np.std(stats[:, 0]),
                    np.mean(stats[:, 1]),
                    np.std(stats[:, 1]),
                    np.mean(stats[:, 2]),
                    np.std(stats[:, 2]),
                    np.mean(stats[:, 3]),
                    np.std(stats[:, 3]),
                )
            )

    def list_attributes(self) -> None:
        """Prints attributes and counts."""
        attribute_counts = dict()
        for record in self.lyftd.sample_annotation:
            for attribute_token in record["attribute_tokens"]:
                att_name = self.lyftd.get("attribute", attribute_token)["name"]
                if att_name not in attribute_counts:
                    attribute_counts[att_name] = 0
                attribute_counts[att_name] += 1

        for name, count in sorted(attribute_counts.items()):
            print("{}: {}".format(name, count))

    def list_scenes(self) -> None:
        """ Lists all scenes with some meta data. """

        def ann_count(record):
            count = 0
            sample = self.lyftd.get("sample", record["first_sample_token"])
            while not sample["next"] == "":
                count += len(sample["anns"])
                sample = self.lyftd.get("sample", sample["next"])
            return count

        recs = [
            (self.lyftd.get("sample", record["first_sample_token"])["timestamp"], record)
            for record in self.lyftd.scene
        ]

        for start_time, record in sorted(recs):
            start_time = self.lyftd.get("sample", record["first_sample_token"])["timestamp"] / 1000000
            length_time = self.lyftd.get("sample", record["last_sample_token"])["timestamp"] / 1000000 - start_time
            location = self.lyftd.get("log", record["log_token"])["location"]
            desc = record["name"] + ", " + record["description"]
            if len(desc) > 55:
                desc = desc[:51] + "..."
            if len(location) > 18:
                location = location[:18]

            print(
                "{:16} [{}] {:4.0f}s, {}, #anns:{}".format(
                    desc,
                    datetime.utcfromtimestamp(start_time).strftime("%y-%m-%d %H:%M:%S"),
                    length_time,
                    location,
                    ann_count(record),
                )
            )

    def list_sample(self, sample_token: str) -> None:
        """Prints sample_data tokens and sample_annotation tokens related to the sample_token."""

        sample_record = self.lyftd.get("sample", sample_token)
        print("Sample: {}\n".format(sample_record["token"]))
        for sd_token in sample_record["data"].values():
            sd_record = self.lyftd.get("sample_data", sd_token)
            print(
                "sample_data_token: {}, mod: {}, channel: {}".format(
                    sd_token, sd_record["sensor_modality"], sd_record["channel"]
                )
            )
        print("")
        for ann_token in sample_record["anns"]:
            ann_record = self.lyftd.get("sample_annotation", ann_token)
            print("sample_annotation_token: {}, category: {}".format(ann_record["token"], ann_record["category_name"]))

    def map_pointcloud_to_image(self, pointsensor_token: str, camera_token: str) -> Tuple:
        """Given a point sensor (lidar/radar) token and camera sample_data token, load point-cloud and map it to
        the image plane.
        Args:
            pointsensor_token: Lidar/radar sample_data token.
            camera_token: Camera sample_data token.
        Returns: (pointcloud <np.float: 2, n)>, coloring <np.float: n>, image <Image>).
        """

        cam = self.lyftd.get("sample_data", camera_token)
        pointsensor = self.lyftd.get("sample_data", pointsensor_token)
        pcl_path = self.lyftd.data_path / ('train_' + pointsensor["filename"])
        if pointsensor["sensor_modality"] == "lidar":
            pc = LidarPointCloud.from_file(pcl_path)
        else:
            pc = RadarPointCloud.from_file(pcl_path)
        im = Image.open(str(self.lyftd.data_path / ('train_' + cam["filename"])))

        # Points live in the point sensor frame. So they need to be transformed via global to the image plane.
        # First step: transform the point-cloud to the ego vehicle frame for the timestamp of the sweep.
        cs_record = self.lyftd.get("calibrated_sensor", pointsensor["calibrated_sensor_token"])
        pc.rotate(Quaternion(cs_record["rotation"]).rotation_matrix)
        pc.translate(np.array(cs_record["translation"]))

        # Second step: transform to the global frame.
        poserecord = self.lyftd.get("ego_pose", pointsensor["ego_pose_token"])
        pc.rotate(Quaternion(poserecord["rotation"]).rotation_matrix)
        pc.translate(np.array(poserecord["translation"]))

        # Third step: transform into the ego vehicle frame for the timestamp of the image.
        poserecord = self.lyftd.get("ego_pose", cam["ego_pose_token"])
        pc.translate(-np.array(poserecord["translation"]))
        pc.rotate(Quaternion(poserecord["rotation"]).rotation_matrix.T)

        # Fourth step: transform into the camera.
        cs_record = self.lyftd.get("calibrated_sensor", cam["calibrated_sensor_token"])
        pc.translate(-np.array(cs_record["translation"]))
        pc.rotate(Quaternion(cs_record["rotation"]).rotation_matrix.T)

        # Fifth step: actually take a "picture" of the point cloud.
        # Grab the depths (camera frame z axis points away from the camera).
        depths = pc.points[2, :]

        # Retrieve the color from the depth.
        coloring = depths

        # Take the actual picture (matrix multiplication with camera-matrix + renormalization).
        points = view_points(pc.points[:3, :], np.array(cs_record["camera_intrinsic"]), normalize=True)

        # Remove points that are either outside or behind the camera. Leave a margin of 1 pixel for aesthetic reasons.
        mask = np.ones(depths.shape[0], dtype=bool)
        mask = np.logical_and(mask, depths > 0)
        mask = np.logical_and(mask, points[0, :] > 1)
        mask = np.logical_and(mask, points[0, :] < im.size[0] - 1)
        mask = np.logical_and(mask, points[1, :] > 1)
        mask = np.logical_and(mask, points[1, :] < im.size[1] - 1)
        points = points[:, mask]
        coloring = coloring[mask]

        return points, coloring, im

    def render_pointcloud_in_image(
        self,
        sample_token: str,
        dot_size: int = 2,
        pointsensor_channel: str = "LIDAR_TOP",
        camera_channel: str = "CAM_FRONT",
        out_path: str = None,
    ) -> None:
        """Scatter-plots a point-cloud on top of image.
        Args:
            sample_token: Sample token.
            dot_size: Scatter plot dot size.
            pointsensor_channel: RADAR or LIDAR channel name, e.g. 'LIDAR_TOP'.
            camera_channel: Camera channel name, e.g. 'CAM_FRONT'.
            out_path: Optional path to save the rendered figure to disk.
        Returns:
        """
        sample_record = self.lyftd.get("sample", sample_token)

        # Here we just grab the front camera and the point sensor.
        pointsensor_token = sample_record["data"][pointsensor_channel]
        camera_token = sample_record["data"][camera_channel]

        points, coloring, im = self.map_pointcloud_to_image(pointsensor_token, camera_token)
        plt.figure(figsize=(9, 16))
        plt.imshow(im)
        plt.scatter(points[0, :], points[1, :], c=coloring, s=dot_size)
        plt.axis("off")

        if out_path is not None:
            plt.savefig(out_path)

    def render_sample(
        self, token: str, box_vis_level: BoxVisibility = BoxVisibility.ANY, nsweeps: int = 1, out_path: str = None
    ) -> None:
        """Render all LIDAR and camera sample_data in sample along with annotations.
        Args:
            token: Sample token.
            box_vis_level: If sample_data is an image, this sets required visibility for boxes.
            nsweeps: Number of sweeps for lidar and radar.
            out_path: Optional path to save the rendered figure to disk.
        Returns:
        """
        record = self.lyftd.get("sample", token)

        # Separate RADAR from LIDAR and vision.
        radar_data = {}
        nonradar_data = {}
        for channel, token in record["data"].items():
            sd_record = self.lyftd.get("sample_data", token)
            sensor_modality = sd_record["sensor_modality"]
            if sensor_modality in ["lidar", "camera"]:
                nonradar_data[channel] = token
            else:
                radar_data[channel] = token

        num_radar_plots = 1 if len(radar_data) > 0 else 0

        # Create plots.
        n = num_radar_plots + len(nonradar_data)
        cols = 2
        fig, axes = plt.subplots(int(np.ceil(n / cols)), cols, figsize=(16, 24))

        if len(radar_data) > 0:
            # Plot radar into a single subplot.
            ax = axes[0, 0]
            for i, (_, sd_token) in enumerate(radar_data.items()):
                self.render_sample_data(
                    sd_token, with_anns=i == 0, box_vis_level=box_vis_level, ax=ax, num_sweeps=nsweeps
                )
            ax.set_title("Fused RADARs")

        # Plot camera and lidar in separate subplots.
        for (_, sd_token), ax in zip(nonradar_data.items(), axes.flatten()[num_radar_plots:]):
            self.render_sample_data(sd_token, box_vis_level=box_vis_level, ax=ax, num_sweeps=nsweeps)

        axes.flatten()[-1].axis("off")
        plt.tight_layout()
        fig.subplots_adjust(wspace=0, hspace=0)

        if out_path is not None:
            plt.savefig(out_path)

    def render_ego_centric_map(self, sample_data_token: str, axes_limit: float = 40, ax: Axes = None) -> None:
        """Render map centered around the associated ego pose.
        Args:
            sample_data_token: Sample_data token.
            axes_limit: Axes limit measured in meters.
            ax: Axes onto which to render.
        """

        def crop_image(image: np.array, x_px: int, y_px: int, axes_limit_px: int) -> np.array:
            x_min = int(x_px - axes_limit_px)
            x_max = int(x_px + axes_limit_px)
            y_min = int(y_px - axes_limit_px)
            y_max = int(y_px + axes_limit_px)

            cropped_image = image[y_min:y_max, x_min:x_max]

            return cropped_image

        sd_record = self.lyftd.get("sample_data", sample_data_token)

        # Init axes.
        if ax is None:
            _, ax = plt.subplots(1, 1, figsize=(9, 9))

        sample = self.lyftd.get("sample", sd_record["sample_token"])
        scene = self.lyftd.get("scene", sample["scene_token"])
        log = self.lyftd.get("log", scene["log_token"])
        map = self.lyftd.get("map", log["map_token"])
        map_mask = map["mask"]

        pose = self.lyftd.get("ego_pose", sd_record["ego_pose_token"])
        pixel_coords = map_mask.to_pixel_coords(pose["translation"][0], pose["translation"][1])

        scaled_limit_px = int(axes_limit * (1.0 / map_mask.resolution))
        mask_raster = map_mask.mask()

        cropped = crop_image(mask_raster, pixel_coords[0], pixel_coords[1], int(scaled_limit_px * math.sqrt(2)))

        ypr_rad = Quaternion(pose["rotation"]).yaw_pitch_roll
        yaw_deg = -math.degrees(ypr_rad[0])

        rotated_cropped = np.array(Image.fromarray(cropped).rotate(yaw_deg))
        ego_centric_map = crop_image(
            rotated_cropped, rotated_cropped.shape[1] / 2, rotated_cropped.shape[0] / 2, scaled_limit_px
        )
        ax.imshow(
            ego_centric_map, extent=[-axes_limit, axes_limit, -axes_limit, axes_limit], cmap="gray", vmin=0, vmax=150
        )

    def render_sample_data(
        self,
        sample_data_token: str,
        with_anns: bool = True,
        box_vis_level: BoxVisibility = BoxVisibility.ANY,
        axes_limit: float = 40,
        ax: Axes = None,
        num_sweeps: int = 1,
        out_path: str = None,
        underlay_map: bool = False,
    ):
        """Render sample data onto axis.
        Args:
            sample_data_token: Sample_data token.
            with_anns: Whether to draw annotations.
            box_vis_level: If sample_data is an image, this sets required visibility for boxes.
            axes_limit: Axes limit for lidar and radar (measured in meters).
            ax: Axes onto which to render.
            num_sweeps: Number of sweeps for lidar and radar.
            out_path: Optional path to save the rendered figure to disk.
            underlay_map: When set to true, LIDAR data is plotted onto the map. This can be slow.
        """

        # Get sensor modality.
        sd_record = self.lyftd.get("sample_data", sample_data_token)
        sensor_modality = sd_record["sensor_modality"]

        if sensor_modality == "lidar":
            # Get boxes in lidar frame.
            _, boxes, _ = self.lyftd.get_sample_data(
                sample_data_token, box_vis_level=box_vis_level, flat_vehicle_coordinates=True
            )

            # Get aggregated point cloud in lidar frame.
            sample_rec = self.lyftd.get("sample", sd_record["sample_token"])
            chan = sd_record["channel"]
            ref_chan = "LIDAR_TOP"
            pc, times = LidarPointCloud.from_file_multisweep(
                self.lyftd, sample_rec, chan, ref_chan, num_sweeps=num_sweeps
            )

            # Compute transformation matrices for lidar point cloud
            cs_record = self.lyftd.get("calibrated_sensor", sd_record["calibrated_sensor_token"])
            pose_record = self.lyftd.get("ego_pose", sd_record["ego_pose_token"])
            vehicle_from_sensor = np.eye(4)
            vehicle_from_sensor[:3, :3] = Quaternion(cs_record["rotation"]).rotation_matrix
            vehicle_from_sensor[:3, 3] = cs_record["translation"]

            ego_yaw = Quaternion(pose_record["rotation"]).yaw_pitch_roll[0]
            rot_vehicle_flat_from_vehicle = np.dot(
                Quaternion(scalar=np.cos(ego_yaw / 2), vector=[0, 0, np.sin(ego_yaw / 2)]).rotation_matrix,
                Quaternion(pose_record["rotation"]).inverse.rotation_matrix,
            )

            vehicle_flat_from_vehicle = np.eye(4)
            vehicle_flat_from_vehicle[:3, :3] = rot_vehicle_flat_from_vehicle

            # Init axes.
            if ax is None:
                _, ax = plt.subplots(1, 1, figsize=(9, 9))

            if underlay_map:
                self.render_ego_centric_map(sample_data_token=sample_data_token, axes_limit=axes_limit, ax=ax)

            # Show point cloud.
            points = view_points(
                pc.points[:3, :], np.dot(vehicle_flat_from_vehicle, vehicle_from_sensor), normalize=False
            )
            dists = np.sqrt(np.sum(pc.points[:2, :] ** 2, axis=0))
            colors = np.minimum(1, dists / axes_limit / np.sqrt(2))
            ax.scatter(points[0, :], points[1, :], c=colors, s=0.2)

            # Show ego vehicle.
            ax.plot(0, 0, "x", color="red")

            # Show boxes.
            if with_anns:
                for box in boxes:
                    c = np.array(self.get_color(box.name)) / 255.0
                    box.render(ax, view=np.eye(4), colors=(c, c, c))

            # Limit visible range.
            ax.set_xlim(-axes_limit, axes_limit)
            ax.set_ylim(-axes_limit, axes_limit)

        elif sensor_modality == "radar":
            # Get boxes in lidar frame.
            sample_rec = self.lyftd.get("sample", sd_record["sample_token"])
            lidar_token = sample_rec["data"]["LIDAR_TOP"]
            _, boxes, _ = self.lyftd.get_sample_data(lidar_token, box_vis_level=box_vis_level)

            # Get aggregated point cloud in lidar frame.
            # The point cloud is transformed to the lidar frame for visualization purposes.
            chan = sd_record["channel"]
            ref_chan = "LIDAR_TOP"
            pc, times = RadarPointCloud.from_file_multisweep(
                self.lyftd, sample_rec, chan, ref_chan, num_sweeps=num_sweeps
            )

            # Transform radar velocities (x is front, y is left), as these are not transformed when loading the point
            # cloud.
            radar_cs_record = self.lyftd.get("calibrated_sensor", sd_record["calibrated_sensor_token"])
            lidar_sd_record = self.lyftd.get("sample_data", lidar_token)
            lidar_cs_record = self.lyftd.get("calibrated_sensor", lidar_sd_record["calibrated_sensor_token"])
            velocities = pc.points[8:10, :]  # Compensated velocity
            velocities = np.vstack((velocities, np.zeros(pc.points.shape[1])))
            velocities = np.dot(Quaternion(radar_cs_record["rotation"]).rotation_matrix, velocities)
            velocities = np.dot(Quaternion(lidar_cs_record["rotation"]).rotation_matrix.T, velocities)
            velocities[2, :] = np.zeros(pc.points.shape[1])

            # Init axes.
            if ax is None:
                _, ax = plt.subplots(1, 1, figsize=(9, 9))

            # Show point cloud.
            points = view_points(pc.points[:3, :], np.eye(4), normalize=False)
            dists = np.sqrt(np.sum(pc.points[:2, :] ** 2, axis=0))
            colors = np.minimum(1, dists / axes_limit / np.sqrt(2))
            sc = ax.scatter(points[0, :], points[1, :], c=colors, s=3)

            # Show velocities.
            points_vel = view_points(pc.points[:3, :] + velocities, np.eye(4), normalize=False)
            max_delta = 10
            deltas_vel = points_vel - points
            deltas_vel = 3 * deltas_vel  # Arbitrary scaling
            deltas_vel = np.clip(deltas_vel, -max_delta, max_delta)  # Arbitrary clipping
            colors_rgba = sc.to_rgba(colors)
            for i in range(points.shape[1]):
                ax.arrow(points[0, i], points[1, i], deltas_vel[0, i], deltas_vel[1, i], color=colors_rgba[i])

            # Show ego vehicle.
            ax.plot(0, 0, "x", color="black")

            # Show boxes.
            if with_anns:
                for box in boxes:
                    c = np.array(self.get_color(box.name)) / 255.0
                    box.render(ax, view=np.eye(4), colors=(c, c, c))

            # Limit visible range.
            ax.set_xlim(-axes_limit, axes_limit)
            ax.set_ylim(-axes_limit, axes_limit)

        elif sensor_modality == "camera":
            # Load boxes and image.
            data_path, boxes, camera_intrinsic = self.lyftd.get_sample_data(
                sample_data_token, box_vis_level=box_vis_level
            )

            data = Image.open(str(data_path)[:len(str(data_path)) - 46] + 'train_images/' +\
                              str(data_path)[len(str(data_path)) - 39 : len(str(data_path))])

            # Init axes.
            if ax is None:
                _, ax = plt.subplots(1, 1, figsize=(9, 16))

            # Show image.
            ax.imshow(data)

            # Show boxes.
            if with_anns:
                for box in boxes:
                    c = np.array(self.get_color(box.name)) / 255.0
                    box.render(ax, view=camera_intrinsic, normalize=True, colors=(c, c, c))

            # Limit visible range.
            ax.set_xlim(0, data.size[0])
            ax.set_ylim(data.size[1], 0)

        else:
            raise ValueError("Error: Unknown sensor modality!")

        ax.axis("off")
        ax.set_title(sd_record["channel"])
        ax.set_aspect("equal")

        if out_path is not None:
            num = len([name for name in os.listdir(out_path)])
            out_path = out_path + str(num).zfill(5) + "_" + sample_data_token + ".png"
            plt.savefig(out_path)
            plt.close("all")
            return out_path

    def render_annotation(
        self,
        ann_token: str,
        margin: float = 10,
        view: np.ndarray = np.eye(4),
        box_vis_level: BoxVisibility = BoxVisibility.ANY,
        out_path: str = None,
    ) -> None:
        """Render selected annotation.
        Args:
            ann_token: Sample_annotation token.
            margin: How many meters in each direction to include in LIDAR view.
            view: LIDAR view point.
            box_vis_level: If sample_data is an image, this sets required visibility for boxes.
            out_path: Optional path to save the rendered figure to disk.
        """

        ann_record = self.lyftd.get("sample_annotation", ann_token)
        sample_record = self.lyftd.get("sample", ann_record["sample_token"])
        assert "LIDAR_TOP" in sample_record["data"].keys(), "No LIDAR_TOP in data, cant render"

        fig, axes = plt.subplots(1, 2, figsize=(18, 9))

        # Figure out which camera the object is fully visible in (this may return nothing)
        boxes, cam = [], []
        cams = [key for key in sample_record["data"].keys() if "CAM" in key]
        for cam in cams:
            _, boxes, _ = self.lyftd.get_sample_data(
                sample_record["data"][cam], box_vis_level=box_vis_level, selected_anntokens=[ann_token]
            )
            if len(boxes) > 0:
                break  # We found an image that matches. Let's abort.
        assert len(boxes) > 0, "Could not find image where annotation is visible. Try using e.g. BoxVisibility.ANY."
        assert len(boxes) < 2, "Found multiple annotations. Something is wrong!"

        cam = sample_record["data"][cam]

        # Plot LIDAR view
        lidar = sample_record["data"]["LIDAR_TOP"]
        data_path, boxes, camera_intrinsic = self.lyftd.get_sample_data(lidar, selected_anntokens=[ann_token])
        LidarPointCloud.from_file(Path(str(data_path)[:len(str(data_path)) - 46] + 'train_lidar/' +\
                                       str(data_path)[len(str(data_path)) - 40 : len(str(data_path))])).render_height(axes[0], view=view)
        for box in boxes:
            c = np.array(self.get_color(box.name)) / 255.0
            box.render(axes[0], view=view, colors=(c, c, c))
            corners = view_points(boxes[0].corners(), view, False)[:2, :]
            axes[0].set_xlim([np.min(corners[0, :]) - margin, np.max(corners[0, :]) + margin])
            axes[0].set_ylim([np.min(corners[1, :]) - margin, np.max(corners[1, :]) + margin])
            axes[0].axis("off")
            axes[0].set_aspect("equal")

        # Plot CAMERA view
        data_path, boxes, camera_intrinsic = self.lyftd.get_sample_data(cam, selected_anntokens=[ann_token])
        im = Image.open(Path(str(data_path)[:len(str(data_path)) - 46] + 'train_images/' +\
                             str(data_path)[len(str(data_path)) - 39 : len(str(data_path))]))
        axes[1].imshow(im)
        axes[1].set_title(self.lyftd.get("sample_data", cam)["channel"])
        axes[1].axis("off")
        axes[1].set_aspect("equal")
        for box in boxes:
            c = np.array(self.get_color(box.name)) / 255.0
            box.render(axes[1], view=camera_intrinsic, normalize=True, colors=(c, c, c))

        if out_path is not None:
            plt.savefig(out_path)

    def render_instance(self, instance_token: str, out_path: str = None) -> None:
        """Finds the annotation of the given instance that is closest to the vehicle, and then renders it.
        Args:
            instance_token: The instance token.
            out_path: Optional path to save the rendered figure to disk.
        Returns:
        """

        ann_tokens = self.lyftd.field2token("sample_annotation", "instance_token", instance_token)
        closest = [np.inf, None]
        for ann_token in ann_tokens:
            ann_record = self.lyftd.get("sample_annotation", ann_token)
            sample_record = self.lyftd.get("sample", ann_record["sample_token"])
            sample_data_record = self.lyftd.get("sample_data", sample_record["data"]["LIDAR_TOP"])
            pose_record = self.lyftd.get("ego_pose", sample_data_record["ego_pose_token"])
            dist = np.linalg.norm(np.array(pose_record["translation"]) - np.array(ann_record["translation"]))
            if dist < closest[0]:
                closest[0] = dist
                closest[1] = ann_token
        self.render_annotation(closest[1], out_path=out_path)

    def render_scene(self, scene_token: str, freq: float = 10, image_width: int = 640, out_path: Path = None) -> None:
        """Renders a full scene with all surround view camera channels.
        Args:
            scene_token: Unique identifier of scene to render.
            freq: Display frequency (Hz).
            image_width: Width of image to render. Height is determined automatically to preserve aspect ratio.
            out_path: Optional path to write a video file of the rendered frames.
        """

        if out_path is not None:
            assert out_path.suffix == ".avi"

        # Get records from DB.
        scene_rec = self.lyftd.get("scene", scene_token)
        first_sample_rec = self.lyftd.get("sample", scene_rec["first_sample_token"])
        last_sample_rec = self.lyftd.get("sample", scene_rec["last_sample_token"])

        channels = ["CAM_FRONT_LEFT", "CAM_FRONT", "CAM_FRONT_RIGHT", "CAM_BACK_LEFT", "CAM_BACK", "CAM_BACK_RIGHT"]

        horizontal_flip = ["CAM_BACK_LEFT", "CAM_BACK", "CAM_BACK_RIGHT"]  # Flip these for aesthetic reasons.

        time_step = 1 / freq * 1e6  # Time-stamps are measured in micro-seconds.

        window_name = "{}".format(scene_rec["name"])
        cv2.namedWindow(window_name)
        cv2.moveWindow(window_name, 0, 0)

        # Load first sample_data record for each channel
        current_recs = {}  # Holds the current record to be displayed by channel.
        prev_recs = {}  # Hold the previous displayed record by channel.
        for channel in channels:
            current_recs[channel] = self.lyftd.get("sample_data", first_sample_rec["data"][channel])
            prev_recs[channel] = None

        # We assume that the resolution is the same for all surround view cameras.
        image_height = int(image_width * current_recs[channels[0]]["height"] / current_recs[channels[0]]["width"])
        image_size = (image_width, image_height)

        # Set some display parameters
        layout = {
            "CAM_FRONT_LEFT": (0, 0),
            "CAM_FRONT": (image_size[0], 0),
            "CAM_FRONT_RIGHT": (2 * image_size[0], 0),
            "CAM_BACK_LEFT": (0, image_size[1]),
            "CAM_BACK": (image_size[0], image_size[1]),
            "CAM_BACK_RIGHT": (2 * image_size[0], image_size[1]),
        }

        canvas = np.ones((2 * image_size[1], 3 * image_size[0], 3), np.uint8)
        if out_path is not None:
            fourcc = cv2.VideoWriter_fourcc(*"MJPG")
            out = cv2.VideoWriter(out_path, fourcc, freq, canvas.shape[1::-1])
        else:
            out = None

        current_time = first_sample_rec["timestamp"]

        while current_time < last_sample_rec["timestamp"]:

            current_time += time_step

            # For each channel, find first sample that has time > current_time.
            for channel, sd_rec in current_recs.items():
                while sd_rec["timestamp"] < current_time and sd_rec["next"] != "":
                    sd_rec = self.lyftd.get("sample_data", sd_rec["next"])
                    current_recs[channel] = sd_rec

            # Now add to canvas
            for channel, sd_rec in current_recs.items():

                # Only update canvas if we have not already rendered this one.
                if not sd_rec == prev_recs[channel]:

                    # Get annotations and params from DB.
                    image_path, boxes, camera_intrinsic = self.lyftd.get_sample_data(
                        sd_rec["token"], box_vis_level=BoxVisibility.ANY
                    )

                    # Load and render
                    if not image_path.exists():
                        raise Exception("Error: Missing image %s" % image_path)
                    im = cv2.imread(str(image_path))
                    for box in boxes:
                        c = self.get_color(box.name)
                        box.render_cv2(im, view=camera_intrinsic, normalize=True, colors=(c, c, c))

                    im = cv2.resize(im, image_size)
                    if channel in horizontal_flip:
                        im = im[:, ::-1, :]

                    canvas[
                        layout[channel][1] : layout[channel][1] + image_size[1],
                        layout[channel][0] : layout[channel][0] + image_size[0],
                        :,
                    ] = im

                    prev_recs[channel] = sd_rec  # Store here so we don't render the same image twice.

            # Show updated canvas.
            cv2.imshow(window_name, canvas)
            if out_path is not None:
                out.write(canvas)

            key = cv2.waitKey(1)  # Wait a very short time (1 ms).

            if key == 32:  # if space is pressed, pause.
                key = cv2.waitKey()

            if key == 27:  # if ESC is pressed, exit.
                cv2.destroyAllWindows()
                break

        cv2.destroyAllWindows()
        if out_path is not None:
            out.release()

    def render_scene_channel(
        self,
        scene_token: str,
        channel: str = "CAM_FRONT",
        freq: float = 10,
        image_size: Tuple[float, float] = (640, 360),
        out_path: Path = None,
    ) -> None:
        """Renders a full scene for a particular camera channel.
        Args:
            scene_token: Unique identifier of scene to render.
            channel: Channel to render.
            freq: Display frequency (Hz).
            image_size: Size of image to render. The larger the slower this will run.
            out_path: Optional path to write a video file of the rendered frames.
        """

        valid_channels = [
            "CAM_FRONT_LEFT",
            "CAM_FRONT",
            "CAM_FRONT_RIGHT",
            "CAM_BACK_LEFT",
            "CAM_BACK",
            "CAM_BACK_RIGHT",
        ]

        assert image_size[0] / image_size[1] == 16 / 9, "Aspect ratio should be 16/9."
        assert channel in valid_channels, "Input channel {} not valid.".format(channel)

        if out_path is not None:
            assert out_path.suffix == ".avi"

        # Get records from DB
        scene_rec = self.lyftd.get("scene", scene_token)
        sample_rec = self.lyftd.get("sample", scene_rec["first_sample_token"])
        sd_rec = self.lyftd.get("sample_data", sample_rec["data"][channel])

        # Open CV init
        name = "{}: {} (Space to pause, ESC to exit)".format(scene_rec["name"], channel)
        cv2.namedWindow(name)
        cv2.moveWindow(name, 0, 0)

        if out_path is not None:
            fourcc = cv2.VideoWriter_fourcc(*"MJPG")
            out = cv2.VideoWriter(out_path, fourcc, freq, image_size)
        else:
            out = None

        has_more_frames = True
        while has_more_frames:

            # Get data from DB
            image_path, boxes, camera_intrinsic = self.lyftd.get_sample_data(
                sd_rec["token"], box_vis_level=BoxVisibility.ANY
            )

            # Load and render
            if not image_path.exists():
                raise Exception("Error: Missing image %s" % image_path)
            image = cv2.imread(str(image_path))
            for box in boxes:
                c = self.get_color(box.name)
                box.render_cv2(image, view=camera_intrinsic, normalize=True, colors=(c, c, c))

            # Render
            image = cv2.resize(image, image_size)
            cv2.imshow(name, image)
            if out_path is not None:
                out.write(image)

            key = cv2.waitKey(10)  # Images stored at approx 10 Hz, so wait 10 ms.
            if key == 32:  # If space is pressed, pause.
                key = cv2.waitKey()

            if key == 27:  # if ESC is pressed, exit
                cv2.destroyAllWindows()
                break

            if not sd_rec["next"] == "":
                sd_rec = self.lyftd.get("sample_data", sd_rec["next"])
            else:
                has_more_frames = False

        cv2.destroyAllWindows()
        if out_path is not None:
            out.release()

    def render_egoposes_on_map(
        self,
        log_location: str,
        scene_tokens: List = None,
        close_dist: float = 100,
        color_fg: Tuple[int, int, int] = (167, 174, 186),
        color_bg: Tuple[int, int, int] = (255, 255, 255),
        out_path: Path = None,
    ) -> None:
        """Renders ego poses a the map. These can be filtered by location or scene.
        Args:
            log_location: Name of the location, e.g. "singapore-onenorth", "singapore-hollandvillage",
                             "singapore-queenstown' and "boston-seaport".
            scene_tokens: Optional list of scene tokens.
            close_dist: Distance in meters for an ego pose to be considered within range of another ego pose.
            color_fg: Color of the semantic prior in RGB format (ignored if map is RGB).
            color_bg: Color of the non-semantic prior in RGB format (ignored if map is RGB).
            out_path: Optional path to save the rendered figure to disk.
        Returns:
        """

        # Get logs by location
        log_tokens = [l["token"] for l in self.lyftd.log if l["location"] == log_location]
        assert len(log_tokens) > 0, "Error: This split has 0 scenes for location %s!" % log_location

        # Filter scenes
        scene_tokens_location = [e["token"] for e in self.lyftd.scene if e["log_token"] in log_tokens]
        if scene_tokens is not None:
            scene_tokens_location = [t for t in scene_tokens_location if t in scene_tokens]
        if len(scene_tokens_location) == 0:
            print("Warning: Found 0 valid scenes for location %s!" % log_location)

        map_poses = []
        map_mask = None

        print("Adding ego poses to map...")
        for scene_token in tqdm(scene_tokens_location):

            # Get records from the database.
            scene_record = self.lyftd.get("scene", scene_token)
            log_record = self.lyftd.get("log", scene_record["log_token"])
            map_record = self.lyftd.get("map", log_record["map_token"])
            map_mask = map_record["mask"]

            # For each sample in the scene, store the ego pose.
            sample_tokens = self.lyftd.field2token("sample", "scene_token", scene_token)
            for sample_token in sample_tokens:
                sample_record = self.lyftd.get("sample", sample_token)

                # Poses are associated with the sample_data. Here we use the lidar sample_data.
                sample_data_record = self.lyftd.get("sample_data", sample_record["data"]["LIDAR_TOP"])
                pose_record = self.lyftd.get("ego_pose", sample_data_record["ego_pose_token"])

                # Calculate the pose on the map and append
                map_poses.append(
                    np.concatenate(
                        map_mask.to_pixel_coords(pose_record["translation"][0], pose_record["translation"][1])
                    )
                )

        # Compute number of close ego poses.
        print("Creating plot...")
        map_poses = np.vstack(map_poses)
        dists = sklearn.metrics.pairwise.euclidean_distances(map_poses * map_mask.resolution)
        close_poses = np.sum(dists < close_dist, axis=0)

        if len(np.array(map_mask.mask()).shape) == 3 and np.array(map_mask.mask()).shape[2] == 3:
            # RGB Colour maps
            mask = map_mask.mask()
        else:
            # Monochrome maps
            # Set the colors for the mask.
            mask = Image.fromarray(map_mask.mask())
            mask = np.array(mask)

            maskr = color_fg[0] * np.ones(np.shape(mask), dtype=np.uint8)
            maskr[mask == 0] = color_bg[0]
            maskg = color_fg[1] * np.ones(np.shape(mask), dtype=np.uint8)
            maskg[mask == 0] = color_bg[1]
            maskb = color_fg[2] * np.ones(np.shape(mask), dtype=np.uint8)
            maskb[mask == 0] = color_bg[2]
            mask = np.concatenate(
                (np.expand_dims(maskr, axis=2), np.expand_dims(maskg, axis=2), np.expand_dims(maskb, axis=2)), axis=2
            )

        # Plot.
        _, ax = plt.subplots(1, 1, figsize=(10, 10))
        ax.imshow(mask)
        title = "Number of ego poses within {}m in {}".format(close_dist, log_location)
        ax.set_title(title, color="k")
        sc = ax.scatter(map_poses[:, 0], map_poses[:, 1], s=10, c=close_poses)
        color_bar = plt.colorbar(sc, fraction=0.025, pad=0.04)
        plt.rcParams["figure.facecolor"] = "black"
        color_bar_ticklabels = plt.getp(color_bar.ax.axes, "yticklabels")
        plt.setp(color_bar_ticklabels, color="k")
        plt.rcParams["figure.facecolor"] = "white"  # Reset for future plots

        if out_path is not None:
            plt.savefig(out_path)
            plt.close("all")
100/33: lyft_dataset = LyftDataset(data_path=DATA_PATH, json_path=DATA_PATH+'train_data')
100/34:
my_scene = lyft_dataset.scene[0]
my_scene
100/35:
def render_scene(index):
    my_scene = lyft_dataset.scene[index]
    my_sample_token = my_scene["first_sample_token"]
    lyft_dataset.render_sample(my_sample_token)
100/36: render_scene(0)
100/37: render_scene(1)
100/38:
my_sample_token = my_scene["first_sample_token"]
my_sample = lyft_dataset.get('sample', my_sample_token)
100/39:
lyft_dataset.render_pointcloud_in_image(sample_token = my_sample["token"],
                                        dot_size = 1,
                                        camera_channel = 'CAM_FRONT')
100/40: my_sample['data']
100/41:
sensor_channel = 'CAM_FRONT'
my_sample_data = lyft_dataset.get('sample_data', my_sample['data'][sensor_channel])
100/42: lyft_dataset.render_sample_data(my_sample_data['token'])
100/43:
sensor_channel = 'CAM_BACK'
my_sample_data = lyft_dataset.get('sample_data', my_sample['data'][sensor_channel])
lyft_dataset.render_sample_data(my_sample_data['token'])
100/44:
sensor_channel = 'CAM_FRONT_LEFT'
my_sample_data = lyft_dataset.get('sample_data', my_sample['data'][sensor_channel])
lyft_dataset.render_sample_data(my_sample_data['token'])
100/45:
sensor_channel = 'CAM_FRONT_RIGHT'
my_sample_data = lyft_dataset.get('sample_data', my_sample['data'][sensor_channel])
lyft_dataset.render_sample_data(my_sample_data['token'])
100/46:
sensor_channel = 'CAM_BACK_LEFT'
my_sample_data = lyft_dataset.get('sample_data', my_sample['data'][sensor_channel])
lyft_dataset.render_sample_data(my_sample_data['token'])
100/47:
sensor_channel = 'CAM_BACK_RIGHT'
my_sample_data = lyft_dataset.get('sample_data', my_sample['data'][sensor_channel])
lyft_dataset.render_sample_data(my_sample_data['token'])
100/48:
my_annotation_token = my_sample['anns'][10]
my_annotation =  my_sample_data.get('sample_annotation', my_annotation_token)
100/49: lyft_dataset.render_annotation(my_annotation_token)
100/50:
my_instance = lyft_dataset.instance[100]
my_instance
100/51:
instance_token = my_instance['token']
lyft_dataset.render_instance(instance_token)
100/52: lyft_dataset.render_annotation(my_instance['last_annotation_token'])
100/53:
my_scene = lyft_dataset.scene[0]
my_sample_token = my_scene["first_sample_token"]
my_sample = lyft_dataset.get('sample', my_sample_token)
lyft_dataset.render_sample_data(my_sample['data']['LIDAR_TOP'], nsweeps=5)
100/54:
my_scene = lyft_dataset.scene[0]
my_sample_token = my_scene["first_sample_token"]
my_sample = lyft_dataset.get('sample', my_sample_token)
lyft_dataset.render_sample_data(my_sample['data']['LIDAR_FRONT_LEFT'], nsweeps=5)
100/55:
my_scene = lyft_dataset.scene[0]
my_sample_token = my_scene["first_sample_token"]
my_sample = lyft_dataset.get('sample', my_sample_token)
lyft_dataset.render_sample_data(my_sample['data']['LIDAR_FRONT_RIGHT'], nsweeps=5)
100/56:
def generate_next_token(scene):
    scene = lyft_dataset.scene[scene]
    sample_token = scene['first_sample_token']
    sample_record = lyft_dataset.get("sample", sample_token)
    
    while sample_record['next']:
        sample_token = sample_record['next']
        sample_record = lyft_dataset.get("sample", sample_token)
        
        yield sample_token

def animate_images(scene, frames, pointsensor_channel='LIDAR_TOP', interval=1):
    cams = [
        'CAM_FRONT',
        'CAM_FRONT_RIGHT',
        'CAM_BACK_RIGHT',
        'CAM_BACK',
        'CAM_BACK_LEFT',
        'CAM_FRONT_LEFT',
    ]

    generator = generate_next_token(scene)

    fig, axs = plt.subplots(
        2, len(cams), figsize=(3*len(cams), 6), 
        sharex=True, sharey=True, gridspec_kw = {'wspace': 0, 'hspace': 0.1}
    )
    
    plt.close(fig)

    def animate_fn(i):
        for _ in range(interval):
            sample_token = next(generator)
            
        for c, camera_channel in enumerate(cams):    
            sample_record = lyft_dataset.get("sample", sample_token)

            pointsensor_token = sample_record["data"][pointsensor_channel]
            camera_token = sample_record["data"][camera_channel]
            
            axs[0, c].clear()
            axs[1, c].clear()
            
            lyft_dataset.render_sample_data(camera_token, with_anns=False, ax=axs[0, c])
            lyft_dataset.render_sample_data(camera_token, with_anns=True, ax=axs[1, c])
            
            axs[0, c].set_title("")
            axs[1, c].set_title("")

    anim = animation.FuncAnimation(fig, animate_fn, frames=frames, interval=interval)
    
    return anim
100/57:
anim = animate_images(scene=3, frames=100, interval=1)
HTML(anim.to_jshtml(fps=8))
100/58:
anim = animate_images(scene=7, frames=100, interval=1)
HTML(anim.to_jshtml(fps=8))
100/59:
anim = animate_images(scene=4, frames=100, interval=1)
HTML(anim.to_jshtml(fps=8))
100/60:
def animate_lidar(scene, frames, pointsensor_channel='LIDAR_TOP', with_anns=True, interval=1):
    generator = generate_next_token(scene)

    fig, axs = plt.subplots(1, 1, figsize=(8, 8))
    plt.close(fig)

    def animate_fn(i):
        for _ in range(interval):
            sample_token = next(generator)
        
        axs.clear()
        sample_record = lyft_dataset.get("sample", sample_token)
        pointsensor_token = sample_record["data"][pointsensor_channel]
        lyft_dataset.render_sample_data(pointsensor_token, with_anns=with_anns, ax=axs)

    anim = animation.FuncAnimation(fig, animate_fn, frames=frames, interval=interval)
    
    return anim
100/61:
anim = animate_lidar(scene=5, frames=100, interval=1)
HTML(anim.to_jshtml(fps=8))
100/62:
anim = animate_lidar(scene=25, frames=100, interval=1)
HTML(anim.to_jshtml(fps=8))
100/63:
anim = animate_lidar(scene=10, frames=100, interval=1)
HTML(anim.to_jshtml(fps=8))
104/1:
from IPython.display import HTML
HTML('<center><iframe width="700" height="400" src="https://www.youtube.com/embed/tlThdr3O5Qo?rel=0&amp;controls=0&amp;showinfo=0" frameborder="0" allowfullscreen></iframe></center>')
104/2: HTML('<center><iframe width="700" height="400" src="https://www.youtube.com/embed/x7De3tCb3_A?rel=0&amp;controls=0&amp;showinfo=0" frameborder="0" allowfullscreen></iframe></center>')
104/3:
import os
import gc
import numpy as np
import pandas as pd

import json
import math
import sys
import time
from datetime import datetime
from typing import Tuple, List

import cv2
import matplotlib.pyplot as plt
import sklearn.metrics
from PIL import Image

from matplotlib.axes import Axes
from matplotlib import animation, rc
import plotly.graph_objs as go
import plotly.tools as tls
from plotly.offline import plot, init_notebook_mode
import plotly.figure_factory as ff

init_notebook_mode(connected=True)

import seaborn as sns
from pyquaternion import Quaternion
from tqdm import tqdm

from lyft_dataset_sdk.utils.map_mask import MapMask
from lyft_dataset_sdk.lyftdataset import LyftDataset
from lyft_dataset_sdk.utils.geometry_utils import view_points, box_in_image, BoxVisibility
from lyft_dataset_sdk.utils.geometry_utils import view_points, transform_matrix
from pathlib import Path

import struct
from abc import ABC, abstractmethod
from functools import reduce
from typing import Tuple, List, Dict
import copy
104/4:
import os
import gc
import numpy as np
import pandas as pd

import json
import math
import sys
import time
from datetime import datetime
from typing import Tuple, List

import cv2
import matplotlib.pyplot as plt
import sklearn.metrics
from PIL import Image

from matplotlib.axes import Axes
from matplotlib import animation, rc
import plotly.graph_objs as go
import plotly.tools as tls
from plotly.offline import plot, init_notebook_mode
import plotly.figure_factory as ff

init_notebook_mode(connected=True)

import seaborn as sns
from pyquaternion import Quaternion
from tqdm import tqdm

from lyft_dataset_sdk.utils.map_mask import MapMask
from lyft_dataset_sdk.lyftdataset import LyftDataset
from lyft_dataset_sdk.utils.geometry_utils import view_points, box_in_image, BoxVisibility
from lyft_dataset_sdk.utils.geometry_utils import view_points, transform_matrix
from pathlib import Path

import struct
from abc import ABC, abstractmethod
from functools import reduce
from typing import Tuple, List, Dict
import copy
104/5: n = np.array([1,2,3])
104/6: print(n)
104/7: DATA_PATH = '/Volumes/DRZ-Seagate/data/3d-object-detection-for-autonomous-vehicles'
104/8:
train = pd.read_csv(DATA_PATH + 'train.csv')
sample_submission = pd.read_csv(DATA_PATH + 'sample_submission.csv')
104/9: DATA_PATH = '/Volumes/DRZ-Seagate/data/3d-object-detection-for-autonomous-vehicles/'
104/10:
train = pd.read_csv(DATA_PATH + 'train.csv')
sample_submission = pd.read_csv(DATA_PATH + 'sample_submission.csv')
104/11:
# Taken from https://www.kaggle.com/gaborfodor/eda-3d-object-detection-challenge

object_columns = ['sample_id', 'object_id', 'center_x', 'center_y', 'center_z',
                  'width', 'length', 'height', 'yaw', 'class_name']
objects = []
for sample_id, ps in tqdm(train.values[:]):
    object_params = ps.split()
    n_objects = len(object_params)
    for i in range(n_objects // 8):
        x, y, z, w, l, h, yaw, c = tuple(object_params[i * 8: (i + 1) * 8])
        objects.append([sample_id, i, x, y, z, w, l, h, yaw, c])
train_objects = pd.DataFrame(
    objects,
    columns = object_columns
)
104/12:
numerical_cols = ['object_id', 'center_x', 'center_y', 'center_z', 'width', 'length', 'height', 'yaw']
train_objects[numerical_cols] = np.float32(train_objects[numerical_cols].values)
104/13: train_objects.head()
104/14:
fig, ax = plt.subplots(figsize=(10, 10))
sns.distplot(train_objects['center_x'], color='darkorange', ax=ax).set_title('center_x and center_y', fontsize=16)
sns.distplot(train_objects['center_y'], color='purple', ax=ax).set_title('center_x and center_y', fontsize=16)
plt.xlabel('center_x and center_y', fontsize=15)
plt.show()
104/15:
fig, ax = plt.subplots(figsize=(10, 10))
sns.distplot(train_objects['center_x'], color='darkorange', ax=ax).set_title('center_x and center_y', fontsize=16)
sns.distplot(train_objects['center_y'], color='purple', ax=ax).set_title('center_x and center_y', fontsize=16)
plt.xlabel('center_x and center_y', fontsize=15)
plt.show()
104/16:
new_train_objects = train_objects.query('class_name == "car"')
plot = sns.jointplot(x=new_train_objects['center_x'][:1000], y=new_train_objects['center_y'][:1000], kind='kde', color='blueviolet')
plot.set_axis_labels('center_x', 'center_y', fontsize=16)
plt.show()
106/1: cd utils/
106/2: import wave
106/3: import numpy as np
106/4: arr = np.array([])
106/5: l = [1,2,3,4]
106/6: arr.extend(l)
106/7: l = np.array([1,2,3,4])
106/8: l2 = l+l
106/9: l2
106/10: l.append(l)
106/11: np.append(l,l)
106/12: l
106/13: arr = np.append(l,l)
106/14: arr
106/15: arr = np.append(arr,l)
106/16: arr
106/17: import inspect
106/18: import subproces
106/19: import subprocess
106/20: src = inspect.getsource(subprocess)
106/21: src
106/22: print(src)
106/23: src
106/24: import numpy
106/25: np_src = inspect.getsource(numpy)
106/26: print(np_src)
106/27: subprocess.run('rec ~/newfile.wav -trim 0 5')
106/28: subprocess.run('rec -q /newfile.wav -trim 0 5')
106/29: subprocess.run('rec -q ~/newfile.wav trim 0 5')
106/30: subprocess.run('rec -q ~/newfile.wav trim 0 5')
106/31: import shlex
106/32: args = 'rec -q ~/newfile.wav trim 0 5'
106/33: subprocess.run(shlex.split(args))
106/34: args = 'rec -q ./newfile.wav trim 0 5'
106/35: subprocess.run(shlex.split(args))
106/36: import wave
106/37: audio, sr = array_from_wave('/Users/dustin/CS/consulting/firstlayerai/phoneme_classification/src/awni_speech/speech/output.wav')
106/38: audio, sr =wave.array_from_wave('/Users/dustin/CS/consulting/firstlayerai/phoneme_classification/src/awni_speech/speech/output.wav')
106/39: ls
106/40: import wave
106/41: audio, sr =wave.array_from_wave('/Users/dustin/CS/consulting/firstlayerai/phoneme_classification/src/awni_speech/speech/output.wav')
106/42: wave
106/43: import soundfile
106/44: file_name = '/Users/dustin/CS/consulting/firstlayerai/phoneme_classification/src/awni_speech/speech/output.wav'
106/45: audio, samp_rate = soundfile.read(file_name, dtype='int16')
106/46: type(audio)
106/47: audio
106/48: file2 = '/Users/dustin/CS/consulting/firstlayerai/phoneme_classification/src/awni_speech/speech/output_2.wav'
106/49: soundfile.write(file2, audio, 16000)
106/50: t = tuple(5,4)
106/51: t = tuple([5,4])
106/52: t
106/53: t[0]
106/54: t[1]
106/55: round(0.3333,2)
104/17: DATA_PATH = '/Volumes/DRZ-Seagate/data/3d-object-detection-for-autonomous-vehicles/'
104/18: DATA_PATH = '/Volumes/DRZ-Seagate/data/3d-object-detection-for-autonomous-vehicles/'
104/19:
train = pd.read_csv(DATA_PATH + 'train.csv')
sample_submission = pd.read_csv(DATA_PATH + 'sample_submission.csv')
104/20:
# Taken from https://www.kaggle.com/gaborfodor/eda-3d-object-detection-challenge

object_columns = ['sample_id', 'object_id', 'center_x', 'center_y', 'center_z',
                  'width', 'length', 'height', 'yaw', 'class_name']
objects = []
for sample_id, ps in tqdm(train.values[:]):
    object_params = ps.split()
    n_objects = len(object_params)
    for i in range(n_objects // 8):
        x, y, z, w, l, h, yaw, c = tuple(object_params[i * 8: (i + 1) * 8])
        objects.append([sample_id, i, x, y, z, w, l, h, yaw, c])
train_objects = pd.DataFrame(
    objects,
    columns = object_columns
)
104/21:
fig, ax = plt.subplots(figsize=(10, 10))
sns.distplot(train_objects['center_x'], color='darkorange', ax=ax).set_title('center_x and center_y', fontsize=16)
sns.distplot(train_objects['center_y'], color='purple', ax=ax).set_title('center_x and center_y', fontsize=16)
plt.xlabel('center_x and center_z', fontsize=15)
plt.show()
104/22:
fig, ax = plt.subplots(figsize=(10, 10))
sns.distplot(train_objects['center_x'], color='darkorange', ax=ax).set_title('center_x and center_y', fontsize=16)
sns.distplot(train_objects['center_y'], color='purple', ax=ax).set_title('center_x and center_y', fontsize=16)
plt.xlabel('center_x and center_y', fontsize=15)
plt.show()
104/23:
numerical_cols = ['object_id', 'center_x', 'center_y', 'center_z', 'width', 'length', 'height', 'yaw']
train_objects[numerical_cols] = np.float32(train_objects[numerical_cols].values)
104/24: train_objects.head()
104/25:
fig, ax = plt.subplots(figsize=(10, 10))
sns.distplot(train_objects['center_x'], color='darkorange', ax=ax).set_title('center_x and center_y', fontsize=16)
sns.distplot(train_objects['center_y'], color='purple', ax=ax).set_title('center_x and center_y', fontsize=16)
plt.xlabel('center_x and center_y', fontsize=15)
plt.show()
104/26:
fig, ax = plt.subplots(figsize=(10, 10))
sns.distplot(train_objects['center_x'], color='darkorange', ax=ax).set_title('center_x and center_y', fontsize=16)
sns.distplot(train_objects['center_y'], color='purple', ax=ax).set_title('center_x and center_y', fontsize=16)
plt.xlabel('center_x and center_z', fontsize=15)
plt.show()
104/27:
fig, ax = plt.subplots(figsize=(10, 10))
sns.distplot(train_objects['center_x'], color='darkorange', ax=ax).set_title('center_x and center_y', fontsize=16)
sns.distplot(train_objects['center_y'], color='purple', ax=ax).set_title('center_x and center_y', fontsize=16)
plt.xlabel('center_x and center_y', fontsize=15)
plt.show()
104/28:
fig, ax = plt.subplots(figsize=(10, 10))
sns.distplot(train_objects['center_x'], color='darkorange', ax=ax).set_title('center_x and center_y', fontsize=16)
sns.distplot(train_objects['center_y'], color='purple', ax=ax).set_title('center_x and center_z', fontsize=16)
plt.xlabel('center_x and center_y', fontsize=15)
plt.show()
104/29:
fig, ax = plt.subplots(figsize=(10, 10))
sns.distplot(train_objects['center_x'], color='darkorange', ax=ax).set_title('center_x and center_y', fontsize=16)
sns.distplot(train_objects['center_y'], color='purple', ax=ax).set_title('center_x and center_y', fontsize=16)
plt.xlabel('center_x and center_y', fontsize=15)
plt.show()
104/30:
fig, ax = plt.subplots(figsize=(10, 10))
sns.distplot(train_objects['center_x'], color='darkorange', ax=ax).set_title('center_x and center_z', fontsize=16)
sns.distplot(train_objects['center_y'], color='purple', ax=ax).set_title('center_x and center_y', fontsize=16)
plt.xlabel('center_x and center_y', fontsize=15)
plt.show()
104/31:
fig, ax = plt.subplots(figsize=(10, 10))
sns.distplot(train_objects['center_x'], color='darkorange', ax=ax).set_title('center_x and center_y', fontsize=16)
sns.distplot(train_objects['center_y'], color='purple', ax=ax).set_title('center_x and center_y', fontsize=16)
plt.xlabel('center_x and center_y', fontsize=15)
plt.show()
104/32:
fig, ax = plt.subplots(figsize=(10, 10))
sns.distplot(train_objects['center_z'], color='navy', ax=ax).set_title('center_z', fontsize=16)
plt.xlabel('center_z', fontsize=15)
plt.show()
104/33:
fig, ax = plt.subplots(figsize=(10, 10))
sns.distplot(train_objects['yaw'], color='darkgreen', ax=ax).set_title('yaw', fontsize=16)
plt.xlabel('yaw', fontsize=15)
plt.show()
104/34:
fig, ax = plt.subplots(figsize=(10, 10))
sns.distplot(train_objects['width'], color='magenta', ax=ax).set_title('width', fontsize=16)
plt.xlabel('width', fontsize=15)
plt.show()
104/35:
fig, ax = plt.subplots(figsize=(10, 10))
sns.distplot(train_objects['length'], color='crimson', ax=ax).set_title('length', fontsize=16)
plt.xlabel('length', fontsize=15)
plt.show()
104/36:
fig, ax = plt.subplots(figsize=(10, 10))
sns.distplot(train_objects['height'], color='indigo', ax=ax).set_title('height', fontsize=16)
plt.xlabel('height', fontsize=15)
plt.show()
104/37:
fig, ax = plt.subplots(figsize=(10, 10))
plot = sns.countplot(y="class_name", data=train_objects.query('class_name != "motorcycle" and class_name != "emergency_vehicle" and class_name != "animal"'),
                     palette=['navy', 'darkblue', 'blue', 'dodgerblue', 'skyblue', 'lightblue']).set_title('Object Frequencies', fontsize=16)
plt.yticks(fontsize=14)
plt.xlabel("Count", fontsize=15)
plt.ylabel("Class Name", fontsize=15)
plt.show(plot)
104/38:
fig, ax = plt.subplots(figsize=(15, 15))

plot = sns.violinplot(x="class_name", y="center_x",
                      data=train_objects.query('class_name != "motorcycle" and class_name != "emergency_vehicle" and class_name != "animal"'),
                      palette='YlGnBu',
                      split=True, ax=ax).set_title('center_x (for different objects)', fontsize=16)

plt.yticks(fontsize=14)
plt.xticks(fontsize=14)
plt.xlabel("Class Name", fontsize=15)
plt.ylabel("center_x", fontsize=15)
plt.show(plot)
104/39:
fig, ax = plt.subplots(figsize=(15, 15))

plot = sns.boxplot(x="class_name", y="center_x",
                   data=train_objects.query('class_name != "motorcycle" and class_name != "emergency_vehicle" and class_name != "animal"'),
                   palette='YlGnBu', ax=ax).set_title('center_x (for different objects)', fontsize=16)

plt.yticks(fontsize=14)
plt.xticks(fontsize=14)
plt.xlabel("Class Name", fontsize=15)
plt.ylabel("center_x", fontsize=15)
plt.show(plot)
104/40:
fig, ax = plt.subplots(figsize=(15, 15))

plot = sns.violinplot(x="class_name", y="center_y",
                      data=train_objects.query('class_name != "motorcycle" and class_name != "emergency_vehicle" and class_name != "animal"'),
                      palette='YlOrRd',
                      split=True, ax=ax).set_title('center_y (for different objects)', fontsize=16)

plt.yticks(fontsize=14)
plt.xticks(fontsize=14)
plt.xlabel("Class Name", fontsize=15)
plt.ylabel("center_y", fontsize=15)
plt.show(plot)
104/41:
fig, ax = plt.subplots(figsize=(15, 15))

plot = sns.boxplot(x="class_name", y="center_y",
                   data=train_objects.query('class_name != "motorcycle" and class_name != "emergency_vehicle" and class_name != "animal"'),
                   palette='YlOrRd', ax=ax).set_title('center_y (for different objects)', fontsize=16)

plt.yticks(fontsize=14)
plt.xticks(fontsize=14)
plt.xlabel("Class Name", fontsize=15)
plt.ylabel("center_y", fontsize=15)
plt.show(plot)
104/42:
fig, ax = plt.subplots(figsize=(15, 15))

plot = sns.violinplot(x="class_name", y="center_z",
                      data=train_objects.query('class_name != "motorcycle" and class_name != "emergency_vehicle" and class_name != "animal"').query('center_z <= -5'),
                      palette='RdPu',
                      split=True, ax=ax).set_title('center_z (for different objects)', fontsize=16)

plt.yticks(fontsize=14)
plt.xticks(fontsize=14)
plt.xlabel("Class Name", fontsize=15)
plt.ylabel("center_z", fontsize=15)
plt.show(plot)
104/43:
fig, ax = plt.subplots(figsize=(15, 15))

plot = sns.boxplot(x="class_name", y="center_z",
                   data=train_objects.query('class_name != "motorcycle" and class_name != "emergency_vehicle" and class_name != "animal"').query('center_z <= -5'),
                   palette='RdPu', ax=ax).set_title('center_z (for different objects)', fontsize=16)

plt.yticks(fontsize=14)
plt.xticks(fontsize=14)
plt.xlabel("Class Name", fontsize=15)
plt.ylabel("center_z", fontsize=15)
plt.show(plot)
104/44:
fig, ax = plt.subplots(figsize=(15, 15))

plot = sns.violinplot(x="class_name", y="width",
                      data=train_objects.query('class_name != "motorcycle" and class_name != "emergency_vehicle" and class_name != "animal"'),
                      palette='YlGn',
                      split=True, ax=ax).set_title('width (for different objects)', fontsize=16)

plt.yticks(fontsize=14)
plt.xticks(fontsize=14)
plt.xlabel("Class Name", fontsize=15)
plt.ylabel("width", fontsize=15)
plt.show(plot)
104/45:
fig, ax = plt.subplots(figsize=(15, 15))

plot = sns.boxplot(x="class_name", y="width",
                   data=train_objects.query('class_name != "motorcycle" and class_name != "emergency_vehicle" and class_name != "animal"'),
                   palette='YlGn', ax=ax).set_title('width (for different objects)', fontsize=16)

plt.yticks(fontsize=14)
plt.xticks(fontsize=14)
plt.xlabel("Class Name", fontsize=15)
plt.ylabel("width", fontsize=15)
plt.show(plot)
104/46:
fig, ax = plt.subplots(figsize=(15, 15))

plot = sns.violinplot(x="class_name", y="length",
                      data=train_objects.query('class_name != "motorcycle" and class_name != "emergency_vehicle" and class_name != "animal" and length < 15'),
                      palette='Purples',
                      split=True, ax=ax).set_title('length (for different objects)', fontsize=16)

plt.yticks(fontsize=14)
plt.xticks(fontsize=14)
plt.xlabel("Class Name", fontsize=15)
plt.ylabel("length", fontsize=15)
plt.show(plot)
104/47:
fig, ax = plt.subplots(figsize=(15, 15))

plot = sns.boxplot(x="class_name", y="length",
                   data=train_objects.query('class_name != "motorcycle" and class_name != "emergency_vehicle" and class_name != "animal" and length < 15'),
                   palette='Purples', ax=ax).set_title('length (for different objects)', fontsize=16)

plt.yticks(fontsize=14)
plt.xticks(fontsize=14)
plt.xlabel("Class Name", fontsize=15)
plt.ylabel("length", fontsize=15)
plt.show(plot)
104/48:
fig, ax = plt.subplots(figsize=(15, 15))

plot = sns.violinplot(x="class_name", y="height",
                      data=train_objects.query('class_name != "motorcycle" and class_name != "emergency_vehicle" and class_name != "animal" and height < 6'),
                      palette='Reds',
                      split=True, ax=ax).set_title('height (for different objects)', fontsize=16)

plt.yticks(fontsize=14)
plt.xticks(fontsize=14)
plt.xlabel("Class Name", fontsize=15)
plt.ylabel("height", fontsize=15)
plt.show(plot)
104/49:
fig, ax = plt.subplots(figsize=(15, 15))

plot = sns.boxplot(x="class_name", y="height",
                   data=train_objects.query('class_name != "motorcycle" and class_name != "emergency_vehicle" and class_name != "animal" and height < 6'),
                   palette='Reds', ax=ax).set_title('height (for different objects)', fontsize=16)

plt.yticks(fontsize=14)
plt.xticks(fontsize=14)
plt.xlabel("Class Name", fontsize=15)
plt.ylabel("height", fontsize=15)
plt.show(plot)
104/50:
# Lyft Dataset SDK dev-kit.
# Code written by Oscar Beijbom, 2018.
# Licensed under the Creative Commons [see licence.txt]
# Modified by Vladimir Iglovikov 2019.

class PointCloud(ABC):
    """
    Abstract class for manipulating and viewing point clouds.
    Every point cloud (lidar and radar) consists of points where:
    - Dimensions 0, 1, 2 represent x, y, z coordinates.
        These are modified when the point cloud is rotated or translated.
    - All other dimensions are optional. Hence these have to be manually modified if the reference frame changes.
    """

    def __init__(self, points: np.ndarray):
        """
        Initialize a point cloud and check it has the correct dimensions.
        :param points: <np.float: d, n>. d-dimensional input point cloud matrix.
        """
        assert points.shape[0] == self.nbr_dims(), (
            "Error: Pointcloud points must have format: %d x n" % self.nbr_dims()
        )
        self.points = points

    @staticmethod
    @abstractmethod
    def nbr_dims() -> int:
        """Returns the number of dimensions.
        Returns: Number of dimensions.
        """
        pass

    @classmethod
    @abstractmethod
    def from_file(cls, file_name: str) -> "PointCloud":
        """Loads point cloud from disk.
        Args:
            file_name: Path of the pointcloud file on disk.
        Returns: PointCloud instance.
        """
        pass

    @classmethod
    def from_file_multisweep(
        cls, lyftd, sample_rec: Dict, chan: str, ref_chan: str, num_sweeps: int = 26, min_distance: float = 1.0
    ) -> Tuple["PointCloud", np.ndarray]:
        """Return a point cloud that aggregates multiple sweeps.
        As every sweep is in a different coordinate frame, we need to map the coordinates to a single reference frame.
        As every sweep has a different timestamp, we need to account for that in the transformations and timestamps.
        Args:
            lyftd: A LyftDataset instance.
            sample_rec: The current sample.
            chan: The radar channel from which we track back n sweeps to aggregate the point cloud.
            ref_chan: The reference channel of the current sample_rec that the point clouds are mapped to.
            num_sweeps: Number of sweeps to aggregated.
            min_distance: Distance below which points are discarded.
        Returns: (all_pc, all_times). The aggregated point cloud and timestamps.
        """

        # Init
        points = np.zeros((cls.nbr_dims(), 0))
        all_pc = cls(points)
        all_times = np.zeros((1, 0))

        # Get reference pose and timestamp
        ref_sd_token = sample_rec["data"][ref_chan]
        ref_sd_rec = lyftd.get("sample_data", ref_sd_token)
        ref_pose_rec = lyftd.get("ego_pose", ref_sd_rec["ego_pose_token"])
        ref_cs_rec = lyftd.get("calibrated_sensor", ref_sd_rec["calibrated_sensor_token"])
        ref_time = 1e-6 * ref_sd_rec["timestamp"]

        # Homogeneous transform from ego car frame to reference frame
        ref_from_car = transform_matrix(ref_cs_rec["translation"], Quaternion(ref_cs_rec["rotation"]), inverse=True)

        # Homogeneous transformation matrix from global to _current_ ego car frame
        car_from_global = transform_matrix(
            ref_pose_rec["translation"], Quaternion(ref_pose_rec["rotation"]), inverse=True
        )

        # Aggregate current and previous sweeps.
        sample_data_token = sample_rec["data"][chan]
        current_sd_rec = lyftd.get("sample_data", sample_data_token)
        for _ in range(num_sweeps):
            # Load up the pointcloud.
            current_pc = cls.from_file(lyftd.data_path / ('train_' + current_sd_rec["filename"]))

            # Get past pose.
            current_pose_rec = lyftd.get("ego_pose", current_sd_rec["ego_pose_token"])
            global_from_car = transform_matrix(
                current_pose_rec["translation"], Quaternion(current_pose_rec["rotation"]), inverse=False
            )

            # Homogeneous transformation matrix from sensor coordinate frame to ego car frame.
            current_cs_rec = lyftd.get("calibrated_sensor", current_sd_rec["calibrated_sensor_token"])
            car_from_current = transform_matrix(
                current_cs_rec["translation"], Quaternion(current_cs_rec["rotation"]), inverse=False
            )

            # Fuse four transformation matrices into one and perform transform.
            trans_matrix = reduce(np.dot, [ref_from_car, car_from_global, global_from_car, car_from_current])
            current_pc.transform(trans_matrix)

            # Remove close points and add timevector.
            current_pc.remove_close(min_distance)
            time_lag = ref_time - 1e-6 * current_sd_rec["timestamp"]  # positive difference
            times = time_lag * np.ones((1, current_pc.nbr_points()))
            all_times = np.hstack((all_times, times))

            # Merge with key pc.
            all_pc.points = np.hstack((all_pc.points, current_pc.points))

            # Abort if there are no previous sweeps.
            if current_sd_rec["prev"] == "":
                break
            else:
                current_sd_rec = lyftd.get("sample_data", current_sd_rec["prev"])

        return all_pc, all_times

    def nbr_points(self) -> int:
        """Returns the number of points."""
        return self.points.shape[1]

    def subsample(self, ratio: float) -> None:
        """Sub-samples the pointcloud.
        Args:
            ratio: Fraction to keep.
        """
        selected_ind = np.random.choice(np.arange(0, self.nbr_points()), size=int(self.nbr_points() * ratio))
        self.points = self.points[:, selected_ind]

    def remove_close(self, radius: float) -> None:
        """Removes point too close within a certain radius from origin.
        Args:
            radius: Radius below which points are removed.
        Returns:
        """
        x_filt = np.abs(self.points[0, :]) < radius
        y_filt = np.abs(self.points[1, :]) < radius
        not_close = np.logical_not(np.logical_and(x_filt, y_filt))
        self.points = self.points[:, not_close]

    def translate(self, x: np.ndarray) -> None:
        """Applies a translation to the point cloud.
        Args:
            x: <np.float: 3, 1>. Translation in x, y, z.
        """
        for i in range(3):
            self.points[i, :] = self.points[i, :] + x[i]

    def rotate(self, rot_matrix: np.ndarray) -> None:
        """Applies a rotation.
        Args:
            rot_matrix: <np.float: 3, 3>. Rotation matrix.
        Returns:
        """
        self.points[:3, :] = np.dot(rot_matrix, self.points[:3, :])

    def transform(self, transf_matrix: np.ndarray) -> None:
        """Applies a homogeneous transform.
        Args:
            transf_matrix: transf_matrix: <np.float: 4, 4>. Homogenous transformation matrix.
        """
        self.points[:3, :] = transf_matrix.dot(np.vstack((self.points[:3, :], np.ones(self.nbr_points()))))[:3, :]

    def render_height(
        self,
        ax: Axes,
        view: np.ndarray = np.eye(4),
        x_lim: Tuple = (-20, 20),
        y_lim: Tuple = (-20, 20),
        marker_size: float = 1,
    ) -> None:
        """Simple method that applies a transformation and then scatter plots the points colored by height (z-value).
        Args:
            ax: Axes on which to render the points.
            view: <np.float: n, n>. Defines an arbitrary projection (n <= 4).
            x_lim: (min <float>, max <float>). x range for plotting.
            y_lim: (min <float>, max <float>). y range for plotting.
            marker_size: Marker size.
        """
        self._render_helper(2, ax, view, x_lim, y_lim, marker_size)

    def render_intensity(
        self,
        ax: Axes,
        view: np.ndarray = np.eye(4),
        x_lim: Tuple = (-20, 20),
        y_lim: Tuple = (-20, 20),
        marker_size: float = 1,
    ) -> None:
        """Very simple method that applies a transformation and then scatter plots the points colored by intensity.
        Args:
            ax: Axes on which to render the points.
            view: <np.float: n, n>. Defines an arbitrary projection (n <= 4).
            x_lim: (min <float>, max <float>).
            y_lim: (min <float>, max <float>).
            marker_size: Marker size.
        Returns:
        """
        self._render_helper(3, ax, view, x_lim, y_lim, marker_size)

    def _render_helper(
        self, color_channel: int, ax: Axes, view: np.ndarray, x_lim: Tuple, y_lim: Tuple, marker_size: float
    ) -> None:
        """Helper function for rendering.
        Args:
            color_channel: Point channel to use as color.
            ax: Axes on which to render the points.
            view: <np.float: n, n>. Defines an arbitrary projection (n <= 4).
            x_lim: (min <float>, max <float>).
            y_lim: (min <float>, max <float>).
            marker_size: Marker size.
        """
        points = view_points(self.points[:3, :], view, normalize=False)
        ax.scatter(points[0, :], points[1, :], c=self.points[color_channel, :], s=marker_size)
        ax.set_xlim(x_lim[0], x_lim[1])
        ax.set_ylim(y_lim[0], y_lim[1])


class LidarPointCloud(PointCloud):
    @staticmethod
    def nbr_dims() -> int:
        """Returns the number of dimensions.
        Returns: Number of dimensions.
        """
        return 4

    @classmethod
    def from_file(cls, file_name: Path) -> "LidarPointCloud":
        """Loads LIDAR data from binary numpy format. Data is stored as (x, y, z, intensity, ring index).
        Args:
            file_name: Path of the pointcloud file on disk.
        Returns: LidarPointCloud instance (x, y, z, intensity).
        """

        assert file_name.suffix == ".bin", "Unsupported filetype {}".format(file_name)

        scan = np.fromfile(str(file_name), dtype=np.float32)
        points = scan.reshape((-1, 5))[:, : cls.nbr_dims()]
        return cls(points.T)


class RadarPointCloud(PointCloud):

    # Class-level settings for radar pointclouds, see from_file().
    invalid_states = [0]  # type: List[int]
    dynprop_states = range(7)  # type: List[int] # Use [0, 2, 6] for moving objects only.
    ambig_states = [3]  # type: List[int]

    @staticmethod
    def nbr_dims() -> int:
        """Returns the number of dimensions.
        Returns: Number of dimensions.
        """
        return 18

    @classmethod
    def from_file(
        cls,
        file_name: Path,
        invalid_states: List[int] = None,
        dynprop_states: List[int] = None,
        ambig_states: List[int] = None,
    ) -> "RadarPointCloud":
        """Loads RADAR data from a Point Cloud Data file. See details below.
        Args:
            file_name: The path of the pointcloud file.
            invalid_states: Radar states to be kept. See details below.
            dynprop_states: Radar states to be kept. Use [0, 2, 6] for moving objects only. See details below.
            ambig_states: Radar states to be kept. See details below. To keep all radar returns,
                set each state filter to range(18).
        Returns: <np.float: d, n>. Point cloud matrix with d dimensions and n points.
        Example of the header fields:
        # .PCD v0.7 - Point Cloud Data file format
        VERSION 0.7
        FIELDS x y z dyn_prop id rcs vx vy vx_comp vy_comp is_quality_valid ambig_
                                                            state x_rms y_rms invalid_state pdh0 vx_rms vy_rms
        SIZE 4 4 4 1 2 4 4 4 4 4 1 1 1 1 1 1 1 1
        TYPE F F F I I F F F F F I I I I I I I I
        COUNT 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1
        WIDTH 125
        HEIGHT 1
        VIEWPOINT 0 0 0 1 0 0 0
        POINTS 125
        DATA binary
        Below some of the fields are explained in more detail:
        x is front, y is left
        vx, vy are the velocities in m/s.
        vx_comp, vy_comp are the velocities in m/s compensated by the ego motion.
        We recommend using the compensated velocities.
        invalid_state: state of Cluster validity state.
        (Invalid states)
        0x01    invalid due to low RCS
        0x02    invalid due to near-field artefact
        0x03    invalid far range cluster because not confirmed in near range
        0x05    reserved
        0x06    invalid cluster due to high mirror probability
        0x07    Invalid cluster because outside sensor field of view
        0x0d    reserved
        0x0e    invalid cluster because it is a harmonics
        (Valid states)
        0x00    valid
        0x04    valid cluster with low RCS
        0x08    valid cluster with azimuth correction due to elevation
        0x09    valid cluster with high child probability
        0x0a    valid cluster with high probability of being a 50 deg artefact
        0x0b    valid cluster but no local maximum
        0x0c    valid cluster with high artefact probability
        0x0f    valid cluster with above 95m in near range
        0x10    valid cluster with high multi-target probability
        0x11    valid cluster with suspicious angle
        dynProp: Dynamic property of cluster to indicate if is moving or not.
        0: moving
        1: stationary
        2: oncoming
        3: stationary candidate
        4: unknown
        5: crossing stationary
        6: crossing moving
        7: stopped
        ambig_state: State of Doppler (radial velocity) ambiguity solution.
        0: invalid
        1: ambiguous
        2: staggered ramp
        3: unambiguous
        4: stationary candidates
        pdh0: False alarm probability of cluster (i.e. probability of being an artefact caused
                                                                                    by multipath or similar).
        0: invalid
        1: <25%
        2: 50%
        3: 75%
        4: 90%
        5: 99%
        6: 99.9%
        7: <=100%
        """

        assert file_name.suffix == ".pcd", "Unsupported filetype {}".format(file_name)

        meta = []
        with open(str(file_name), "rb") as f:
            for line in f:
                line = line.strip().decode("utf-8")
                meta.append(line)
                if line.startswith("DATA"):
                    break

            data_binary = f.read()

        # Get the header rows and check if they appear as expected.
        assert meta[0].startswith("#"), "First line must be comment"
        assert meta[1].startswith("VERSION"), "Second line must be VERSION"
        sizes = meta[3].split(" ")[1:]
        types = meta[4].split(" ")[1:]
        counts = meta[5].split(" ")[1:]
        width = int(meta[6].split(" ")[1])
        height = int(meta[7].split(" ")[1])
        data = meta[10].split(" ")[1]
        feature_count = len(types)
        assert width > 0
        assert len([c for c in counts if c != c]) == 0, "Error: COUNT not supported!"
        assert height == 1, "Error: height != 0 not supported!"
        assert data == "binary"

        # Lookup table for how to decode the binaries.
        unpacking_lut = {
            "F": {2: "e", 4: "f", 8: "d"},
            "I": {1: "b", 2: "h", 4: "i", 8: "q"},
            "U": {1: "B", 2: "H", 4: "I", 8: "Q"},
        }
        types_str = "".join([unpacking_lut[t][int(s)] for t, s in zip(types, sizes)])

        # Decode each point.
        offset = 0
        point_count = width
        points = []
        for i in range(point_count):
            point = []
            for p in range(feature_count):
                start_p = offset
                end_p = start_p + int(sizes[p])
                assert end_p < len(data_binary)
                point_p = struct.unpack(types_str[p], data_binary[start_p:end_p])[0]
                point.append(point_p)
                offset = end_p
            points.append(point)

        # A NaN in the first point indicates an empty pointcloud.
        point = np.array(points[0])
        if np.any(np.isnan(point)):
            return cls(np.zeros((feature_count, 0)))

        # Convert to numpy matrix.
        points = np.array(points).transpose()

        # If no parameters are provided, use default settings.
        invalid_states = cls.invalid_states if invalid_states is None else invalid_states
        dynprop_states = cls.dynprop_states if dynprop_states is None else dynprop_states
        ambig_states = cls.ambig_states if ambig_states is None else ambig_states

        # Filter points with an invalid state.
        valid = [p in invalid_states for p in points[-4, :]]
        points = points[:, valid]

        # Filter by dynProp.
        valid = [p in dynprop_states for p in points[3, :]]
        points = points[:, valid]

        # Filter by ambig_state.
        valid = [p in ambig_states for p in points[11, :]]
        points = points[:, valid]

        return cls(points)


class Box:
    """ Simple data class representing a 3d box including, label, score and velocity. """

    def __init__(
        self,
        center: List[float],
        size: List[float],
        orientation: Quaternion,
        label: int = np.nan,
        score: float = np.nan,
        velocity: Tuple = (np.nan, np.nan, np.nan),
        name: str = None,
        token: str = None,
    ):
        """
        Args:
            center: Center of box given as x, y, z.
            size: Size of box in width, length, height.
            orientation: Box orientation.
            label: Integer label, optional.
            score: Classification score, optional.
            velocity: Box velocity in x, y, z direction.
            name: Box name, optional. Can be used e.g. for denote category name.
            token: Unique string identifier from DB.
        """
        assert not np.any(np.isnan(center))
        assert not np.any(np.isnan(size))
        assert len(center) == 3
        assert len(size) == 3
        assert type(orientation) == Quaternion

        self.center = np.array(center)
        self.wlh = np.array(size)
        self.orientation = orientation
        self.label = int(label) if not np.isnan(label) else label
        self.score = float(score) if not np.isnan(score) else score
        self.velocity = np.array(velocity)
        self.name = name
        self.token = token

    def __eq__(self, other):
        center = np.allclose(self.center, other.center)
        wlh = np.allclose(self.wlh, other.wlh)
        orientation = np.allclose(self.orientation.elements, other.orientation.elements)
        label = (self.label == other.label) or (np.isnan(self.label) and np.isnan(other.label))
        score = (self.score == other.score) or (np.isnan(self.score) and np.isnan(other.score))
        vel = np.allclose(self.velocity, other.velocity) or (
            np.all(np.isnan(self.velocity)) and np.all(np.isnan(other.velocity))
        )

        return center and wlh and orientation and label and score and vel

    def __repr__(self):
        repr_str = (
            "label: {}, score: {:.2f}, xyz: [{:.2f}, {:.2f}, {:.2f}], wlh: [{:.2f}, {:.2f}, {:.2f}], "
            "rot axis: [{:.2f}, {:.2f}, {:.2f}], ang(degrees): {:.2f}, ang(rad): {:.2f}, "
            "vel: {:.2f}, {:.2f}, {:.2f}, name: {}, token: {}"
        )

        return repr_str.format(
            self.label,
            self.score,
            self.center[0],
            self.center[1],
            self.center[2],
            self.wlh[0],
            self.wlh[1],
            self.wlh[2],
            self.orientation.axis[0],
            self.orientation.axis[1],
            self.orientation.axis[2],
            self.orientation.degrees,
            self.orientation.radians,
            self.velocity[0],
            self.velocity[1],
            self.velocity[2],
            self.name,
            self.token,
        )

    @property
    def rotation_matrix(self) -> np.ndarray:
        """Return a rotation matrix.
        Returns: <np.float: 3, 3>. The box's rotation matrix.
        """
        return self.orientation.rotation_matrix

    def translate(self, x: np.ndarray) -> None:
        """Applies a translation.
        Args:
            x: <np.float: 3, 1>. Translation in x, y, z direction.
        """
        self.center += x

    def rotate(self, quaternion: Quaternion) -> None:
        """Rotates box.
        Args:
            quaternion: Rotation to apply.
        """
        self.center = np.dot(quaternion.rotation_matrix, self.center)
        self.orientation = quaternion * self.orientation
        self.velocity = np.dot(quaternion.rotation_matrix, self.velocity)

    def corners(self, wlh_factor: float = 1.0) -> np.ndarray:
        """Returns the bounding box corners.
        Args:
            wlh_factor: Multiply width, length, height by a factor to scale the box.
        Returns: First four corners are the ones facing forward.
                The last four are the ones facing backwards.
        """

        width, length, height = self.wlh * wlh_factor

        # 3D bounding box corners. (Convention: x points forward, y to the left, z up.)
        x_corners = length / 2 * np.array([1, 1, 1, 1, -1, -1, -1, -1])
        y_corners = width / 2 * np.array([1, -1, -1, 1, 1, -1, -1, 1])
        z_corners = height / 2 * np.array([1, 1, -1, -1, 1, 1, -1, -1])
        corners = np.vstack((x_corners, y_corners, z_corners))

        # Rotate
        corners = np.dot(self.orientation.rotation_matrix, corners)

        # Translate
        x, y, z = self.center
        corners[0, :] = corners[0, :] + x
        corners[1, :] = corners[1, :] + y
        corners[2, :] = corners[2, :] + z

        return corners

    def bottom_corners(self) -> np.ndarray:
        """Returns the four bottom corners.
        Returns: <np.float: 3, 4>. Bottom corners. First two face forward, last two face backwards.
        """
        return self.corners()[:, [2, 3, 7, 6]]

    def render(
        self,
        axis: Axes,
        view: np.ndarray = np.eye(3),
        normalize: bool = False,
        colors: Tuple = ("b", "r", "k"),
        linewidth: float = 2,
    ):
        """Renders the box in the provided Matplotlib axis.
        Args:
            axis: Axis onto which the box should be drawn.
            view: <np.array: 3, 3>. Define a projection in needed (e.g. for drawing projection in an image).
            normalize: Whether to normalize the remaining coordinate.
            colors: (<Matplotlib.colors>: 3). Valid Matplotlib colors (<str> or normalized RGB tuple) for front,
            back and sides.
            linewidth: Width in pixel of the box sides.
        """
        corners = view_points(self.corners(), view, normalize=normalize)[:2, :]

        def draw_rect(selected_corners, color):
            prev = selected_corners[-1]
            for corner in selected_corners:
                axis.plot([prev[0], corner[0]], [prev[1], corner[1]], color=color, linewidth=linewidth)
                prev = corner

        # Draw the sides
        for i in range(4):
            axis.plot(
                [corners.T[i][0], corners.T[i + 4][0]],
                [corners.T[i][1], corners.T[i + 4][1]],
                color=colors[2],
                linewidth=linewidth,
            )

        # Draw front (first 4 corners) and rear (last 4 corners) rectangles(3d)/lines(2d)
        draw_rect(corners.T[:4], colors[0])
        draw_rect(corners.T[4:], colors[1])

        # Draw line indicating the front
        center_bottom_forward = np.mean(corners.T[2:4], axis=0)
        center_bottom = np.mean(corners.T[[2, 3, 7, 6]], axis=0)
        axis.plot(
            [center_bottom[0], center_bottom_forward[0]],
            [center_bottom[1], center_bottom_forward[1]],
            color=colors[0],
            linewidth=linewidth,
        )

    def render_cv2(
        self,
        image: np.ndarray,
        view: np.ndarray = np.eye(3),
        normalize: bool = False,
        colors: Tuple = ((0, 0, 255), (255, 0, 0), (155, 155, 155)),
        linewidth: int = 2,
    ) -> None:
        """Renders box using OpenCV2.
        Args:
            image: <np.array: width, height, 3>. Image array. Channels are in BGR order.
            view: <np.array: 3, 3>. Define a projection if needed (e.g. for drawing projection in an image).
            normalize: Whether to normalize the remaining coordinate.
            colors: ((R, G, B), (R, G, B), (R, G, B)). Colors for front, side & rear.
            linewidth: Linewidth for plot.
        Returns:
        """
        corners = view_points(self.corners(), view, normalize=normalize)[:2, :]

        def draw_rect(selected_corners, color):
            prev = selected_corners[-1]
            for corner in selected_corners:
                cv2.line(image, (int(prev[0]), int(prev[1])), (int(corner[0]), int(corner[1])), color, linewidth)
                prev = corner

        # Draw the sides
        for i in range(4):
            cv2.line(
                image,
                (int(corners.T[i][0]), int(corners.T[i][1])),
                (int(corners.T[i + 4][0]), int(corners.T[i + 4][1])),
                colors[2][::-1],
                linewidth,
            )

        # Draw front (first 4 corners) and rear (last 4 corners) rectangles(3d)/lines(2d)
        draw_rect(corners.T[:4], colors[0][::-1])
        draw_rect(corners.T[4:], colors[1][::-1])

        # Draw line indicating the front
        center_bottom_forward = np.mean(corners.T[2:4], axis=0)
        center_bottom = np.mean(corners.T[[2, 3, 7, 6]], axis=0)
        cv2.line(
            image,
            (int(center_bottom[0]), int(center_bottom[1])),
            (int(center_bottom_forward[0]), int(center_bottom_forward[1])),
            colors[0][::-1],
            linewidth,
        )

    def copy(self) -> "Box":
        """        Create a copy of self.
        Returns: A copy.
        """
        return copy.deepcopy(self)
104/51:
# Lyft Dataset SDK dev-kit.
# Code written by Oscar Beijbom, 2018.
# Licensed under the Creative Commons [see licence.txt]
# Modified by Vladimir Iglovikov 2019.

PYTHON_VERSION = sys.version_info[0]

if not PYTHON_VERSION == 3:
    raise ValueError("LyftDataset sdk only supports Python version 3.")


class LyftDataset:
    """Database class for Lyft Dataset to help query and retrieve information from the database."""

    def __init__(self, data_path: str, json_path: str, verbose: bool = True, map_resolution: float = 0.1):
        """Loads database and creates reverse indexes and shortcuts.
        Args:
            data_path: Path to the tables and data.
            json_path: Path to the folder with json files
            verbose: Whether to print status messages during load.
            map_resolution: Resolution of maps (meters).
        """

        self.data_path = Path(data_path).expanduser().absolute()
        self.json_path = Path(json_path)

        self.table_names = [
            "category",
            "attribute",
            "visibility",
            "instance",
            "sensor",
            "calibrated_sensor",
            "ego_pose",
            "log",
            "scene",
            "sample",
            "sample_data",
            "sample_annotation",
            "map",
        ]

        start_time = time.time()

        # Explicitly assign tables to help the IDE determine valid class members.
        self.category = self.__load_table__("category")
        self.attribute = self.__load_table__("attribute")
        self.visibility = self.__load_table__("visibility")
        self.instance = self.__load_table__("instance")
        self.sensor = self.__load_table__("sensor")
        self.calibrated_sensor = self.__load_table__("calibrated_sensor")
        self.ego_pose = self.__load_table__("ego_pose")
        self.log = self.__load_table__("log")
        self.scene = self.__load_table__("scene")
        self.sample = self.__load_table__("sample")
        self.sample_data = self.__load_table__("sample_data")
        self.sample_annotation = self.__load_table__("sample_annotation")
        self.map = self.__load_table__("map")

        # Initialize map mask for each map record.
        for map_record in self.map:
            map_record["mask"] = MapMask(self.data_path / 'train_maps/map_raster_palo_alto.png', resolution=map_resolution)

        if verbose:
            for table in self.table_names:
                print("{} {},".format(len(getattr(self, table)), table))
            print("Done loading in {:.1f} seconds.\n======".format(time.time() - start_time))

        # Make reverse indexes for common lookups.
        self.__make_reverse_index__(verbose)

        # Initialize LyftDatasetExplorer class
        self.explorer = LyftDatasetExplorer(self)

    def __load_table__(self, table_name) -> dict:
        """Loads a table."""
        with open(str(self.json_path.joinpath("{}.json".format(table_name)))) as f:
            table = json.load(f)
        return table

    def __make_reverse_index__(self, verbose: bool) -> None:
        """De-normalizes database to create reverse indices for common cases.
        Args:
            verbose: Whether to print outputs.
        """

        start_time = time.time()
        if verbose:
            print("Reverse indexing ...")

        # Store the mapping from token to table index for each table.
        self._token2ind = dict()
        for table in self.table_names:
            self._token2ind[table] = dict()

            for ind, member in enumerate(getattr(self, table)):
                self._token2ind[table][member["token"]] = ind

        # Decorate (adds short-cut) sample_annotation table with for category name.
        for record in self.sample_annotation:
            inst = self.get("instance", record["instance_token"])
            record["category_name"] = self.get("category", inst["category_token"])["name"]

        # Decorate (adds short-cut) sample_data with sensor information.
        for record in self.sample_data:
            cs_record = self.get("calibrated_sensor", record["calibrated_sensor_token"])
            sensor_record = self.get("sensor", cs_record["sensor_token"])
            record["sensor_modality"] = sensor_record["modality"]
            record["channel"] = sensor_record["channel"]

        # Reverse-index samples with sample_data and annotations.
        for record in self.sample:
            record["data"] = {}
            record["anns"] = []

        for record in self.sample_data:
            if record["is_key_frame"]:
                sample_record = self.get("sample", record["sample_token"])
                sample_record["data"][record["channel"]] = record["token"]

        for ann_record in self.sample_annotation:
            sample_record = self.get("sample", ann_record["sample_token"])
            sample_record["anns"].append(ann_record["token"])

        # Add reverse indices from log records to map records.
        if "log_tokens" not in self.map[0].keys():
            raise Exception("Error: log_tokens not in map table. This code is not compatible with the teaser dataset.")
        log_to_map = dict()
        for map_record in self.map:
            for log_token in map_record["log_tokens"]:
                log_to_map[log_token] = map_record["token"]
        for log_record in self.log:
            log_record["map_token"] = log_to_map[log_record["token"]]

        if verbose:
            print("Done reverse indexing in {:.1f} seconds.\n======".format(time.time() - start_time))

    def get(self, table_name: str, token: str) -> dict:
        """Returns a record from table in constant runtime.
        Args:
            table_name: Table name.
            token: Token of the record.
        Returns: Table record.
        """

        assert table_name in self.table_names, "Table {} not found".format(table_name)

        return getattr(self, table_name)[self.getind(table_name, token)]

    def getind(self, table_name: str, token: str) -> int:
        """Returns the index of the record in a table in constant runtime.
        Args:
            table_name: Table name.
            token: The index of the record in table, table is an array.
        Returns:
        """
        return self._token2ind[table_name][token]

    def field2token(self, table_name: str, field: str, query) -> List[str]:
        """Query all records for a certain field value, and returns the tokens for the matching records.
        Runs in linear time.
        Args:
            table_name: Table name.
            field: Field name.
            query: Query to match against. Needs to type match the content of the query field.
        Returns: List of tokens for the matching records.
        """
        matches = []
        for member in getattr(self, table_name):
            if member[field] == query:
                matches.append(member["token"])
        return matches

    def get_sample_data_path(self, sample_data_token: str) -> Path:
        """Returns the path to a sample_data.
        Args:
            sample_data_token:
        Returns:
        """

        sd_record = self.get("sample_data", sample_data_token)
        return self.data_path / sd_record["filename"]

    def get_sample_data(
        self,
        sample_data_token: str,
        box_vis_level: BoxVisibility = BoxVisibility.ANY,
        selected_anntokens: List[str] = None,
        flat_vehicle_coordinates: bool = False,
    ) -> Tuple[Path, List[Box], np.array]:
        """Returns the data path as well as all annotations related to that sample_data.
        The boxes are transformed into the current sensor's coordinate frame.
        Args:
            sample_data_token: Sample_data token.
            box_vis_level: If sample_data is an image, this sets required visibility for boxes.
            selected_anntokens: If provided only return the selected annotation.
            flat_vehicle_coordinates: Instead of current sensor's coordinate frame, use vehicle frame which is
        aligned to z-plane in world
        Returns: (data_path, boxes, camera_intrinsic <np.array: 3, 3>)
        """

        # Retrieve sensor & pose records
        sd_record = self.get("sample_data", sample_data_token)
        cs_record = self.get("calibrated_sensor", sd_record["calibrated_sensor_token"])
        sensor_record = self.get("sensor", cs_record["sensor_token"])
        pose_record = self.get("ego_pose", sd_record["ego_pose_token"])

        data_path = self.get_sample_data_path(sample_data_token)

        if sensor_record["modality"] == "camera":
            cam_intrinsic = np.array(cs_record["camera_intrinsic"])
            imsize = (sd_record["width"], sd_record["height"])
        else:
            cam_intrinsic = None
            imsize = None

        # Retrieve all sample annotations and map to sensor coordinate system.
        if selected_anntokens is not None:
            boxes = list(map(self.get_box, selected_anntokens))
        else:
            boxes = self.get_boxes(sample_data_token)

        # Make list of Box objects including coord system transforms.
        box_list = []
        for box in boxes:
            if flat_vehicle_coordinates:
                # Move box to ego vehicle coord system parallel to world z plane
                ypr = Quaternion(pose_record["rotation"]).yaw_pitch_roll
                yaw = ypr[0]

                box.translate(-np.array(pose_record["translation"]))
                box.rotate(Quaternion(scalar=np.cos(yaw / 2), vector=[0, 0, np.sin(yaw / 2)]).inverse)

            else:
                # Move box to ego vehicle coord system
                box.translate(-np.array(pose_record["translation"]))
                box.rotate(Quaternion(pose_record["rotation"]).inverse)

                #  Move box to sensor coord system
                box.translate(-np.array(cs_record["translation"]))
                box.rotate(Quaternion(cs_record["rotation"]).inverse)

            if sensor_record["modality"] == "camera" and not box_in_image(
                box, cam_intrinsic, imsize, vis_level=box_vis_level
            ):
                continue

            box_list.append(box)

        return data_path, box_list, cam_intrinsic

    def get_box(self, sample_annotation_token: str) -> Box:
        """Instantiates a Box class from a sample annotation record.
        Args:
            sample_annotation_token: Unique sample_annotation identifier.
        Returns:
        """
        record = self.get("sample_annotation", sample_annotation_token)
        return Box(
            record["translation"],
            record["size"],
            Quaternion(record["rotation"]),
            name=record["category_name"],
            token=record["token"],
        )

    def get_boxes(self, sample_data_token: str) -> List[Box]:
        """Instantiates Boxes for all annotation for a particular sample_data record. If the sample_data is a
        keyframe, this returns the annotations for that sample. But if the sample_data is an intermediate
        sample_data, a linear interpolation is applied to estimate the location of the boxes at the time the
        sample_data was captured.
        Args:
            sample_data_token: Unique sample_data identifier.
        Returns:
        """

        # Retrieve sensor & pose records
        sd_record = self.get("sample_data", sample_data_token)
        curr_sample_record = self.get("sample", sd_record["sample_token"])

        if curr_sample_record["prev"] == "" or sd_record["is_key_frame"]:
            # If no previous annotations available, or if sample_data is keyframe just return the current ones.
            boxes = list(map(self.get_box, curr_sample_record["anns"]))

        else:
            prev_sample_record = self.get("sample", curr_sample_record["prev"])

            curr_ann_recs = [self.get("sample_annotation", token) for token in curr_sample_record["anns"]]
            prev_ann_recs = [self.get("sample_annotation", token) for token in prev_sample_record["anns"]]

            # Maps instance tokens to prev_ann records
            prev_inst_map = {entry["instance_token"]: entry for entry in prev_ann_recs}

            t0 = prev_sample_record["timestamp"]
            t1 = curr_sample_record["timestamp"]
            t = sd_record["timestamp"]

            # There are rare situations where the timestamps in the DB are off so ensure that t0 < t < t1.
            t = max(t0, min(t1, t))

            boxes = []
            for curr_ann_rec in curr_ann_recs:

                if curr_ann_rec["instance_token"] in prev_inst_map:
                    # If the annotated instance existed in the previous frame, interpolate center & orientation.
                    prev_ann_rec = prev_inst_map[curr_ann_rec["instance_token"]]

                    # Interpolate center.
                    center = [
                        np.interp(t, [t0, t1], [c0, c1])
                        for c0, c1 in zip(prev_ann_rec["translation"], curr_ann_rec["translation"])
                    ]

                    # Interpolate orientation.
                    rotation = Quaternion.slerp(
                        q0=Quaternion(prev_ann_rec["rotation"]),
                        q1=Quaternion(curr_ann_rec["rotation"]),
                        amount=(t - t0) / (t1 - t0),
                    )

                    box = Box(
                        center,
                        curr_ann_rec["size"],
                        rotation,
                        name=curr_ann_rec["category_name"],
                        token=curr_ann_rec["token"],
                    )
                else:
                    # If not, simply grab the current annotation.
                    box = self.get_box(curr_ann_rec["token"])

                boxes.append(box)
        return boxes

    def box_velocity(self, sample_annotation_token: str, max_time_diff: float = 1.5) -> np.ndarray:
        """Estimate the velocity for an annotation.
        If possible, we compute the centered difference between the previous and next frame.
        Otherwise we use the difference between the current and previous/next frame.
        If the velocity cannot be estimated, values are set to np.nan.
        Args:
            sample_annotation_token: Unique sample_annotation identifier.
            max_time_diff: Max allowed time diff between consecutive samples that are used to estimate velocities.
        Returns: <np.float: 3>. Velocity in x/y/z direction in m/s.
        """

        current = self.get("sample_annotation", sample_annotation_token)
        has_prev = current["prev"] != ""
        has_next = current["next"] != ""

        # Cannot estimate velocity for a single annotation.
        if not has_prev and not has_next:
            return np.array([np.nan, np.nan, np.nan])

        if has_prev:
            first = self.get("sample_annotation", current["prev"])
        else:
            first = current

        if has_next:
            last = self.get("sample_annotation", current["next"])
        else:
            last = current

        pos_last = np.array(last["translation"])
        pos_first = np.array(first["translation"])
        pos_diff = pos_last - pos_first

        time_last = 1e-6 * self.get("sample", last["sample_token"])["timestamp"]
        time_first = 1e-6 * self.get("sample", first["sample_token"])["timestamp"]
        time_diff = time_last - time_first

        if has_next and has_prev:
            # If doing centered difference, allow for up to double the max_time_diff.
            max_time_diff *= 2

        if time_diff > max_time_diff:
            # If time_diff is too big, don't return an estimate.
            return np.array([np.nan, np.nan, np.nan])
        else:
            return pos_diff / time_diff

    def list_categories(self) -> None:
        self.explorer.list_categories()

    def list_attributes(self) -> None:
        self.explorer.list_attributes()

    def list_scenes(self) -> None:
        self.explorer.list_scenes()

    def list_sample(self, sample_token: str) -> None:
        self.explorer.list_sample(sample_token)

    def render_pointcloud_in_image(
        self,
        sample_token: str,
        dot_size: int = 5,
        pointsensor_channel: str = "LIDAR_TOP",
        camera_channel: str = "CAM_FRONT",
        out_path: str = None,
    ) -> None:
        self.explorer.render_pointcloud_in_image(
            sample_token,
            dot_size,
            pointsensor_channel=pointsensor_channel,
            camera_channel=camera_channel,
            out_path=out_path,
        )

    def render_sample(
        self,
        sample_token: str,
        box_vis_level: BoxVisibility = BoxVisibility.ANY,
        nsweeps: int = 1,
        out_path: str = None,
    ) -> None:
        self.explorer.render_sample(sample_token, box_vis_level, nsweeps=nsweeps, out_path=out_path)

    def render_sample_data(
        self,
        sample_data_token: str,
        with_anns: bool = True,
        box_vis_level: BoxVisibility = BoxVisibility.ANY,
        axes_limit: float = 40,
        ax: Axes = None,
        nsweeps: int = 1,
        out_path: str = None,
        underlay_map: bool = False,
    ) -> None:
        return self.explorer.render_sample_data(
            sample_data_token,
            with_anns,
            box_vis_level,
            axes_limit,
            ax,
            num_sweeps=nsweeps,
            out_path=out_path,
            underlay_map=underlay_map,
        )

    def render_annotation(
        self,
        sample_annotation_token: str,
        margin: float = 10,
        view: np.ndarray = np.eye(4),
        box_vis_level: BoxVisibility = BoxVisibility.ANY,
        out_path: str = None,
    ) -> None:
        self.explorer.render_annotation(sample_annotation_token, margin, view, box_vis_level, out_path)

    def render_instance(self, instance_token: str, out_path: str = None) -> None:
        self.explorer.render_instance(instance_token, out_path=out_path)

    def render_scene(self, scene_token: str, freq: float = 10, imwidth: int = 640, out_path: str = None) -> None:
        self.explorer.render_scene(scene_token, freq, image_width=imwidth, out_path=out_path)

    def render_scene_channel(
        self,
        scene_token: str,
        channel: str = "CAM_FRONT",
        freq: float = 10,
        imsize: Tuple[float, float] = (640, 360),
        out_path: str = None,
    ) -> None:
        self.explorer.render_scene_channel(
            scene_token=scene_token, channel=channel, freq=freq, image_size=imsize, out_path=out_path
        )

    def render_egoposes_on_map(self, log_location: str, scene_tokens: List = None, out_path: str = None) -> None:
        self.explorer.render_egoposes_on_map(log_location, scene_tokens, out_path=out_path)
104/52:
class LyftDatasetExplorer:
    """Helper class to list and visualize Lyft Dataset data. These are meant to serve as tutorials and templates for
    working with the data."""

    def __init__(self, lyftd: LyftDataset):
        self.lyftd = lyftd

    @staticmethod
    def get_color(category_name: str) -> Tuple[int, int, int]:
        """Provides the default colors based on the category names.
        This method works for the general Lyft Dataset categories, as well as the Lyft Dataset detection categories.
        Args:
            category_name:
        Returns:
        """
        if "bicycle" in category_name or "motorcycle" in category_name:
            return 255, 61, 99  # Red
        elif "vehicle" in category_name or category_name in ["bus", "car", "construction_vehicle", "trailer", "truck"]:
            return 255, 158, 0  # Orange
        elif "pedestrian" in category_name:
            return 0, 0, 230  # Blue
        elif "cone" in category_name or "barrier" in category_name:
            return 0, 0, 0  # Black
        else:
            return 255, 0, 255  # Magenta

    def list_categories(self) -> None:
        """Print categories, counts and stats."""

        print("Category stats")

        # Add all annotations
        categories = dict()
        for record in self.lyftd.sample_annotation:
            if record["category_name"] not in categories:
                categories[record["category_name"]] = []
            categories[record["category_name"]].append(record["size"] + [record["size"][1] / record["size"][0]])

        # Print stats
        for name, stats in sorted(categories.items()):
            stats = np.array(stats)
            print(
                "{:27} n={:5}, width={:5.2f}\u00B1{:.2f}, len={:5.2f}\u00B1{:.2f}, height={:5.2f}\u00B1{:.2f}, "
                "lw_aspect={:5.2f}\u00B1{:.2f}".format(
                    name[:27],
                    stats.shape[0],
                    np.mean(stats[:, 0]),
                    np.std(stats[:, 0]),
                    np.mean(stats[:, 1]),
                    np.std(stats[:, 1]),
                    np.mean(stats[:, 2]),
                    np.std(stats[:, 2]),
                    np.mean(stats[:, 3]),
                    np.std(stats[:, 3]),
                )
            )

    def list_attributes(self) -> None:
        """Prints attributes and counts."""
        attribute_counts = dict()
        for record in self.lyftd.sample_annotation:
            for attribute_token in record["attribute_tokens"]:
                att_name = self.lyftd.get("attribute", attribute_token)["name"]
                if att_name not in attribute_counts:
                    attribute_counts[att_name] = 0
                attribute_counts[att_name] += 1

        for name, count in sorted(attribute_counts.items()):
            print("{}: {}".format(name, count))

    def list_scenes(self) -> None:
        """ Lists all scenes with some meta data. """

        def ann_count(record):
            count = 0
            sample = self.lyftd.get("sample", record["first_sample_token"])
            while not sample["next"] == "":
                count += len(sample["anns"])
                sample = self.lyftd.get("sample", sample["next"])
            return count

        recs = [
            (self.lyftd.get("sample", record["first_sample_token"])["timestamp"], record)
            for record in self.lyftd.scene
        ]

        for start_time, record in sorted(recs):
            start_time = self.lyftd.get("sample", record["first_sample_token"])["timestamp"] / 1000000
            length_time = self.lyftd.get("sample", record["last_sample_token"])["timestamp"] / 1000000 - start_time
            location = self.lyftd.get("log", record["log_token"])["location"]
            desc = record["name"] + ", " + record["description"]
            if len(desc) > 55:
                desc = desc[:51] + "..."
            if len(location) > 18:
                location = location[:18]

            print(
                "{:16} [{}] {:4.0f}s, {}, #anns:{}".format(
                    desc,
                    datetime.utcfromtimestamp(start_time).strftime("%y-%m-%d %H:%M:%S"),
                    length_time,
                    location,
                    ann_count(record),
                )
            )

    def list_sample(self, sample_token: str) -> None:
        """Prints sample_data tokens and sample_annotation tokens related to the sample_token."""

        sample_record = self.lyftd.get("sample", sample_token)
        print("Sample: {}\n".format(sample_record["token"]))
        for sd_token in sample_record["data"].values():
            sd_record = self.lyftd.get("sample_data", sd_token)
            print(
                "sample_data_token: {}, mod: {}, channel: {}".format(
                    sd_token, sd_record["sensor_modality"], sd_record["channel"]
                )
            )
        print("")
        for ann_token in sample_record["anns"]:
            ann_record = self.lyftd.get("sample_annotation", ann_token)
            print("sample_annotation_token: {}, category: {}".format(ann_record["token"], ann_record["category_name"]))

    def map_pointcloud_to_image(self, pointsensor_token: str, camera_token: str) -> Tuple:
        """Given a point sensor (lidar/radar) token and camera sample_data token, load point-cloud and map it to
        the image plane.
        Args:
            pointsensor_token: Lidar/radar sample_data token.
            camera_token: Camera sample_data token.
        Returns: (pointcloud <np.float: 2, n)>, coloring <np.float: n>, image <Image>).
        """

        cam = self.lyftd.get("sample_data", camera_token)
        pointsensor = self.lyftd.get("sample_data", pointsensor_token)
        pcl_path = self.lyftd.data_path / ('train_' + pointsensor["filename"])
        if pointsensor["sensor_modality"] == "lidar":
            pc = LidarPointCloud.from_file(pcl_path)
        else:
            pc = RadarPointCloud.from_file(pcl_path)
        im = Image.open(str(self.lyftd.data_path / ('train_' + cam["filename"])))

        # Points live in the point sensor frame. So they need to be transformed via global to the image plane.
        # First step: transform the point-cloud to the ego vehicle frame for the timestamp of the sweep.
        cs_record = self.lyftd.get("calibrated_sensor", pointsensor["calibrated_sensor_token"])
        pc.rotate(Quaternion(cs_record["rotation"]).rotation_matrix)
        pc.translate(np.array(cs_record["translation"]))

        # Second step: transform to the global frame.
        poserecord = self.lyftd.get("ego_pose", pointsensor["ego_pose_token"])
        pc.rotate(Quaternion(poserecord["rotation"]).rotation_matrix)
        pc.translate(np.array(poserecord["translation"]))

        # Third step: transform into the ego vehicle frame for the timestamp of the image.
        poserecord = self.lyftd.get("ego_pose", cam["ego_pose_token"])
        pc.translate(-np.array(poserecord["translation"]))
        pc.rotate(Quaternion(poserecord["rotation"]).rotation_matrix.T)

        # Fourth step: transform into the camera.
        cs_record = self.lyftd.get("calibrated_sensor", cam["calibrated_sensor_token"])
        pc.translate(-np.array(cs_record["translation"]))
        pc.rotate(Quaternion(cs_record["rotation"]).rotation_matrix.T)

        # Fifth step: actually take a "picture" of the point cloud.
        # Grab the depths (camera frame z axis points away from the camera).
        depths = pc.points[2, :]

        # Retrieve the color from the depth.
        coloring = depths

        # Take the actual picture (matrix multiplication with camera-matrix + renormalization).
        points = view_points(pc.points[:3, :], np.array(cs_record["camera_intrinsic"]), normalize=True)

        # Remove points that are either outside or behind the camera. Leave a margin of 1 pixel for aesthetic reasons.
        mask = np.ones(depths.shape[0], dtype=bool)
        mask = np.logical_and(mask, depths > 0)
        mask = np.logical_and(mask, points[0, :] > 1)
        mask = np.logical_and(mask, points[0, :] < im.size[0] - 1)
        mask = np.logical_and(mask, points[1, :] > 1)
        mask = np.logical_and(mask, points[1, :] < im.size[1] - 1)
        points = points[:, mask]
        coloring = coloring[mask]

        return points, coloring, im

    def render_pointcloud_in_image(
        self,
        sample_token: str,
        dot_size: int = 2,
        pointsensor_channel: str = "LIDAR_TOP",
        camera_channel: str = "CAM_FRONT",
        out_path: str = None,
    ) -> None:
        """Scatter-plots a point-cloud on top of image.
        Args:
            sample_token: Sample token.
            dot_size: Scatter plot dot size.
            pointsensor_channel: RADAR or LIDAR channel name, e.g. 'LIDAR_TOP'.
            camera_channel: Camera channel name, e.g. 'CAM_FRONT'.
            out_path: Optional path to save the rendered figure to disk.
        Returns:
        """
        sample_record = self.lyftd.get("sample", sample_token)

        # Here we just grab the front camera and the point sensor.
        pointsensor_token = sample_record["data"][pointsensor_channel]
        camera_token = sample_record["data"][camera_channel]

        points, coloring, im = self.map_pointcloud_to_image(pointsensor_token, camera_token)
        plt.figure(figsize=(9, 16))
        plt.imshow(im)
        plt.scatter(points[0, :], points[1, :], c=coloring, s=dot_size)
        plt.axis("off")

        if out_path is not None:
            plt.savefig(out_path)

    def render_sample(
        self, token: str, box_vis_level: BoxVisibility = BoxVisibility.ANY, nsweeps: int = 1, out_path: str = None
    ) -> None:
        """Render all LIDAR and camera sample_data in sample along with annotations.
        Args:
            token: Sample token.
            box_vis_level: If sample_data is an image, this sets required visibility for boxes.
            nsweeps: Number of sweeps for lidar and radar.
            out_path: Optional path to save the rendered figure to disk.
        Returns:
        """
        record = self.lyftd.get("sample", token)

        # Separate RADAR from LIDAR and vision.
        radar_data = {}
        nonradar_data = {}
        for channel, token in record["data"].items():
            sd_record = self.lyftd.get("sample_data", token)
            sensor_modality = sd_record["sensor_modality"]
            if sensor_modality in ["lidar", "camera"]:
                nonradar_data[channel] = token
            else:
                radar_data[channel] = token

        num_radar_plots = 1 if len(radar_data) > 0 else 0

        # Create plots.
        n = num_radar_plots + len(nonradar_data)
        cols = 2
        fig, axes = plt.subplots(int(np.ceil(n / cols)), cols, figsize=(16, 24))

        if len(radar_data) > 0:
            # Plot radar into a single subplot.
            ax = axes[0, 0]
            for i, (_, sd_token) in enumerate(radar_data.items()):
                self.render_sample_data(
                    sd_token, with_anns=i == 0, box_vis_level=box_vis_level, ax=ax, num_sweeps=nsweeps
                )
            ax.set_title("Fused RADARs")

        # Plot camera and lidar in separate subplots.
        for (_, sd_token), ax in zip(nonradar_data.items(), axes.flatten()[num_radar_plots:]):
            self.render_sample_data(sd_token, box_vis_level=box_vis_level, ax=ax, num_sweeps=nsweeps)

        axes.flatten()[-1].axis("off")
        plt.tight_layout()
        fig.subplots_adjust(wspace=0, hspace=0)

        if out_path is not None:
            plt.savefig(out_path)

    def render_ego_centric_map(self, sample_data_token: str, axes_limit: float = 40, ax: Axes = None) -> None:
        """Render map centered around the associated ego pose.
        Args:
            sample_data_token: Sample_data token.
            axes_limit: Axes limit measured in meters.
            ax: Axes onto which to render.
        """

        def crop_image(image: np.array, x_px: int, y_px: int, axes_limit_px: int) -> np.array:
            x_min = int(x_px - axes_limit_px)
            x_max = int(x_px + axes_limit_px)
            y_min = int(y_px - axes_limit_px)
            y_max = int(y_px + axes_limit_px)

            cropped_image = image[y_min:y_max, x_min:x_max]

            return cropped_image

        sd_record = self.lyftd.get("sample_data", sample_data_token)

        # Init axes.
        if ax is None:
            _, ax = plt.subplots(1, 1, figsize=(9, 9))

        sample = self.lyftd.get("sample", sd_record["sample_token"])
        scene = self.lyftd.get("scene", sample["scene_token"])
        log = self.lyftd.get("log", scene["log_token"])
        map = self.lyftd.get("map", log["map_token"])
        map_mask = map["mask"]

        pose = self.lyftd.get("ego_pose", sd_record["ego_pose_token"])
        pixel_coords = map_mask.to_pixel_coords(pose["translation"][0], pose["translation"][1])

        scaled_limit_px = int(axes_limit * (1.0 / map_mask.resolution))
        mask_raster = map_mask.mask()

        cropped = crop_image(mask_raster, pixel_coords[0], pixel_coords[1], int(scaled_limit_px * math.sqrt(2)))

        ypr_rad = Quaternion(pose["rotation"]).yaw_pitch_roll
        yaw_deg = -math.degrees(ypr_rad[0])

        rotated_cropped = np.array(Image.fromarray(cropped).rotate(yaw_deg))
        ego_centric_map = crop_image(
            rotated_cropped, rotated_cropped.shape[1] / 2, rotated_cropped.shape[0] / 2, scaled_limit_px
        )
        ax.imshow(
            ego_centric_map, extent=[-axes_limit, axes_limit, -axes_limit, axes_limit], cmap="gray", vmin=0, vmax=150
        )

    def render_sample_data(
        self,
        sample_data_token: str,
        with_anns: bool = True,
        box_vis_level: BoxVisibility = BoxVisibility.ANY,
        axes_limit: float = 40,
        ax: Axes = None,
        num_sweeps: int = 1,
        out_path: str = None,
        underlay_map: bool = False,
    ):
        """Render sample data onto axis.
        Args:
            sample_data_token: Sample_data token.
            with_anns: Whether to draw annotations.
            box_vis_level: If sample_data is an image, this sets required visibility for boxes.
            axes_limit: Axes limit for lidar and radar (measured in meters).
            ax: Axes onto which to render.
            num_sweeps: Number of sweeps for lidar and radar.
            out_path: Optional path to save the rendered figure to disk.
            underlay_map: When set to true, LIDAR data is plotted onto the map. This can be slow.
        """

        # Get sensor modality.
        sd_record = self.lyftd.get("sample_data", sample_data_token)
        sensor_modality = sd_record["sensor_modality"]

        if sensor_modality == "lidar":
            # Get boxes in lidar frame.
            _, boxes, _ = self.lyftd.get_sample_data(
                sample_data_token, box_vis_level=box_vis_level, flat_vehicle_coordinates=True
            )

            # Get aggregated point cloud in lidar frame.
            sample_rec = self.lyftd.get("sample", sd_record["sample_token"])
            chan = sd_record["channel"]
            ref_chan = "LIDAR_TOP"
            pc, times = LidarPointCloud.from_file_multisweep(
                self.lyftd, sample_rec, chan, ref_chan, num_sweeps=num_sweeps
            )

            # Compute transformation matrices for lidar point cloud
            cs_record = self.lyftd.get("calibrated_sensor", sd_record["calibrated_sensor_token"])
            pose_record = self.lyftd.get("ego_pose", sd_record["ego_pose_token"])
            vehicle_from_sensor = np.eye(4)
            vehicle_from_sensor[:3, :3] = Quaternion(cs_record["rotation"]).rotation_matrix
            vehicle_from_sensor[:3, 3] = cs_record["translation"]

            ego_yaw = Quaternion(pose_record["rotation"]).yaw_pitch_roll[0]
            rot_vehicle_flat_from_vehicle = np.dot(
                Quaternion(scalar=np.cos(ego_yaw / 2), vector=[0, 0, np.sin(ego_yaw / 2)]).rotation_matrix,
                Quaternion(pose_record["rotation"]).inverse.rotation_matrix,
            )

            vehicle_flat_from_vehicle = np.eye(4)
            vehicle_flat_from_vehicle[:3, :3] = rot_vehicle_flat_from_vehicle

            # Init axes.
            if ax is None:
                _, ax = plt.subplots(1, 1, figsize=(9, 9))

            if underlay_map:
                self.render_ego_centric_map(sample_data_token=sample_data_token, axes_limit=axes_limit, ax=ax)

            # Show point cloud.
            points = view_points(
                pc.points[:3, :], np.dot(vehicle_flat_from_vehicle, vehicle_from_sensor), normalize=False
            )
            dists = np.sqrt(np.sum(pc.points[:2, :] ** 2, axis=0))
            colors = np.minimum(1, dists / axes_limit / np.sqrt(2))
            ax.scatter(points[0, :], points[1, :], c=colors, s=0.2)

            # Show ego vehicle.
            ax.plot(0, 0, "x", color="red")

            # Show boxes.
            if with_anns:
                for box in boxes:
                    c = np.array(self.get_color(box.name)) / 255.0
                    box.render(ax, view=np.eye(4), colors=(c, c, c))

            # Limit visible range.
            ax.set_xlim(-axes_limit, axes_limit)
            ax.set_ylim(-axes_limit, axes_limit)

        elif sensor_modality == "radar":
            # Get boxes in lidar frame.
            sample_rec = self.lyftd.get("sample", sd_record["sample_token"])
            lidar_token = sample_rec["data"]["LIDAR_TOP"]
            _, boxes, _ = self.lyftd.get_sample_data(lidar_token, box_vis_level=box_vis_level)

            # Get aggregated point cloud in lidar frame.
            # The point cloud is transformed to the lidar frame for visualization purposes.
            chan = sd_record["channel"]
            ref_chan = "LIDAR_TOP"
            pc, times = RadarPointCloud.from_file_multisweep(
                self.lyftd, sample_rec, chan, ref_chan, num_sweeps=num_sweeps
            )

            # Transform radar velocities (x is front, y is left), as these are not transformed when loading the point
            # cloud.
            radar_cs_record = self.lyftd.get("calibrated_sensor", sd_record["calibrated_sensor_token"])
            lidar_sd_record = self.lyftd.get("sample_data", lidar_token)
            lidar_cs_record = self.lyftd.get("calibrated_sensor", lidar_sd_record["calibrated_sensor_token"])
            velocities = pc.points[8:10, :]  # Compensated velocity
            velocities = np.vstack((velocities, np.zeros(pc.points.shape[1])))
            velocities = np.dot(Quaternion(radar_cs_record["rotation"]).rotation_matrix, velocities)
            velocities = np.dot(Quaternion(lidar_cs_record["rotation"]).rotation_matrix.T, velocities)
            velocities[2, :] = np.zeros(pc.points.shape[1])

            # Init axes.
            if ax is None:
                _, ax = plt.subplots(1, 1, figsize=(9, 9))

            # Show point cloud.
            points = view_points(pc.points[:3, :], np.eye(4), normalize=False)
            dists = np.sqrt(np.sum(pc.points[:2, :] ** 2, axis=0))
            colors = np.minimum(1, dists / axes_limit / np.sqrt(2))
            sc = ax.scatter(points[0, :], points[1, :], c=colors, s=3)

            # Show velocities.
            points_vel = view_points(pc.points[:3, :] + velocities, np.eye(4), normalize=False)
            max_delta = 10
            deltas_vel = points_vel - points
            deltas_vel = 3 * deltas_vel  # Arbitrary scaling
            deltas_vel = np.clip(deltas_vel, -max_delta, max_delta)  # Arbitrary clipping
            colors_rgba = sc.to_rgba(colors)
            for i in range(points.shape[1]):
                ax.arrow(points[0, i], points[1, i], deltas_vel[0, i], deltas_vel[1, i], color=colors_rgba[i])

            # Show ego vehicle.
            ax.plot(0, 0, "x", color="black")

            # Show boxes.
            if with_anns:
                for box in boxes:
                    c = np.array(self.get_color(box.name)) / 255.0
                    box.render(ax, view=np.eye(4), colors=(c, c, c))

            # Limit visible range.
            ax.set_xlim(-axes_limit, axes_limit)
            ax.set_ylim(-axes_limit, axes_limit)

        elif sensor_modality == "camera":
            # Load boxes and image.
            data_path, boxes, camera_intrinsic = self.lyftd.get_sample_data(
                sample_data_token, box_vis_level=box_vis_level
            )

            data = Image.open(str(data_path)[:len(str(data_path)) - 46] + 'train_images/' +\
                              str(data_path)[len(str(data_path)) - 39 : len(str(data_path))])

            # Init axes.
            if ax is None:
                _, ax = plt.subplots(1, 1, figsize=(9, 16))

            # Show image.
            ax.imshow(data)

            # Show boxes.
            if with_anns:
                for box in boxes:
                    c = np.array(self.get_color(box.name)) / 255.0
                    box.render(ax, view=camera_intrinsic, normalize=True, colors=(c, c, c))

            # Limit visible range.
            ax.set_xlim(0, data.size[0])
            ax.set_ylim(data.size[1], 0)

        else:
            raise ValueError("Error: Unknown sensor modality!")

        ax.axis("off")
        ax.set_title(sd_record["channel"])
        ax.set_aspect("equal")

        if out_path is not None:
            num = len([name for name in os.listdir(out_path)])
            out_path = out_path + str(num).zfill(5) + "_" + sample_data_token + ".png"
            plt.savefig(out_path)
            plt.close("all")
            return out_path

    def render_annotation(
        self,
        ann_token: str,
        margin: float = 10,
        view: np.ndarray = np.eye(4),
        box_vis_level: BoxVisibility = BoxVisibility.ANY,
        out_path: str = None,
    ) -> None:
        """Render selected annotation.
        Args:
            ann_token: Sample_annotation token.
            margin: How many meters in each direction to include in LIDAR view.
            view: LIDAR view point.
            box_vis_level: If sample_data is an image, this sets required visibility for boxes.
            out_path: Optional path to save the rendered figure to disk.
        """

        ann_record = self.lyftd.get("sample_annotation", ann_token)
        sample_record = self.lyftd.get("sample", ann_record["sample_token"])
        assert "LIDAR_TOP" in sample_record["data"].keys(), "No LIDAR_TOP in data, cant render"

        fig, axes = plt.subplots(1, 2, figsize=(18, 9))

        # Figure out which camera the object is fully visible in (this may return nothing)
        boxes, cam = [], []
        cams = [key for key in sample_record["data"].keys() if "CAM" in key]
        for cam in cams:
            _, boxes, _ = self.lyftd.get_sample_data(
                sample_record["data"][cam], box_vis_level=box_vis_level, selected_anntokens=[ann_token]
            )
            if len(boxes) > 0:
                break  # We found an image that matches. Let's abort.
        assert len(boxes) > 0, "Could not find image where annotation is visible. Try using e.g. BoxVisibility.ANY."
        assert len(boxes) < 2, "Found multiple annotations. Something is wrong!"

        cam = sample_record["data"][cam]

        # Plot LIDAR view
        lidar = sample_record["data"]["LIDAR_TOP"]
        data_path, boxes, camera_intrinsic = self.lyftd.get_sample_data(lidar, selected_anntokens=[ann_token])
        LidarPointCloud.from_file(Path(str(data_path)[:len(str(data_path)) - 46] + 'train_lidar/' +\
                                       str(data_path)[len(str(data_path)) - 40 : len(str(data_path))])).render_height(axes[0], view=view)
        for box in boxes:
            c = np.array(self.get_color(box.name)) / 255.0
            box.render(axes[0], view=view, colors=(c, c, c))
            corners = view_points(boxes[0].corners(), view, False)[:2, :]
            axes[0].set_xlim([np.min(corners[0, :]) - margin, np.max(corners[0, :]) + margin])
            axes[0].set_ylim([np.min(corners[1, :]) - margin, np.max(corners[1, :]) + margin])
            axes[0].axis("off")
            axes[0].set_aspect("equal")

        # Plot CAMERA view
        data_path, boxes, camera_intrinsic = self.lyftd.get_sample_data(cam, selected_anntokens=[ann_token])
        im = Image.open(Path(str(data_path)[:len(str(data_path)) - 46] + 'train_images/' +\
                             str(data_path)[len(str(data_path)) - 39 : len(str(data_path))]))
        axes[1].imshow(im)
        axes[1].set_title(self.lyftd.get("sample_data", cam)["channel"])
        axes[1].axis("off")
        axes[1].set_aspect("equal")
        for box in boxes:
            c = np.array(self.get_color(box.name)) / 255.0
            box.render(axes[1], view=camera_intrinsic, normalize=True, colors=(c, c, c))

        if out_path is not None:
            plt.savefig(out_path)

    def render_instance(self, instance_token: str, out_path: str = None) -> None:
        """Finds the annotation of the given instance that is closest to the vehicle, and then renders it.
        Args:
            instance_token: The instance token.
            out_path: Optional path to save the rendered figure to disk.
        Returns:
        """

        ann_tokens = self.lyftd.field2token("sample_annotation", "instance_token", instance_token)
        closest = [np.inf, None]
        for ann_token in ann_tokens:
            ann_record = self.lyftd.get("sample_annotation", ann_token)
            sample_record = self.lyftd.get("sample", ann_record["sample_token"])
            sample_data_record = self.lyftd.get("sample_data", sample_record["data"]["LIDAR_TOP"])
            pose_record = self.lyftd.get("ego_pose", sample_data_record["ego_pose_token"])
            dist = np.linalg.norm(np.array(pose_record["translation"]) - np.array(ann_record["translation"]))
            if dist < closest[0]:
                closest[0] = dist
                closest[1] = ann_token
        self.render_annotation(closest[1], out_path=out_path)

    def render_scene(self, scene_token: str, freq: float = 10, image_width: int = 640, out_path: Path = None) -> None:
        """Renders a full scene with all surround view camera channels.
        Args:
            scene_token: Unique identifier of scene to render.
            freq: Display frequency (Hz).
            image_width: Width of image to render. Height is determined automatically to preserve aspect ratio.
            out_path: Optional path to write a video file of the rendered frames.
        """

        if out_path is not None:
            assert out_path.suffix == ".avi"

        # Get records from DB.
        scene_rec = self.lyftd.get("scene", scene_token)
        first_sample_rec = self.lyftd.get("sample", scene_rec["first_sample_token"])
        last_sample_rec = self.lyftd.get("sample", scene_rec["last_sample_token"])

        channels = ["CAM_FRONT_LEFT", "CAM_FRONT", "CAM_FRONT_RIGHT", "CAM_BACK_LEFT", "CAM_BACK", "CAM_BACK_RIGHT"]

        horizontal_flip = ["CAM_BACK_LEFT", "CAM_BACK", "CAM_BACK_RIGHT"]  # Flip these for aesthetic reasons.

        time_step = 1 / freq * 1e6  # Time-stamps are measured in micro-seconds.

        window_name = "{}".format(scene_rec["name"])
        cv2.namedWindow(window_name)
        cv2.moveWindow(window_name, 0, 0)

        # Load first sample_data record for each channel
        current_recs = {}  # Holds the current record to be displayed by channel.
        prev_recs = {}  # Hold the previous displayed record by channel.
        for channel in channels:
            current_recs[channel] = self.lyftd.get("sample_data", first_sample_rec["data"][channel])
            prev_recs[channel] = None

        # We assume that the resolution is the same for all surround view cameras.
        image_height = int(image_width * current_recs[channels[0]]["height"] / current_recs[channels[0]]["width"])
        image_size = (image_width, image_height)

        # Set some display parameters
        layout = {
            "CAM_FRONT_LEFT": (0, 0),
            "CAM_FRONT": (image_size[0], 0),
            "CAM_FRONT_RIGHT": (2 * image_size[0], 0),
            "CAM_BACK_LEFT": (0, image_size[1]),
            "CAM_BACK": (image_size[0], image_size[1]),
            "CAM_BACK_RIGHT": (2 * image_size[0], image_size[1]),
        }

        canvas = np.ones((2 * image_size[1], 3 * image_size[0], 3), np.uint8)
        if out_path is not None:
            fourcc = cv2.VideoWriter_fourcc(*"MJPG")
            out = cv2.VideoWriter(out_path, fourcc, freq, canvas.shape[1::-1])
        else:
            out = None

        current_time = first_sample_rec["timestamp"]

        while current_time < last_sample_rec["timestamp"]:

            current_time += time_step

            # For each channel, find first sample that has time > current_time.
            for channel, sd_rec in current_recs.items():
                while sd_rec["timestamp"] < current_time and sd_rec["next"] != "":
                    sd_rec = self.lyftd.get("sample_data", sd_rec["next"])
                    current_recs[channel] = sd_rec

            # Now add to canvas
            for channel, sd_rec in current_recs.items():

                # Only update canvas if we have not already rendered this one.
                if not sd_rec == prev_recs[channel]:

                    # Get annotations and params from DB.
                    image_path, boxes, camera_intrinsic = self.lyftd.get_sample_data(
                        sd_rec["token"], box_vis_level=BoxVisibility.ANY
                    )

                    # Load and render
                    if not image_path.exists():
                        raise Exception("Error: Missing image %s" % image_path)
                    im = cv2.imread(str(image_path))
                    for box in boxes:
                        c = self.get_color(box.name)
                        box.render_cv2(im, view=camera_intrinsic, normalize=True, colors=(c, c, c))

                    im = cv2.resize(im, image_size)
                    if channel in horizontal_flip:
                        im = im[:, ::-1, :]

                    canvas[
                        layout[channel][1] : layout[channel][1] + image_size[1],
                        layout[channel][0] : layout[channel][0] + image_size[0],
                        :,
                    ] = im

                    prev_recs[channel] = sd_rec  # Store here so we don't render the same image twice.

            # Show updated canvas.
            cv2.imshow(window_name, canvas)
            if out_path is not None:
                out.write(canvas)

            key = cv2.waitKey(1)  # Wait a very short time (1 ms).

            if key == 32:  # if space is pressed, pause.
                key = cv2.waitKey()

            if key == 27:  # if ESC is pressed, exit.
                cv2.destroyAllWindows()
                break

        cv2.destroyAllWindows()
        if out_path is not None:
            out.release()

    def render_scene_channel(
        self,
        scene_token: str,
        channel: str = "CAM_FRONT",
        freq: float = 10,
        image_size: Tuple[float, float] = (640, 360),
        out_path: Path = None,
    ) -> None:
        """Renders a full scene for a particular camera channel.
        Args:
            scene_token: Unique identifier of scene to render.
            channel: Channel to render.
            freq: Display frequency (Hz).
            image_size: Size of image to render. The larger the slower this will run.
            out_path: Optional path to write a video file of the rendered frames.
        """

        valid_channels = [
            "CAM_FRONT_LEFT",
            "CAM_FRONT",
            "CAM_FRONT_RIGHT",
            "CAM_BACK_LEFT",
            "CAM_BACK",
            "CAM_BACK_RIGHT",
        ]

        assert image_size[0] / image_size[1] == 16 / 9, "Aspect ratio should be 16/9."
        assert channel in valid_channels, "Input channel {} not valid.".format(channel)

        if out_path is not None:
            assert out_path.suffix == ".avi"

        # Get records from DB
        scene_rec = self.lyftd.get("scene", scene_token)
        sample_rec = self.lyftd.get("sample", scene_rec["first_sample_token"])
        sd_rec = self.lyftd.get("sample_data", sample_rec["data"][channel])

        # Open CV init
        name = "{}: {} (Space to pause, ESC to exit)".format(scene_rec["name"], channel)
        cv2.namedWindow(name)
        cv2.moveWindow(name, 0, 0)

        if out_path is not None:
            fourcc = cv2.VideoWriter_fourcc(*"MJPG")
            out = cv2.VideoWriter(out_path, fourcc, freq, image_size)
        else:
            out = None

        has_more_frames = True
        while has_more_frames:

            # Get data from DB
            image_path, boxes, camera_intrinsic = self.lyftd.get_sample_data(
                sd_rec["token"], box_vis_level=BoxVisibility.ANY
            )

            # Load and render
            if not image_path.exists():
                raise Exception("Error: Missing image %s" % image_path)
            image = cv2.imread(str(image_path))
            for box in boxes:
                c = self.get_color(box.name)
                box.render_cv2(image, view=camera_intrinsic, normalize=True, colors=(c, c, c))

            # Render
            image = cv2.resize(image, image_size)
            cv2.imshow(name, image)
            if out_path is not None:
                out.write(image)

            key = cv2.waitKey(10)  # Images stored at approx 10 Hz, so wait 10 ms.
            if key == 32:  # If space is pressed, pause.
                key = cv2.waitKey()

            if key == 27:  # if ESC is pressed, exit
                cv2.destroyAllWindows()
                break

            if not sd_rec["next"] == "":
                sd_rec = self.lyftd.get("sample_data", sd_rec["next"])
            else:
                has_more_frames = False

        cv2.destroyAllWindows()
        if out_path is not None:
            out.release()

    def render_egoposes_on_map(
        self,
        log_location: str,
        scene_tokens: List = None,
        close_dist: float = 100,
        color_fg: Tuple[int, int, int] = (167, 174, 186),
        color_bg: Tuple[int, int, int] = (255, 255, 255),
        out_path: Path = None,
    ) -> None:
        """Renders ego poses a the map. These can be filtered by location or scene.
        Args:
            log_location: Name of the location, e.g. "singapore-onenorth", "singapore-hollandvillage",
                             "singapore-queenstown' and "boston-seaport".
            scene_tokens: Optional list of scene tokens.
            close_dist: Distance in meters for an ego pose to be considered within range of another ego pose.
            color_fg: Color of the semantic prior in RGB format (ignored if map is RGB).
            color_bg: Color of the non-semantic prior in RGB format (ignored if map is RGB).
            out_path: Optional path to save the rendered figure to disk.
        Returns:
        """

        # Get logs by location
        log_tokens = [l["token"] for l in self.lyftd.log if l["location"] == log_location]
        assert len(log_tokens) > 0, "Error: This split has 0 scenes for location %s!" % log_location

        # Filter scenes
        scene_tokens_location = [e["token"] for e in self.lyftd.scene if e["log_token"] in log_tokens]
        if scene_tokens is not None:
            scene_tokens_location = [t for t in scene_tokens_location if t in scene_tokens]
        if len(scene_tokens_location) == 0:
            print("Warning: Found 0 valid scenes for location %s!" % log_location)

        map_poses = []
        map_mask = None

        print("Adding ego poses to map...")
        for scene_token in tqdm(scene_tokens_location):

            # Get records from the database.
            scene_record = self.lyftd.get("scene", scene_token)
            log_record = self.lyftd.get("log", scene_record["log_token"])
            map_record = self.lyftd.get("map", log_record["map_token"])
            map_mask = map_record["mask"]

            # For each sample in the scene, store the ego pose.
            sample_tokens = self.lyftd.field2token("sample", "scene_token", scene_token)
            for sample_token in sample_tokens:
                sample_record = self.lyftd.get("sample", sample_token)

                # Poses are associated with the sample_data. Here we use the lidar sample_data.
                sample_data_record = self.lyftd.get("sample_data", sample_record["data"]["LIDAR_TOP"])
                pose_record = self.lyftd.get("ego_pose", sample_data_record["ego_pose_token"])

                # Calculate the pose on the map and append
                map_poses.append(
                    np.concatenate(
                        map_mask.to_pixel_coords(pose_record["translation"][0], pose_record["translation"][1])
                    )
                )

        # Compute number of close ego poses.
        print("Creating plot...")
        map_poses = np.vstack(map_poses)
        dists = sklearn.metrics.pairwise.euclidean_distances(map_poses * map_mask.resolution)
        close_poses = np.sum(dists < close_dist, axis=0)

        if len(np.array(map_mask.mask()).shape) == 3 and np.array(map_mask.mask()).shape[2] == 3:
            # RGB Colour maps
            mask = map_mask.mask()
        else:
            # Monochrome maps
            # Set the colors for the mask.
            mask = Image.fromarray(map_mask.mask())
            mask = np.array(mask)

            maskr = color_fg[0] * np.ones(np.shape(mask), dtype=np.uint8)
            maskr[mask == 0] = color_bg[0]
            maskg = color_fg[1] * np.ones(np.shape(mask), dtype=np.uint8)
            maskg[mask == 0] = color_bg[1]
            maskb = color_fg[2] * np.ones(np.shape(mask), dtype=np.uint8)
            maskb[mask == 0] = color_bg[2]
            mask = np.concatenate(
                (np.expand_dims(maskr, axis=2), np.expand_dims(maskg, axis=2), np.expand_dims(maskb, axis=2)), axis=2
            )

        # Plot.
        _, ax = plt.subplots(1, 1, figsize=(10, 10))
        ax.imshow(mask)
        title = "Number of ego poses within {}m in {}".format(close_dist, log_location)
        ax.set_title(title, color="k")
        sc = ax.scatter(map_poses[:, 0], map_poses[:, 1], s=10, c=close_poses)
        color_bar = plt.colorbar(sc, fraction=0.025, pad=0.04)
        plt.rcParams["figure.facecolor"] = "black"
        color_bar_ticklabels = plt.getp(color_bar.ax.axes, "yticklabels")
        plt.setp(color_bar_ticklabels, color="k")
        plt.rcParams["figure.facecolor"] = "white"  # Reset for future plots

        if out_path is not None:
            plt.savefig(out_path)
            plt.close("all")
104/53: lyft_dataset = LyftDataset(data_path=DATA_PATH, json_path=DATA_PATH+'train_data')
104/54:
my_scene = lyft_dataset.scene[0]
my_scene
104/55:
def render_scene(index):
    my_scene = lyft_dataset.scene[index]
    my_sample_token = my_scene["first_sample_token"]
    lyft_dataset.render_sample(my_sample_token)
104/56: render_scene(0)
104/57: render_scene(1)
104/58:
my_sample_token = my_scene["first_sample_token"]
my_sample = lyft_dataset.get('sample', my_sample_token)
104/59:
lyft_dataset.render_pointcloud_in_image(sample_token = my_sample["token"],
                                        dot_size = 1,
                                        camera_channel = 'CAM_FRONT')
104/60: my_sample['data']
104/61:
sensor_channel = 'CAM_FRONT'
my_sample_data = lyft_dataset.get('sample_data', my_sample['data'][sensor_channel])
104/62: lyft_dataset.render_sample_data(my_sample_data['token'])
104/63:
sensor_channel = 'CAM_BACK'
my_sample_data = lyft_dataset.get('sample_data', my_sample['data'][sensor_channel])
lyft_dataset.render_sample_data(my_sample_data['token'])
104/64:
sensor_channel = 'CAM_FRONT_LEFT'
my_sample_data = lyft_dataset.get('sample_data', my_sample['data'][sensor_channel])
lyft_dataset.render_sample_data(my_sample_data['token'])
104/65:
sensor_channel = 'CAM_FRONT_RIGHT'
my_sample_data = lyft_dataset.get('sample_data', my_sample['data'][sensor_channel])
lyft_dataset.render_sample_data(my_sample_data['token'])
104/66:
sensor_channel = 'CAM_BACK_LEFT'
my_sample_data = lyft_dataset.get('sample_data', my_sample['data'][sensor_channel])
lyft_dataset.render_sample_data(my_sample_data['token'])
104/67:
sensor_channel = 'CAM_BACK_RIGHT'
my_sample_data = lyft_dataset.get('sample_data', my_sample['data'][sensor_channel])
lyft_dataset.render_sample_data(my_sample_data['token'])
104/68:
my_annotation_token = my_sample['anns'][10]
my_annotation =  my_sample_data.get('sample_annotation', my_annotation_token)
104/69: lyft_dataset.render_annotation(my_annotation_token)
104/70:
my_instance = lyft_dataset.instance[100]
my_instance
104/71:
instance_token = my_instance['token']
lyft_dataset.render_instance(instance_token)
104/72: lyft_dataset.render_annotation(my_instance['last_annotation_token'])
104/73:
my_scene = lyft_dataset.scene[0]
my_sample_token = my_scene["first_sample_token"]
my_sample = lyft_dataset.get('sample', my_sample_token)
lyft_dataset.render_sample_data(my_sample['data']['LIDAR_TOP'], nsweeps=5)
104/74:
my_scene = lyft_dataset.scene[0]
my_sample_token = my_scene["first_sample_token"]
my_sample = lyft_dataset.get('sample', my_sample_token)
lyft_dataset.render_sample_data(my_sample['data']['LIDAR_FRONT_LEFT'], nsweeps=5)
104/75:
my_scene = lyft_dataset.scene[0]
my_sample_token = my_scene["first_sample_token"]
my_sample = lyft_dataset.get('sample', my_sample_token)
lyft_dataset.render_sample_data(my_sample['data']['LIDAR_FRONT_RIGHT'], nsweeps=5)
104/76:
def generate_next_token(scene):
    scene = lyft_dataset.scene[scene]
    sample_token = scene['first_sample_token']
    sample_record = lyft_dataset.get("sample", sample_token)
    
    while sample_record['next']:
        sample_token = sample_record['next']
        sample_record = lyft_dataset.get("sample", sample_token)
        
        yield sample_token

def animate_images(scene, frames, pointsensor_channel='LIDAR_TOP', interval=1):
    cams = [
        'CAM_FRONT',
        'CAM_FRONT_RIGHT',
        'CAM_BACK_RIGHT',
        'CAM_BACK',
        'CAM_BACK_LEFT',
        'CAM_FRONT_LEFT',
    ]

    generator = generate_next_token(scene)

    fig, axs = plt.subplots(
        2, len(cams), figsize=(3*len(cams), 6), 
        sharex=True, sharey=True, gridspec_kw = {'wspace': 0, 'hspace': 0.1}
    )
    
    plt.close(fig)

    def animate_fn(i):
        for _ in range(interval):
            sample_token = next(generator)
            
        for c, camera_channel in enumerate(cams):    
            sample_record = lyft_dataset.get("sample", sample_token)

            pointsensor_token = sample_record["data"][pointsensor_channel]
            camera_token = sample_record["data"][camera_channel]
            
            axs[0, c].clear()
            axs[1, c].clear()
            
            lyft_dataset.render_sample_data(camera_token, with_anns=False, ax=axs[0, c])
            lyft_dataset.render_sample_data(camera_token, with_anns=True, ax=axs[1, c])
            
            axs[0, c].set_title("")
            axs[1, c].set_title("")

    anim = animation.FuncAnimation(fig, animate_fn, frames=frames, interval=interval)
    
    return anim
104/77:
anim = animate_images(scene=3, frames=100, interval=1)
HTML(anim.to_jshtml(fps=8))
104/78:
anim = animate_images(scene=7, frames=100, interval=1)
HTML(anim.to_jshtml(fps=8))
104/79:
anim = animate_images(scene=4, frames=100, interval=1)
HTML(anim.to_jshtml(fps=8))
104/80:
def animate_lidar(scene, frames, pointsensor_channel='LIDAR_TOP', with_anns=True, interval=1):
    generator = generate_next_token(scene)

    fig, axs = plt.subplots(1, 1, figsize=(8, 8))
    plt.close(fig)

    def animate_fn(i):
        for _ in range(interval):
            sample_token = next(generator)
        
        axs.clear()
        sample_record = lyft_dataset.get("sample", sample_token)
        pointsensor_token = sample_record["data"][pointsensor_channel]
        lyft_dataset.render_sample_data(pointsensor_token, with_anns=with_anns, ax=axs)

    anim = animation.FuncAnimation(fig, animate_fn, frames=frames, interval=interval)
    
    return anim
104/81:
anim = animate_lidar(scene=5, frames=100, interval=1)
HTML(anim.to_jshtml(fps=8))
104/82:
anim = animate_lidar(scene=25, frames=100, interval=1)
HTML(anim.to_jshtml(fps=8))
104/83:
anim = animate_lidar(scene=10, frames=100, interval=1)
HTML(anim.to_jshtml(fps=8))
104/84:
from IPython.display import HTML
HTML('<center><iframe width="700" height="400" src="https://www.youtube.com/embed/tlThdr3O5Qo?rel=0&amp;controls=0&amp;showinfo=0" frameborder="0" allowfullscreen></iframe></center>')
104/85: HTML('<center><iframe width="700" height="400" src="https://www.youtube.com/embed/x7De3tCb3_A?rel=0&amp;controls=0&amp;showinfo=0" frameborder="0" allowfullscreen></iframe></center>')
104/86: !pip install pip install lyft-dataset-sdk
104/87:
import os
import gc
import numpy as np
import pandas as pd

import json
import math
import sys
import time
from datetime import datetime
from typing import Tuple, List

import cv2
import matplotlib.pyplot as plt
import sklearn.metrics
from PIL import Image

from matplotlib.axes import Axes
from matplotlib import animation, rc
import plotly.graph_objs as go
import plotly.tools as tls
from plotly.offline import plot, init_notebook_mode
import plotly.figure_factory as ff

init_notebook_mode(connected=True)

import seaborn as sns
from pyquaternion import Quaternion
from tqdm import tqdm

from lyft_dataset_sdk.utils.map_mask import MapMask
from lyft_dataset_sdk.lyftdataset import LyftDataset
from lyft_dataset_sdk.utils.geometry_utils import view_points, box_in_image, BoxVisibility
from lyft_dataset_sdk.utils.geometry_utils import view_points, transform_matrix
from pathlib import Path

import struct
from abc import ABC, abstractmethod
from functools import reduce
from typing import Tuple, List, Dict
import copy
104/88: DATA_PATH = '/Volumes/DRZ-Seagate/data/3d-object-detection-for-autonomous-vehicles/'
104/89:
train = pd.read_csv(DATA_PATH + 'train.csv')
sample_submission = pd.read_csv(DATA_PATH + 'sample_submission.csv')
104/90:
# Taken from https://www.kaggle.com/gaborfodor/eda-3d-object-detection-challenge

object_columns = ['sample_id', 'object_id', 'center_x', 'center_y', 'center_z',
                  'width', 'length', 'height', 'yaw', 'class_name']
objects = []
for sample_id, ps in tqdm(train.values[:]):
    object_params = ps.split()
    n_objects = len(object_params)
    for i in range(n_objects // 8):
        x, y, z, w, l, h, yaw, c = tuple(object_params[i * 8: (i + 1) * 8])
        objects.append([sample_id, i, x, y, z, w, l, h, yaw, c])
train_objects = pd.DataFrame(
    objects,
    columns = object_columns
)
104/91:
numerical_cols = ['object_id', 'center_x', 'center_y', 'center_z', 'width', 'length', 'height', 'yaw']
train_objects[numerical_cols] = np.float32(train_objects[numerical_cols].values)
104/92: train_objects.head()
104/93:
fig, ax = plt.subplots(figsize=(10, 10))
sns.distplot(train_objects['center_x'], color='darkorange', ax=ax).set_title('center_x and center_y', fontsize=16)
sns.distplot(train_objects['center_y'], color='purple', ax=ax).set_title('center_x and center_y', fontsize=16)
plt.xlabel('center_x and center_y', fontsize=15)
plt.show()
104/94:
fig, ax = plt.subplots(figsize=(10, 10))
sns.distplot(train_objects['center_z'], color='navy', ax=ax).set_title('center_z', fontsize=16)
plt.xlabel('center_z', fontsize=15)
plt.show()
104/95:
fig, ax = plt.subplots(figsize=(10, 10))
sns.distplot(train_objects['center_z'], color='navy', ax=ax).set_title('center_z', fontsize=16)
plt.xlabel('center_z', fontsize=15)
plt.show()
104/96:
fig, ax = plt.subplots(figsize=(10, 10))
plot = sns.countplot(y="class_name", data=train_objects.query('class_name != "animal"'),
                     palette=['navy', 'darkblue', 'blue', 'dodgerblue', 'skyblue', 'lightblue']).set_title('Object Frequencies', fontsize=16)
plt.yticks(fontsize=14)
plt.xlabel("Count", fontsize=15)
plt.ylabel("Class Name", fontsize=15)
plt.show(plot)
106/56: int('h')
106/57: 2^3
106/58: 2**3
106/59: ls
106/60: cd ..
106/61: ls
106/62: cd ../examples/librispeech
106/63: import preprocess
108/1: import preprocess
108/2: preprocess.check_phones()
109/1: import preprocess
109/2: preprocess.check_phones()
109/3: s = set([])
109/4: s.append(1)
109/5: s.add(1)
109/6: s
109/7: 1 in x
109/8: 1 in s
109/9: 0 in ss
109/10: 0 in s
110/1: import preprocess
110/2: preprocess.check_phones()
111/1: import preprocess
111/2: preprocess.check_phones()
112/1: import preprocess
112/2: preprocess.check_phones()
113/1: import preprocess
113/2: preprocess.check_phones()
114/1: import numpy as np
114/2: np.zeros((1,0))
114/3: np.zeros((1,1))
114/4: np.zeros((1,))
114/5: np.zeros((1,)).shape
114/6: np.zeros((1,0)).shape
114/7: np.zeros((1,1)).shape
113/3: 100
113/4: ov = 0.1
113/5: buff = 10
113/6: int(buff*(1+2*ov))
113/7: buff = 20
113/8: int(buff*(1+2*ov))
113/9: buff = 50
113/10: int(buff*(1+2*ov))
113/11: import numpy as np
113/12: n = np.array(1)
113/13: n.shape
113/14: n = np.array([1])
113/15: n.shape
113/16: n.shape[0]
115/1: from PIL import  Image
115/2: from PIL import PILLOW_VERSION
116/1: protoc --version
117/1: from onnx import onnx_pb
118/1: x = (([1]), ([2]))
118/2: x[0]
118/3: x[1]
119/1: x = (([1],), ([2],))
119/2: x[0]
119/3: x[0][0]
119/4: x[1][0]
120/1: from speech.models.transducer_model import Transducer
120/2: import torch
121/1: import torch
121/2: from speech.models.transducer_model import Transducer
122/1: d = dict()
122/2: d
122/3: d[0] = 5
122/4: d
122/5: d[4] = 5
122/6: d[2] = d[4] + 1
122/7: d
122/8:
class Node:
    def __init__(self,data):
        self.right=self.left=None
        self.data = data
122/9: root = Node()
122/10: root = Node(5)
122/11: r = "right"
122/12: d
122/13: d.items()
122/14: d.values()
122/15: max(d.values())
122/16: from functools import reduce
122/17: numbers = [1,2,3,4,5,6]
122/18: odd_numbers = filter(lambda n: n % 2 == 1, numbers)
122/19: odd_numbers
122/20: l = [i for i in odd_numbers]
122/21: l
122/22: odd_numbers
122/23: squared_odd_numbers = map(lambda n: n**2, odd_numbers)
122/24: sum_squared_odd_numbers = reduce(lambda acc, n: acc + n, squared_odd_numbers)
122/25: l
122/26: l = [i for i in odd_numbers]
122/27: l
122/28: odd_numbers = filter(lambda n: n % 2 == 1, numbers)
122/29: l = [i for i in odd_numbers]
122/30: l
122/31: l
122/32: l = [i for i in odd_numbers]
122/33: l
122/34: odd_numbers = filter(lambda n: n % 2 == 1, numbers)
122/35: squared_odd_numbers = map(lambda n: n**2, odd_numbers)
122/36: sum_squared_odd_numbers = reduce(lambda acc, n: acc + n, squared_odd_numbers)
122/37: sum_squared_odd_numbers
122/38: x='buffalo'
122/39: exec("%s = %d" % (x,2))
122/40: buffalo
124/1: t = 'test'
124/2: eval(t,'_run')
124/3: exec(t,'_run')
124/4: exec(t'_run')
124/5: exec('%s+_run'%t)
124/6: p = 'p'
124/7: rint = eval(p+'rint')
124/8: rint("hello")
125/1: lr = 'right'
125/2: eval("root."+lr+".data")
126/1: from itertools import chain
126/2: chain('abc', 'def')
126/3: cha = chain('abc', 'def')
126/4: [i for i in cha]
127/1: 4//2
127/2: [i for i in range(2,2)]
127/3: [i for i in range(2)]
127/4: [i for i in range(1,2)]
127/5: [i for i in range(1,2+1)]
127/6: [i for i in range(2,2+1)]
127/7: [i for i in range(2,2//2+1)]
127/8: [i for i in range(2,1//2+1)]
127/9: [i for i in range(2,2//2+1)]
127/10: [i for i in range(2,3//2+1)]
127/11: [i for i in range(2,4//2+1)]
127/12: [i for i in range(2,5//2+1)]
127/13: [i for i in range(2,6//2+1)]
127/14: [i for i in range(2,7//2+1)]
127/15: [i for i in range(2,8//2+1)]
128/1: import torch
129/1: import torch
129/2: x = torch.randn(2, 3, 4)
129/3: xv = x.view(2, -1)
129/4: x
129/5: xv
129/6: xsp = torch.split(x, dim=1)
129/7: xsp = torch.split(x, 3, dim=1)
129/8: xsp
129/9: xsp = torch.split(x, 2, dim=1)
129/10: xsp
129/11: xsp = torch.split(x, 4, dim=1)
129/12: xsp
129/13: x
129/14: x.size()
129/15: x.permute(0,1 -1).size()
129/16: x.permute(0,1, -1).size()
129/17: x.permute(0, -1).size()
129/18: x.transpose(0, -1).size()
129/19: x.size()
129/20: xsp = torch.split(x,[1,1,1], dim=2)
129/21: xsp = torch.split(x,[1,1,1,1], dim=2)
129/22: xsp
129/23: x
129/24: xv
129/25: xsp = torch.split(x,[1,1,1], dim=1)
129/26: xsp
129/27: xcat=torch.cat(xsp, dim=2)
129/28: xcat
129/29: xv
129/30: xcat.size()
129/31: xv
129/32: xv.size()
129/33: xcat = xcat.squeeze()
129/34: xcat.size()
129/35: xcat
129/36: xsp1 = torch.split(x,[1,1,1,1], dim=2)
129/37: xsp1
129/38: xst=torch.stack(xsp1, dim=2)
129/39: xst
129/40: xst.size()
129/41: xsp = torch.split(x,3, dim=1)
129/42: xsp
129/43: xsp = torch.split(x,[1,1,1], dim=1)
129/44: xsp
129/45: xsp = torch.split(x,3, dim=1)
129/46: xsp
129/47: torch.split(x,3, dim=1)
129/48: torch.split(x,2, dim=1)
129/49: torch.split(x,1, dim=1)
129/50: x
129/51: torch.split(x,1, dim=1)
129/52: xsp = torch.split(x,1, dim=1)
129/53: xcat = torch.cat(xsp, dim=2)
129/54: xcat
129/55: xv
129/56: p = torch.randn(1,196, 34, 32)
129/57: psp = torch.split(p,1, dim=2)
129/58: pcat = torch.cat(psp, dim=2)
129/59: print(torch.sum(p==pcat).items())
129/60: print(torch.sum(p==pcat)[0])
129/61: print(torch.sum(p==pcat).item()[0])
129/62: print(torch.sum(p==pcat).item())
129/63: p = torch.randn(1,196, 34, 32)
129/64: p = torch.split(p,1, dim=2)
129/65: p = torch.cat(p, dim=2)
129/66: p.size()
129/67: p = torch.randn(1,196, 34, 32)
129/68: p = torch.randn(1,196, 34, 32)
129/69: pv = p.view(1, 196 -1)
129/70: pv = p.view(1, 196, -1)
129/71: pv.size()
129/72: psp = torch.split(p,1, dim=2)
129/73: psp.size()
129/74: len(psp)
129/75: psp[0].size()
129/76: pcat = torch.cat(psp,dim=3)
129/77: pcat.size()
129/78: pcat.squeeze(2).size()
129/79: pcat = pcat.squeeze(2)
129/80: pcat.size()
129/81: pv.size()
129/82: print(torch.sum(pv==pcat).item())
129/83: p.size()
129/84: p = torch.split(p,1, dim=2)
129/85: p = torch.cat(p,dim=3)
129/86: p.size()
129/87: p.unsqueeze(2)
129/88: p = p.unsqueeze(2)
129/89: p.size()
129/90: p = p.squeeze(2)
129/91: p.size()
129/92: p = p.squeeze(2)
129/93: p.size()
129/94: print(torch.sum(pv==p).item())
130/1: l1 = [1,2,3]
130/2: l2 = [4,5,6]
130/3:
for i1, i2 in zip(l1, l2):
    print(i1, i2)
130/4: l3 = ['ab', 'cd', 'ef']
130/5: l3
130/6: l3 = ['ab', 'ef', 'cd']
130/7: l3.sort()
130/8: l3
130/9: 1&3
130/10: 1&5
131/1: import numpy
131/2: n = numpy.random.randn(1,3,224, 224)
131/3: n.shape()
131/4: n.shape
131/5: ls
135/1: cd speech/utils
135/2: cd speech/
135/3: ls
135/4: pwd
135/5: cd utils
135/6: import io
135/7: io.export_state_dict('/Users/dustin/CS/consulting/firstlayerai/phoneme_classification/src/awni_speech/speech/examples/librispeech/models/ctc_models/20200121/20200127/best_model', '/Users/dustin/CS/consulting/firstlayerai/phoneme_classification/src/awni_speech/speech/examples/librispeech/models/ctc_models/20200121/20200127/state_dict.pth')
135/8: export_state_dict('/Users/dustin/CS/consulting/firstlayerai/phoneme_classification/src/awni_speech/speech/examples/librispeech/models/ctc_models/20200121/20200127/best_model', '/Users/dustin/CS/consulting/firstlayerai/phoneme_classification/src/awni_speech/speech/examples/librispeech/models/ctc_models/20200121/20200127/state_dict.pth')
135/9: from io import export_state_dict
135/10: model_in_path = '/Users/dustin/CS/consulting/firstlayerai/phoneme_classification/src/awni_speech/speech/examples/librispeech/models/ctc_models/20200121/20200127/best_model'
135/11: import torch
135/12: model = torch.load(model_in_path, map_location=torch.device('cpu'))
135/13: model = torch.load(model_in_path, map_location=torch.device('cpu'))
135/14: model = torch.load(model_in_path, map_location=torch.device('cpu'))
136/1: import torch
136/2: model_in_path =
136/3: model_in_path = '/Users/dustin/CS/consulting/firstlayerai/phoneme_classification/src/awni_speech/speech/examples/librispeech/models/ctc_models/20200121/20200127/best_model'
136/4: model = torch.load(model_in_path, map_location=torch.device('cpu'))
136/5: ls
136/6: exitt
137/1: import torch
137/2: model_in_path='/Users/dustin/CS/consulting/firstlayerai/phoneme_classification/src/awni_speech/speech/examples/librispeech/models/ctc_models/20200121/20200127/best_model'
137/3: model = torch.load(model_in_path, map_location=torch.device('cpu'))
138/1: import torch
138/2: model_in_path = '/Users/dustin/CS/consulting/firstlayerai/phoneme_classification/src/awni_speech/speech/examples/librispeech/models/ctc_models/20200121/20200127/best_model'
138/3: model = torch.load(model_in_path, map_location=torch.device('cpu'))
141/1: ls
141/2: cd ..
141/3: ls
141/4: cd ..
141/5: cd speech/
141/6: ls
141/7: cd utils/
141/8: import utils.io
141/9: cd ..
141/10: import utils.io
141/11: cd utils/
141/12: import io
141/13: from io import load
141/14: io.load
141/15:
 with open('/Users/dustin/CS/consulting/firstlayerai/phoneme_classification/src/awni_speech/speech/examples/librispeech/models/ctc_models/20200115/20200120/best_preproc.pyc', 'rb') as fid:
        preproc = pickle.load(fid)
141/16: import pickle
141/17:
 with open('/Users/dustin/CS/consulting/firstlayerai/phoneme_classification/src/awni_speech/speech/examples/librispeech/models/ctc_models/20200115/20200120/best_preproc.pyc', 'rb') as fid:
        preproc = pickle.load(fid)
141/18: import torch
141/19: cd ..
141/20: import torch
141/21: cd ..
141/22: import torch
142/1: import torch
143/1: import torch
143/2: import sys
143/3: print(sys.path)
144/1: [i for i in range(2, 10, 2)]
144/2: [i for i in range(3, 10, 2)]
144/3: 10&1
145/1: x=5
145/2: x==5 or 3
145/3: x==5 and 3
145/4: x==4 or 3
145/5: x==(4 or 3)
145/6: x==(5 or 3)
145/7: x==(5 and 3)
145/8: x
145/9: y = (1, 2)
145/10:
def f(x, y):
    print(*y)
    return x, y
145/11: f(x, y)
145/12:
def f(x, y):
    print(*y)
    return x, *y
145/13:
def f(x, y):
    print(*y)
    return x, *y
145/14:
def f(x, y):
    print(*y)
    return x, y
145/15: f(x, y)
145/16:
def f(x, y):
    print(*y)
    return x, y
   exit
146/1: x = "LSTM"
146/2: x == ("GRU" or "LSTM")
146/3: x == ( "LSTM" or "GRU")
146/4: x == "LSTM" or x == "GRU"
146/5: x == "LSTM" or x == "GRU"
146/6: x="GRU"
146/7: x == "LSTM" or x == "GRU"
146/8: x="GU"
146/9: x == "LSTM" or x == "GRU"
146/10:
def pprint(x):
    print(x)
146/11: l=['hello', 1, 2]
146/12: pprint(*l)
146/13: a, b, c = *[pprint(i) for i in l]
146/14: a, b, c = [pprint(i) for i in l]
146/15: a
146/16: a, b, c = [i for i in l]
146/17: a
146/18: import numpy as np
146/19: n = np.random.randn(1, 3,3)
146/20: import torch
147/1: import numpy as np
147/2: import torch
148/1: import torch
149/1: import torch
150/1: import zipp
151/1: import torch
151/2: import zipp
151/3: import numpy as np
151/4: n = np.random.randn(1,3,3)
151/5: t = torch.from_numpy(n)
151/6: t
151/7: n
151/8: x, y = 4, 5
152/1: from collections import Counter
152/2: c = Counter('hello')
152/3: c['e']
152/4: c['l']
152/5: c['a']
152/6: s = 'hello'
152/7: s.count('l')
152/8: sset = set(s)
152/9: sset
152/10: dct = {let: s.count(let) for let in sset}
152/11: dct
152/12: dct['a']
152/13: from collections import defaultdict
152/14: d = defaultdict(int)
152/15: d = {let: s.count(let) for let in sset}
152/16: d['e']
152/17: d['a']
152/18: d = defaultdict(0)
152/19: d = defaultdict(int)
152/20: d['a']
152/21: d = {let: s.count(let) for let in sset}
152/22: d['a']
152/23: d = defaultdict({let: s.count(let) for let in sset})
152/24: d = defaultdict(int)
152/25:
for k in s:
    d[k]+=1
152/26: d
152/27: abs(5-4)
153/1: l=[[1,1], [4,1], [5,0], [6,1], [5,0]]
153/2: sorted(l, key=lambda x: x[0])
153/3: sorted(l, key=lambda x: x[1])
153/4: sorted(l, key=lambda x: x[0] if x[1]==1)
153/5: sorted(l, key=lambda x: x[0] and x[1]==1)
153/6: sorted(l, key=lambda x: x[0] and x[1]==1, reversed=True)
153/7: sorted(l, key=lambda x: x[0] and x[1]==1, reverse=True)
153/8: sorted(l, key=lambda x: x[0] and x[1]==1)
153/9: m = sorted(l, key=lambda x: x[0] and x[1]==1)
153/10: m
153/11: m.reverse()
153/12: m
153/13: mm
153/14: m
153/15: m = sorted(l, key=lambda x: x[0] and x[1]==1).reverse()
153/16: m
153/17: m
153/18: l
153/19: l.sort(key=lambda x: x[0] and x[1]==1).reverse()
153/20: l.sort(key=lambda x: x[0] and x[1]==1)
153/21: l
153/22: l.reverse()
153/23: l
153/24: l = [[5, 1], [2, 1], [1, 1], [8, 1], [10, 0], [5, 0]]
153/25: l
153/26: sorted(l, key=lambda x: x[0] and x[1]==1)
153/27: l.sort(key=lambda x: x[1], x[0])
153/28: l.sort(key=lambda x: (x[1], x[0]))
153/29: l
153/30: l.sort(key=lambda x: (x[1], x[0]), reverse=True)
153/31: l
153/32: l = [[5, 1], [2, 1], [1, 1], [8, 1], [10, 0], [5, 0]]
153/33: sorted(l, key=lambda x: (x[0], x[1]==1))
153/34: l.append([10,1])
153/35: l.append([10,2])
153/36: sorted(l, key=lambda x: (x[0], x[1]==1))
153/37: sorted([True, False])
153/38: sorted(l, key=lambda x: (x[1]==0, x[0]))
153/39: sorted(l, key=lambda x: (x[1]==0, x[0]), reverse=True)
153/40: sorted(l, key=lambda x: (x[1]==1, x[0]), reverse=True)
154/1: import sys
154/2: print(sys.path)
155/1: import sys
155/2: print(sys.path)
156/1: import sys
156/2: print(sys.path)
157/1: import speech.models
157/2: exti
158/1: import speech.model
158/2: import speech.models
158/3: ctc = model.ctc()
158/4: ctc = speech.models.ctc()
158/5: print(speech.models.__dir__())
158/6: ctc = speech.models.CTC()
158/7: ctc = speech.models.CTC(161, e
159/1: TRAINED_MODEL_FN = '/Users/dustin/CS/consulting/firstlayerai/phoneme_classification/src/awni_speech/speech/onnx_coreml/torch_models/20200121-0127_best_model.pth'
159/2: import torch
159/3: trained_model = torch.load(TRAINED_MODEL_FN, map_location=torch.device('cpu'))
159/4: trained_model
159/5: import numpy as np
159/6:
    test_x_zeros = np.zeros((1, 396, 161)).astype(np.float32)
    test_h_zeros = np.zeros((5, 1, 512)).astype(np.float32)
    test_c_zeros = np.zeros((5, 1, 512)).astype(np.float32)
159/7: trained_output = trained_model(torch.from_numpy(test_x_zeros),torch.from_numpy(test_h_zeros), torch.from_numpy(test_c_zeros))
160/1: import torch
160/2: TRAINED_MODEL_FN = '/Users/dustin/CS/consulting/firstlayerai/phoneme_classification/src/awni_speech/speech/onnx_coreml/torch_models/20200121-0127_best_model.pth'
160/3: trained_model
160/4: trained_model = torch.load(TRAINED_MODEL_FN, map_location=torch.device('cpu'))
161/1: import torch
161/2: TRAINED_MODEL_FN = '/Users/dustin/CS/consulting/firstlayerai/phoneme_classification/src/awni_speech/speech/onnx_coreml/torch_models/20200121-0127_best_model.pth'
161/3: trained_model = torch.load(TRAINED_MODEL_FN, map_location=torch.device('cpu'))
161/4: import numpy as np
161/5:
    test_x_zeros = np.zeros((1, 396, 161)).astype(np.float32)
    test_h_zeros = np.zeros((5, 1, 512)).astype(np.float32)
    test_c_zeros = np.zeros((5, 1, 512)).astype(np.float32)
161/6: trained_output = trained_model(torch.from_numpy(test_x_zeros),torch.from_numpy(test_h_zeros), torch.from_numpy(test_c_zeros))
161/7: trained_output = trained_model(torch.from_numpy(test_x_zeros))
162/1: import torch
163/1: import torch
164/1: import torch
165/1: import sys
165/2: print(sys.path)
165/3: exitt
165/4: exitr
166/1: import torch
166/2: conda source awni_env36
166/3: source activate awni_env36
167/1: import sys
167/2: print(sys.path)
168/1: import sys
168/2: print(sys.path)
169/1: import sys
169/2: print(sys.path)
170/1: import sys
170/2: print(sys.path)
171/1: import sys
171/2: print(sys.path)
172/1: import torch
172/2: TRAINED_MODEL_FN = '/Users/dustin/CS/consulting/firstlayerai/phoneme_classification/src/awni_speech/speech/onnx_coreml/torch_models/20200121-0127_best_model.pth'
172/3: trained_model = torch.load(TRAINED_MODEL_FN, map_location=torch.device('cpu'))
172/4: import numpy as np
172/5:
    test_x_zeros = np.zeros((1, 396, 161)).astype(np.float32)
    test_h_zeros = np.zeros((5, 1, 512)).astype(np.float32)
    test_c_zeros = np.zeros((5, 1, 512)).astype(np.float32)
172/6: trained_output = trained_model(torch.from_numpy(test_x_zeros))
173/1: import torch
173/2: TRAINED_MODEL_FN = '/Users/dustin/CS/consulting/firstlayerai/phoneme_classification/src/awni_speech/speech/onnx_coreml/torch_models/20200121-0127_best_model.pth'
173/3: trained_model = torch.load(TRAINED_MODEL_FN, map_location=torch.device('cpu'))
173/4: import numpy as np
173/5:
    test_x_zeros = np.zeros((1, 396, 161)).astype(np.float32)
    test_h_zeros = np.zeros((5, 1, 512)).astype(np.float32)
    test_c_zeros = np.zeros((5, 1, 512)).astype(np.float32)
173/6: trained_output = trained_model(torch.from_numpy(test_x_zeros))
173/7: trained_output
174/1: import torch
174/2: TRAINED_MODEL_FN = '/Users/dustin/CS/consulting/firstlayerai/phoneme_classification/src/awni_speech/speech/onnx_coreml/torch_models/20200121-0127_best_model.pth'
174/3: trained_model = torch.load(TRAINED_MODEL_FN, map_location=torch.device('cpu'))
174/4: import numpy as np
174/5:
    test_x_zeros = np.zeros((1, 396, 161)).astype(np.float32)
    test_h_zeros = np.zeros((5, 1, 512)).astype(np.float32)
    test_c_zeros = np.zeros((5, 1, 512)).astype(np.float32)
174/6: trained_output = trained_model(torch.from_numpy(test_x_zeros), torch.from_numpy(test_h_zeros), torch.from_numpy(test_c_zeros))
174/7: trained_output
175/1: import torch
175/2: TRAINED_MODEL_FN = '/Users/dustin/CS/consulting/firstlayerai/phoneme_classification/src/awni_speech/speech/onnx_coreml/torch_models/20200121-0127_best_model.pth'
175/3: trained_model = torch.load(TRAINED_MODEL_FN, map_location=torch.device('cpu'))
175/4: import numpy as np
175/5:
    test_x_zeros = np.zeros((1, 396, 161)).astype(np.float32)
    test_h_zeros = np.zeros((5, 1, 512)).astype(np.float32)
    test_c_zeros = np.zeros((5, 1, 512)).astype(np.float32)
175/6: trained_output = trained_model(torch.from_numpy(test_x_zeros), torch.from_numpy(test_h_zeros), torch.from_numpy(test_c_zeros))
176/1: import torch
176/2: TRAINED_MODEL_FN = '/Users/dustin/CS/consulting/firstlayerai/phoneme_classification/src/awni_speech/speech/onnx_coreml/torch_models/20200121-0127_best_model.pth'
176/3: trained_model = torch.load(TRAINED_MODEL_FN, map_location=torch.device('cpu'))
176/4: import numpy as np
176/5:
    test_x_zeros = np.zeros((1, 396, 161)).astype(np.float32)
    test_h_zeros = np.zeros((5, 1, 512)).astype(np.float32)
    test_c_zeros = np.zeros((5, 1, 512)).astype(np.float32)
176/6: trained_output = trained_model(torch.from_numpy(test_x_zeros))
177/1: import torch
177/2: TRAINED_MODEL_FN = '/Users/dustin/CS/consulting/firstlayerai/phoneme_classification/src/awni_speech/speech/onnx_coreml/torch_models/20200121-0127_best_model.pth'
177/3: trained_model = torch.load(TRAINED_MODEL_FN, map_location=torch.device('cpu'))
177/4: import numpy as np
177/5:
    test_x_zeros = np.zeros((1, 396, 161)).astype(np.float32)
    test_h_zeros = np.zeros((5, 1, 512)).astype(np.float32)
    test_c_zeros = np.zeros((5, 1, 512)).astype(np.float32)
177/6: trained_output = trained_model(torch.from_numpy(test_x_zeros))
177/7: trained_output = trained_model(torch.from_numpy(test_x_zeros), torch.from_numpy(test_h_zeros), torch.from_numpy(test_c_zeros))
177/8: trained_output = trained_model(torch.from_numpy(test_x_zeros), torch.from_numpy(test_h_zeros), torch.from_numpy(test_c_zeros))
178/1: import torch
178/2: TRAINED_MODEL_FN = '/Users/dustin/CS/consulting/firstlayerai/phoneme_classification/src/awni_speech/speech/onnx_coreml/torch_models/20200121-0127_best_model.pth'
178/3: trained_model = torch.load(TRAINED_MODEL_FN, map_location=torch.device('cpu'))
178/4: import numpy as np
178/5:
    test_x_zeros = np.zeros((1, 396, 161)).astype(np.float32)
    test_h_zeros = np.zeros((5, 1, 512)).astype(np.float32)
    test_c_zeros = np.zeros((5, 1, 512)).astype(np.float32)
178/6: trained_output = trained_model(torch.from_numpy(test_x_zeros), torch.from_numpy(test_h_zeros), torch.from_numpy(test_c_zeros))
178/7: trained_output = trained_model(torch.from_numpy(test_x_zeros), torch.from_numpy(test_h_zeros), torch.from_numpy(test_c_zeros))
178/8: trained_output = trained_model(torch.from_numpy(test_x_zeros), torch.from_numpy(test_h_zeros), torch.from_numpy(test_c_zeros))
178/9: trained_output = trained_model(torch.from_numpy(test_x_zeros), torch.from_numpy(test_h_zeros))
178/10: trained_output = trained_model(torch.from_numpy(test_x_zeros), torch.from_numpy(test_h_zeros), torch.from_numpy(test_c_zeros))
178/11: trained_output = trained_model(torch.from_numpy(test_x_zeros), torch.from_numpy(test_h_zeros), torch.from_numpy(test_c_zeros))
178/12: trained_output = trained_model(torch.from_numpy(test_x_zeros), (torch.from_numpy(test_h_zeros), torch.from_numpy(test_c_zeros)))
178/13: trained_output = trained_model(torch.from_numpy(test_x_zeros), h_prev=torch.from_numpy(test_h_zeros), c_prev=torch.from_numpy(test_c_zeros))
179/1: import torch.
179/2: import torch
179/3: state_dict_path = '/Users/dustin/CS/consulting/firstlayerai/phoneme_classification/src/awni_speech/speech/onnx_coreml/validation_scripts/state_params_20200121-0127.pth'
179/4: import speech.models as models
180/1: import import_export
180/2: import_export.export_torch_model()
180/3: import_export.export_torch_model()
181/1: import import_export
181/2: import_export.export_torch_model()
182/1: import import_export
182/2: import_export.export_torch_model()
183/1: bool('False')
183/2: bool(xit'False')
184/1: l = [2,1,3,5,6]
184/2: a = list(enumerate(l))
184/3: a
184/4: sorted(a, key=lambda x: x[1])
184/5: b =sorted(a, key=lambda x: x[1])
184/6: b[0]
184/7: b[0][0]
184/8: b[1][0]
184/9: b[1][1]
184/10: b[ 2][1]
184/11: a
184/12: l
184/13: dct = {k:v for k,v in enumerate(l)}
184/14: dct
184/15: 2 in dct
184/16: 6 in dct
184/17: 6 in dct.items()
184/18: 6 in dct.values()
184/19: dct = {c:i for i,c in enumerate(l)}
184/20: 6 in dct
184/21: dct
185/1: import torch
185/2: torch_path = './torch_models/20200211_model_win16-step8.pth'
185/3: torch_device = 'cpu'
185/4: ctc_model = torch.load(torch_path, map_location=torch.device(torch_device))
185/5: torch.save(ctc_model, torch_path)
185/6: ctc_model = torch.load(torch_path, map_location=torch.device(torch_device))
185/7: ctc_model.eval()
185/8: from get_test_input import generate_test_input
185/9: input_tensor = generate_test_input("pytorch", model_name, set_device=torch_device)
185/10: model_name = '20200211_model_win16-step8'
185/11: input_tensor = generate_test_input("pytorch", model_name, set_device=torch_device)
185/12: from get_paths import pytorch_onnx_paths
185/13: torch_path, onnx_path = pytorch_onnx_paths(model_name)
185/14: from import_export import torch_load, torch_onnx_export
185/15:     torch_onnx_export(ctc_model, input_tensor, onnx_path)
186/1: s="string'
186/2: s="string"
186/3: s[:-2]
187/1: s = "string"
187/2: i = 5
187/3: eval("print(i)")
187/4: eval("print(i, s)")
188/1: import validation as val
188/2: preproc_path = "./preproc/20200211-0212_w32-s16_3sec_preproc.pyc'
188/3: preproc_path = "./preproc/20200211-0212_w32-s16_3sec_preproc.pyc"
188/4: time_dim = 186
188/5: freq_dim = 257
188/6: test_data = val.gen_test_data(preproc_path, time_dim, freq_dim)
188/7: print(type(test_data["Speak_5_out"][0]))
188/8: print(type(test_data["Speak_5_out"][0]).shape)
188/9: print(test_data["Speak_5_out"][0].shape)
188/10: test_x = test_data["Speak_5_out"][0]
188/11: text_h = test_data["Speak_5_out"][1]
188/12: test_h = test_data["Speak_5_out"][1]
188/13: test_c = test_data["Speak_5_out"][2]
188/14: test_x.shape
188/15: test_x[:, 0:14, :].shape
188/16: test_x[:,:14, :].shape
188/17: test_x[:,0:14, :].shape
188/18: test_x[:,14:28, :].shape
188/19: help(torch.arange)
188/20: import torch
188/21: help(torch.arange)
188/22: a = torch.arange(0,17)
188/23: a
188/24: a = torch.arange(0,27)
188/25: help(torch.reshape)
188/26: torch.reshape(a, [9,3])
188/27: a
188/28: a = torch.reshape(a, [9,3])
188/29: a
188/30: a = torch.arange(0,27)
188/31: a
188/32: help(torch.nn.conv1d)
188/33: help(torch.nn.conv2d)
188/34: help(torch.nn.Conv2d)
188/35: help(torch.nn.Conv1d)
188/36: conv1 = torch.nn.Conv1d(1, 1, 3)
188/37: out_full = conv1(a)
188/38: a
188/39: help(torch.nn.Conv1d)
188/40: a_full = torch.unsqueeze(a)
188/41: a_full = torch.unsqueeze(a, 0)
188/42: a_full.shape
188/43: out_full = conv1(a_full)
188/44: help(torch.nn.Conv1d)
188/45: a_full = torch.unsqueeze(a, 0)
188/46: a_full = torch.unsqueeze(a_full, 0)
188/47: a_full.shape
188/48: out_full = conv1(a_full)
188/49: help(torch.arange)
188/50: a = torch.arange(0,27, dtype=float32)
188/51: a = torch.arange(0,27, dtype=torch.float32)
188/52: help(torch.unsqueeze)
188/53: a_full = torch.unsqueeze(a, 0)
188/54: a_full = torch.unsqueeze(a_full, 0)
188/55: a_full.shape
188/56: a_full.type
188/57: a_full.dtype
188/58: out_full = conv1(a_full)
188/59: out_full.shape
188/60: conv
188/61: conv1
188/62: out_full
188/63: help(torch.split)
188/64: help(torch.split)
188/65: a_chunk = torch.split(a, split_size_or_sections=3)
188/66: a_chunk
188/67: out_chunk= ()
188/68: out_chunk.append(5)
188/69: a_chunk[0]
188/70: conv1(a_chunk[0])
188/71: help(torch.split)
188/72: a_chunk
188/73: a_chunk = map(torch.unsqueeze, a_chunk)
188/74: a_chunk
188/75: a_chunk = list(a_chunk)
188/76: a_chunk = torch.split(a, split_size_or_sections=3)
188/77: a_chunk = list(map(torch.unsqueeze(dim=0), a_chunk))
189/1: list(range(1))
189/2: excit
190/1: import numpy as np
190/2: n = np.array()
190/3: n = np.empty()
190/4: help(np.empty)
190/5: import editdistance as ed
190/6: c14 =  ['ih', 'l', 'm', 'iy', 'w', 'iy', 'r', 'dh', 'eh', 'ah', 'v', 'f']
190/7: full=['k', 'uh', 'ch', 'uw', 't', 'ih', 'l', 'm', 'iy', 'w', 'eh', 'r', 'd', 'dh', 'ow', 'ah', 'v', 'f', 'iy']
190/8: editdistance.eval(full, c14)
190/9: ed.eval(full, c14)
190/10: help(ed.eval)
190/11: ed.eval(c14, full)
190/12: c30 = ['jh', 'uh', 't', 'ih', 'l', 'm', 'iy', 'w', 'eh', 'r', 'dh', 'eh', 'ah', 'v', 'f', 'iy']
190/13: ed.eval(c30, full)
190/14: c40 =  ['ih', 'l', 'm', 'iy', 'w', 'iy', 'r', 'dh', 'eh', 'ah', 'v', 'f', 'iy']
190/15: ed.eval(c40, full)
190/16: c50 =['k', 'uh', 'ch', 'ah', 't', 'ih', 'l', 'm', 'iy', 'w', 'eh', 'r', 'd', 'dh', 'ow', 'ah', 'v', 'f', 'iy']
190/17: ed.eval(c50, full)
190/18: c60=['k', 'uh', 'ch', 'ih', 't', 'ih', 'l', 'm', 'iy', 'w', 'eh', 'r', 'dh', 'eh', 'ah', 'v', 'f', 'ih']
190/19: ed.eval(c60, full)
190/20: c80 =  ['k', 'uh', 'ch', 'uw', 't', 'ih', 'l', 'iy', 'w', 'eh', 'r', 'd', 'dh', 'ow', 'f', 'iy']
190/21: ed.eval(c80, full)
190/22: c100=['k', 'uh', 'ch', 'uw', 't', 'ih', 'l', 'm', 'iy', 'w', 'eh', 'r', 'd', 'dh', 'ow', 'ah', 'v', 'f', 'iy']
190/23: ed.eval(c100, full)
190/24: c120=['k', 'uh', 'ch', 'uw', 't', 'ih', 'l', 'm', 'iy', 'w', 'eh', 'r', 'dh', 'eh', 'ah', 'f', 'iy']
190/25: ed.eval(c120, full)
190/26: d = c = 5
190/27: d
190/28: c
192/1: import numpy as np
192/2: e = np.eye(5)
192/3: e
192/4: e = np.eye(10)
192/5: win=2
192/6: chan=5
192/7: e = np.eye(win*chan)
192/8: er = e.reshape(win, chan, win_chan)
192/9: er = e.reshape(win, chan, win*chan)
192/10: er
192/11: er.shape
192/12: er[0, :, :]
192/13: e
192/14: er
192/15: win=3
192/16: e = np.eye(win*chan)
192/17: er = e.reshape(win, chan, win*chan)
192/18: e
192/19: er
192/20: e
192/21: e
192/22: numpy.core.arrayprint._line_width = 160
192/23: np.core.arrayprint._line_width = 160
192/24: e
192/25: er
192/26: er.shape
192/27: e.shape
192/28: import torch
192/29: %env
192/30: a_range = torch.arange(100)
192/31: a_range.reshape(10,10)
192/32: a_range.shape
192/33: ar = a_range.reshape(10,10)
192/34: ar.shape
192/35: ar
192/36: ar.unsqueeze(0)
192/37: ar.shape
192/38: ar = ar.unsqueeze(0)
192/39: ar.shape
192/40: help(torch.conv2d)
192/41: help(torch.nn.conv2d)
192/42: help(torch.nn.Conv2d)
192/43: help(torch.nn.Conv2d)
192/44: conv1 = torch.nn.Conv2d(1, 3, padding=(0,3))
192/45: help(torch.nn.conv2d)
192/46: help(torch.nn.Conv2d)
192/47: conv1 = torch.nn.Conv2d(1, 1, 3, padding=(0,3))
192/48: conv1_out = conv1(ar)
192/49: ar = ar.unsqueeze(0)
192/50: conv1_out = conv1(ar)
192/51: ar = ar.asType(torch.float32)
192/52: ar = ar.astype(torch.float32)
192/53: ar = ar.type(torch.float32)
192/54: ar.type
192/55: ar.dtype
192/56: conv1_out = conv1(ar)
192/57: conv1_out
192/58: conv1_out.shape
192/59: conv2 = torch.nn.Conv2d(1, 1, 3)
192/60: conv2_out = conv2(ar)
192/61: conv2_out.shape
192/62: conv2_out
192/63: torch.nn.functional.pad(ar, (0,0,0,0,0,0,3,3))
192/64: torch.nn.functional.pad(ar, (0,0,0,0,0,0,3,3)).shape
192/65: ar.shape
192/66: torch.nn.functional.pad(ar, (3,3,0,0,0,0,0,0)).shape
192/67: ar_pad = torch.nn.functional.pad(ar, (3,3,0,0,0,0,0,0))
192/68: %history
192/69: conv1_nopad = torch.nn.Conv2d(1, 1, 3)
192/70: ar_pad.shape
192/71: help(torch.slice)
192/72: help(torch.split)
192/73: conv1_out
192/74: ar
192/75: conv1
192/76: %hist -h
192/77: %hist -help
192/78: %hist -g conv1
192/79: %recall 44
192/80: conv1 = torch.nn.Conv2d(1, 3, padding=(3,0))
192/81: conv1 = torch.nn.Conv2d(1, 1, 3, padding=(3,0))
192/82: %rerun 48
192/83: ar.shape
192/84: conv1_out.shape
192/85: conv1 = torch.nn.Conv2d(1, 1, 3,)
192/86: %hist -g pad
192/87: %recall 67
192/88: ar_pad = torch.nn.functional.pad(ar, (3,0,0,0,0,0,0,0))
192/89: ar.shape
192/90: ar_pad.shape
192/91: ar_pad = torch.nn.functional.pad(ar, (0,0,3,0,0,0,0,0))
192/92: ar_pad.shape
192/93: ar
192/94: ar_pad
192/95: ar_pad[0][0]
192/96: ar_pad[0][0].shape
192/97: conv1
192/98: ar_pad = torch.nn.functional.pad(ar, (0,0,2,0,0,0,0,0))
192/99: ar_pad.shape
192/100: conv1_out = conv1(ar_pad)
192/101: conv1_out.shape
192/102: ar
192/103: ar.shape
192/104: conv1 = torch.nn.Conv2d(1, 1, 3)
192/105: conv2 = conv1
192/106: conv3 = conv2
192/107: conv3
192/108: conv1_out = conv1(ar)
192/109: conv1_out.shape
192/110: conv2_out = conv2(conv1_out)
192/111: conv2_out.shape
192/112: conv3_out = conv3(conv2_out)
192/113: conv3_out.shape
192/114: conv3_out
192/115: ar[:,:,0:7,:].shape
192/116: ar[:,:,1:8,:].shape
192/117: ar
192/118: ar[:,:,0:7,:]
192/119:
def ar_slice(ar):
    for i in range(0);
192/120:
def ar_slice(ar):
    ar_slice = []
    for i in range(0):
        ar_slice.append(ar[:,:,i:i+7,:])
    return ar_slice
192/121: ar_sliced = ar_slice(ar)
192/122: ar_sliced
192/123: ar
192/124:
def ar_slice(ar):
    ar_slice = []
    for i in range(4):
        ar_slice.append(ar[:,:,i:i+7,:])
    return ar_slice
192/125: ar_sliced = ar_slice(ar)
192/126: ar_sliced
192/127: ar
192/128: conv1_out_slice1 = conv1(ar_sliced[0])
192/129: conv2_out_slice2 = conv2(conv1_out_slice1)
192/130: conv2_out_slic1 = conv2(conv1_out_slice1)
192/131: conv3_out_slic1 = conv3(conv2_out_slice1)
192/132: conv2_out_slice1 = conv2(conv1_out_slice1)
192/133: conv3_out_slice1 = conv3(conv2_out_slice1)
192/134: conv3_out
192/135: conv3_out_slice1
192/136: conv1_out_slice2 = conv1(ar_sliced[1])
192/137: conv2_out_slice2 = conv2(conv1_out_slice2)
192/138: conv3_out_slice2 = conv3(conv2_out_slice2)
192/139: ar
192/140: conv3_out_slice2
192/141: conv3_out
192/142: conv1_out_slice3 = conv1(ar_sliced[2])
192/143: conv2_out_slice3 = conv2(conv1_out_slice3)
192/144: conv3_out_slice3 = conv3(conv2_out_slice3)
192/145: conv3_out_slice3
192/146: conv3_out
192/147: conv1_out_slice4 = conv1(ar_sliced[3])
192/148: conv2_out_slice4 = conv2(conv1_out_slice4)
192/149: conv3_out_slice4 = conv3(conv2_out_slice4)
192/150: conv3_out_slice4
193/1: cd utils/
193/2: ls
193/3: %ls
193/4: import spec_augment
193/5: import spec_augment
193/6: import spec_augment
194/1: from speech.utils import spec_augment
194/2: from speech.utils import spec_augment
194/3: from speech.loader import log_specgram_from_file
194/4: audio_path = './onnx_coreml/audio_files/out.wav'
194/5: audio_path = './onnx_coreml/audio_files/ST-out.wav'
194/6: log_spec = log_specgram_from_file(audio_path)
194/7: log_spec.shape
194/8: aug_log_spec = spec_augment.spec_augment(log_spec)
194/9: import numpy as np
194/10: npa = np.array([1,2,3])
194/11: npa.unsqueeze(0)
194/12: aug_log_spec = spec_augment.spec_augment(log_spec)
194/13: from speech.utils import spec_augment
194/14: aug_log_spec = spec_augment.spec_augment(log_spec)
195/1: %history
195/2: %hist
195/3: %hist
195/4: from speech.utils import spec_augment
195/5: audio_path = './onnx_coreml/audio_files/ST-out.wav'
195/6: from speech.loader import log_specgram_from_file
195/7: log_spec = log_specgram_from_file(audio_path)
195/8: aug_log_spec = spec_augment.spec_augment(log_spec)
195/9: import numpy as np
195/10: npa=np.array([[1,2],[3,4]])
195/11: npa.shape
195/12: np.expand_dims(npa, dim=0)
195/13: np.expand_dims(npa, 0)
195/14: npa.shape
195/15: npa = np.expand_dims(npa, 0)
195/16: npa.shape
196/1: %history
196/2: from speech.utils import spec_augment
196/3: from speech.loader import log_specgram_from_file
196/4: log_spec = log_specgram_from_file(audio_path)
196/5: aug_log_spec = spec_augment.spec_augment(log_spec)
196/6: audio_path = './onnx_coreml/audio_files/ST-out.wav'
196/7: log_spec = log_specgram_from_file(audio_path)
196/8: aug_log_spec = spec_augment.spec_augment(log_spec)
196/9: aug_log_spec = spec_augment.spec_augment(log_spec)
196/10: aug_log_spec = spec_augment.spec_augment(log_spec)
197/1: from speech.utils import spec_augment
197/2: from speech.loader import log_specgram_from_file
197/3: audio_path = './onnx_coreml/audio_files/ST-out.wav'
197/4: log_spec = log_specgram_from_file(audio_path)
197/5: aug_log_spec = spec_augment.spec_augment(log_spec)
198/1: from speech.utils import spec_augment
198/2: from speech.loader import log_specgram_from_file
198/3: audio_path = './onnx_coreml/audio_files/ST-out.wav'
198/4: log_spec = log_specgram_from_file(audio_path)
198/5: import torch
198/6: log_spec = torch.from_numpy(log_spec)
198/7: aug_log_spec = spec_augment.spec_augment(log_spec)
198/8: aug_log_spec.shape
198/9: spec_augment.visualization_spectrogram(aug_log_spec, "test")
198/10: spec_augment.visualization_spectrogram(aug_log_spec, "test")
199/1: from speech.utils import spec_augment
199/2: from speech.loader import log_specgram_from_file
199/3: audio_path = './onnx_coreml/audio_files/ST-out.wav'
199/4: log_spec = log_specgram_from_file(audio_path)
199/5: import torch
199/6: log_spec = torch.from_numpy(log_spec)
199/7: aug_log_spec = spec_augment.spec_augment(log_spec)
199/8: spec_augment.visualization_spectrogram(aug_log_spec, "test")
200/1: %history
200/2: history
200/3: from speech.utils import spec_augment
200/4: from speech.loader import log_specgram_from_file
200/5: audio_path = './onnx_coreml/audio_files/ST-out.wav'
200/6: import torch
200/7: log_spec = log_specgram_from_file(audio_path)
200/8: log_spec = torch.from_numpy(log_spec)
200/9: aug_log_spec = spec_augment.spec_augment(log_spec)
200/10: spec_augment.visualization_spectrogram(aug_log_spec, "test")
200/11: aug_log_spec.shape
200/12: aug_log_spec.numpy()
200/13: aug_log_spec.numpy().shape
200/14: aug_log_spec.shape
200/15: spec_augment.visualization_spectrogram(aug_log_spec.numpy(), "test")
201/1: %paste
201/2: spec_augment.visualization_spectrogram(aug_log_spec.numpy(), "test")
201/3: spec_augment.visualization_spectrogram(aug_log_spec.numpy(), "test")
202/1: %paste
202/2:
from speech.utils import spec_augment 
from speech.loader import log_specgram_from_file
import torch      
audio_path = './onnx_coreml/audio_files/ST-out.wav'
log_spec = log_specgram_from_file(audio_path)  
log_spec = torch.from_numpy(log_spec)
aug_log_spec = spec_augment.spec_augment(log_spec)
202/3: spec_augment.visualization_spectrogram(aug_log_spec.numpy(), "test")
202/4: spec_augment.visualization_spectrogram(aug_log_spec.numpy(), "test")
203/1: %paste
204/1: %paste
204/2: spec_augment.visualization_spectrogram(aug_log_spec.numpy(), "test")
205/1: %paste
205/2: spec_augment.visualization_spectrogram(aug_log_spec.numpy(), "test")
206/1: %paste
206/2: %paste
206/3: spec_augment.visualization_spectrogram(aug_log_spec.numpy(), "test")
207/1: %paste
207/2: spec_augment.visualization_spectrogram(aug_log_spec.numpy(), "test")
207/3: import matplotlib
207/4: matplotlib.pyplot.specgram(aug_log_spec.numpy())
209/1: s = set('a', 'b', 'b', 'c')
209/2: s = set(['a', 'b', 'b', 'c'])
209/3: s
209/4: s = {'a', 'b', 'b', 'c'}
209/5: s
210/1: import torch
210/2: x = torch.arange(100)
210/3: x.reshape(1,20, 5)
210/4: x = x.reshape(1,20, 5)
210/5: x.shape
210/6: torch.nn.functional.pad(x, (0,0,5,5))
210/7: x.shape
210/8: x_pad = torch.nn.functional.pad(x, (0,0,5,5))
210/9: x.shape
210/10: x_pad.shape
210/11: conv_cfg = [[32, 11, 41, 2, 2, 5, 20],[32, 11, 21, 1, 2, 5, 10], [96, 11, 21, 1, 1, 5, 10]]
210/12: convs[]
210/13: import torch.nn as nn
210/14: convs = []
210/15: in_c =1
210/16: %paste
210/17: %paste
210/18: conv = nn.Sequential(*convs)
210/19: list(conv.children())[0][0].filter_size()
210/20: list(conv.children())[0].kernel_size[0]
210/21: list(conv.children())[0].kernel_size[0]//2
210/22: x_pad
210/23: x_pad = torch.nn.functional.pad(x.unsqueeze(1), (0,0,5,5))
210/24: x_pad.shape
210/25: x.shape
210/26: np
210/27: import numpy as np
210/28: m = np.array([1,3,34,45])
210/29: m.shape
210/30: type(m)
210/31: assert type(m) =='numpy.ndarray'
210/32: assert type(m) ==numpy.ndarray
210/33: assert type(m) ==np.ndarray
210/34: assert type(m) ==np.array
210/35: assert type(m) ==np.ndarray, "input is not numpy array"
210/36: assert type(1) ==np.ndarray, "input is not numpy array"
210/37: np.random.uniform(low=0, high=3)
210/38: np.random.uniform(low=0, high=3)
210/39: np.random.uniform(low=0, high=3)
210/40: np.random.uniform(low=0, high=3)
210/41: np.random.uniform(low=0, high=3)
210/42: np.random.randint(low=0, high=3)
210/43: np.random.randint(low=0, high=3)
210/44: np.random.randint(low=0, high=3)
210/45: np.random.randint(low=0, high=3)
210/46: np.random.randint(low=0, high=3)
210/47: np.random.randint(low=0, high=3)
210/48: np.random.randint(low=0, high=3)
210/49: np.random.randint(low=0, high=3)
210/50: np.random.randint(low=0, high=3)
210/51: np.random.randint(low=0, high=3)
210/52: np.random.randint(low=0, high=3)
210/53: np.random.randint(low=0, high=4)
210/54: np.random.randint(low=0, high=4)
210/55: np.random.randint(low=0, high=4)
210/56: np.random.randint(low=0, high=4)
210/57: np.random.randint(low=0, high=3)
210/58: np.random.randint(low=0, high=3)
210/59: np.random.randint(low=0, high=3)
210/60: np.random.randint(low=0, high=3)
210/61: np.random.randint(low=0, high=3)
210/62: np.random.randint(low=0, high=3)
210/63: np.random.randint(low=0, high=3)
210/64: np.random.randint(low=0, high=3)
210/65: np.random.randint(low=0, high=3)
210/66: np.random.randint(low=0, high=3)
210/67: np.random.randint(low=0, high=3)
210/68: np.random.randint(low=0, high=3)
210/69: np.random.randint(low=0, high=3)
210/70: np.random.randint(low=0, high=3)
210/71: np.random.randint(low=0, high=3)
210/72: np.random.randint(low=0, high=3)
210/73: np.random.randint(low=0, high=3)
210/74: np.random.randint(low=0, high=3)
210/75: np.random.randint(low=0, high=3)
210/76: np.random.randint(low=0, high=3)
210/77: np.random.randint(low=0, high=4)
210/78: np.random.randint(low=0, high=4)
210/79: np.random.randint(low=0, high=4)
210/80: np.random.randint(low=0, high=4)
210/81: np.random.randint(low=0, high=4)
210/82: np.random.randint(low=0, high=4)
210/83: np.random.randint(low=0, high=4)
210/84: np.random.randint(low=0, high=4)
210/85: np.random.randint(low=0, high=4)
210/86: np.random.randint(low=0, high=4)
210/87: np.random.randint(low=0, high=4)
210/88: np.random.randint(low=0, high=4)
210/89: np.random.randint(low=0, high=4)
210/90: np.random.randint(low=0, high=4)
210/91: np.random.randint(low=0, high=4)
210/92: np.random.randint(low=0, high=4)
210/93: l
210/94: ls
210/95: cd speech
210/96: ls
210/97: import loader
210/98: audio_path = '../onnx_coreml/audio_files/ST-out.wav'
210/99: inputs = loader.log_specgram_from_file(audio_path)
210/100: inputs = loader.apply_spec_augment(inputs)
211/1: import loader
211/2: cd speech
211/3: import loader
211/4: audio_path = '../onnx_coreml/audio_files/ST-out.wav'
211/5: inputs = loader.log_specgram_from_file(audio_path)
211/6: inputs = loader.apply_spec_augment(inputs)
212/1: import randomm
212/2: import random
212/3: list(random.randrange(0,5))
212/4: random.randrange(0,5)
212/5: random.randrange(0,5)
212/6: random.randrange(0,5)
212/7: random.randrange(0,5)
212/8: random.randrange(0,5)
212/9: random.randrange(6,5)
212/10: spec_len=10
212/11: w=5
212/12: random.randrange(5,5)
212/13: assert spec_len*2 > W, "spec_len not big enough"
212/14: assert spec_len*2 > w, "spec_len not big enough"
212/15: assert spec_len*2 > w, "spec_len not big enough"
212/16: 10>10
212/17: spec_len
212/18: assert spec_len > 2*w, "spec_len not big enough"
212/19:
for i in range(5):
    if i >3:
        continue
    print(i)
212/20:
for i in range(6):
    if i >3:
        continue
    print(i)
212/21:
for i in range(6):
    if i >3:
        continue
    print(i)
212/22:
for i in range(10):
    if i >3:
        continue
    print(i)
213/1: cd examples/librispeech/
213/2: import json
213/3:
with open("ctc_config.json") as cfg: 
    config = json.dumps(cfg)
213/4:
with open("ctc_config.json", 'r') as cfg: 
    config = json.load(cfg)
213/5: preproc=config['preproc']
213/6: preproc
213/7: preproc['use_spec_augment']
213/8: type(preproc['use_spec_augment'])
213/9: preproc['use_spec_augment']
213/10:
if preproc['use_spec_augment']:
    print("hello")
213/11: !preproc['use_spec_augment']
213/12: not preproc['use_spec_augment']
213/13: np
213/14: import numpy as np
213/15: np.random.binomial(1, 0.5)
213/16: np.random.binomial(1, 0.5)
213/17: np.random.binomial(1, 0.5)
213/18: np.random.binomial(1, 0.5)
213/19: np.random.binomial(1, 0.5)
213/20: np.random.binomial(1, 0.5)
213/21: np.random.binomial(1, 0.5)
213/22: np.random.binomial(1, 0.5)
213/23: np.random.binomial(1, 0.5)
213/24: np.random.binomial(1, 0.5)
213/25: np.random.binomial(1, 0.5)
213/26: np.random.binomial(1, 0.5)
213/27: np.random.binomial(1, 0.5)
213/28: np.random.binomial(1, 0.5)
213/29: np.random.binomial(1, 0.5)
213/30: np.random.binomial(1, 1)
213/31: np.random.binomial(1, 1)
213/32: np.random.binomial(1, 1)
213/33: np.random.binomial(1, 1)
213/34: np.random.binomial(1, 1)
213/35: np.random.binomial(1, 1)
213/36: np.random.binomial(1, 1)
213/37: np.random.binomial(1, 1)
213/38: np.random.binomial(1, 1)
213/39: np.random.binomial(1, 1)
213/40: np.random.binomial(1, 1)
213/41: np.random.binomial(1, 1)
213/42: np.random.binomial(1, 1)
213/43: np.random.binomial(1, 1)
213/44: np.random.binomial(1, 0)
213/45: np.random.binomial(1, 0)
213/46: np.random.binomial(1, 0)
213/47: np.random.binomial(1, 0)
213/48: np.random.binomial(1, 0)
213/49: np.random.binomial(1, 0)
213/50: np.random.binomial(1, 0)
213/51: np.random.binomial(1, 0)
213/52: np.random.binomial(1, 0.25)
213/53: np.random.binomial(1, 0.25)
213/54: np.random.binomial(1, 0.25)
213/55: np.random.binomial(1, 0.25)
213/56: np.random.binomial(1, 0.25)
213/57: np.random.binomial(1, 0.25)
213/58: np.random.binomial(1, 0.25)
213/59: np.random.binomial(1, 0.25)
213/60: count = 0
213/61:
for i in range (100):
    count += np.random.binomial(1, 0.25)
213/62: count
213/63: count = 0
213/64:
for i in range (100):
    count += np.random.binomial(1, 0.50)
213/65: count
213/66: count = 0
213/67:
for i in range (100):
    count += np.random.binomial(1, 0.75)
213/68: count
213/69: import os
213/70: os.path.join('hello', "sph")
213/71: os.listdir(os.path.join('hello', "sph"))
213/72: ls
213/73: pwd
213/74: dir = '/Users/dustin/CS/consulting/firstlayerai/phoneme_classification/src/awni_speech/speech/examples/librispeech'
213/75: os.listdir(os.path.join(dir, "models"))
213/76: ls models/
214/1: l = ['CAPS', 'ADASDF', 'ASDFEW']
214/2: m = map(lambda x: x.lower(), l)
214/3: m
214/4: m = list(map(lambda x: x.lower(), l))
214/5: m
214/6: import string
214/7: m[0] +='5'
214/8: m[1] +='6'
214/9: m[2] +='4'
214/10: m
214/11: m2 = list(map(lambda x: x.rstrip(string.digits), m))
214/12: m2
214/13: mn
214/14: m
214/15: m = list(map(lambda x: x.rstrip(string.digits), m))
214/16: m
214/17: m
214/18: l = ['AA1' 'L' 'B' 'AO2' 'M']
214/19: ls
214/20: cd examples/librispeech/
214/21:
with open("librispeech-lexicon.txt", 'r') as fid:
    lexicon = (l.strip().lower().split() for l in fid)
214/22: lexicon
214/23: l
214/24: l = ['AA1', 'L', 'B', 'AO2', 'M']
214/25: l = list(map(lambda x: x.rstrip(string.digits), l))
214/26: l
214/27: lex_dict = defaultdict(lambda: "unk")
214/28: from collections import defaultdict
214/29: lex_dict = defaultdict(lambda: "unk")
214/30:
        for line in lexicon: 
            word = line[0]
            phones = line[1:]
            # remove the accent digit from the phone, string.digits = '0123456789'
            phones = list(map(lambda x: x.rstrip(string.digits), phones))
            lex_dict[word] = phones
214/31: lexicon
214/32: %paste
214/33: lex_dict
214/34: %paste
214/35: lex_dict
214/36: lex_dict.get('acer')
214/37: lex_dict['acer']
214/38: lex_dict['acer'] = ['test']
214/39: lex_dict['acer']
214/40: lex_dict.get('acer')
214/41: len(lex_dict)
214/42: lex_dict['1234']
214/43: ind = ['1234']
214/44:
if lex_dict[ind] == 'unk':
    lex_dict[ind] = 'test'
214/45: ind = '1234'
214/46:
if lex_dict[ind] == 'unk':
    lex_dict[ind] = 'test'
214/47: lex_dict[ind]
214/48: ind = 'achim'
214/49: lex_dict[ind]
214/50:
if lex_dict[ind] == 'unk':
    lex_dict[ind] = 'test'
214/51: lex_dict[ind]
214/52: trans = "hello how are you"
215/1: d ={}
215/2: d.update({1:3})
215/3: d
215/4: d.update((4, 5))
215/5: f = '/Users/dustin/CS/consulting/firstlayerai/data/LibriSpeech/test-clean/61/70968/61-70968.trans.txt'
215/6:
with open(f) as fid:
    lines = (l.strip().lower().split() for l in fid)
    lines = ((l[0], " ".join(l[1:])) for l in lines)
    print(lines)
215/7: data = {}
215/8:
with open(f) as fid:
    lines = (l.strip().lower().split() for l in fid)
    lines = ((l[0], " ".join(l[1:])) for l in lines)
    print(lines)
215/9: data
215/10:
with open(f) as fid:
    lines = (l.strip().lower().split() for l in fid)
    lines = ((l[0], " ".join(l[1:])) for l in lines)
    data.update(lines)
215/11: data
215/12: l = [['a', 1], ['b', 2], ['c',3]]
215/13: d
215/14: d = {}
215/15: m = ((p[0], p[1]) for p in l)
215/16: type(m)
215/17: d
215/18: d.update(m)
215/19: d
215/20: m
215/21: list(m)
215/22: m
215/23: l
215/24: m = ((p[0], p[1]) for p in l)
215/25: tuple(m)
215/26: m
215/27: tuple(m)
215/28: m = ((p[0], p[1]) for p in l)
215/29: t = tuple(m)
215/30: t
215/31: d
215/32: d={}
215/33: d.update(t)
215/34: d
215/35: d.update(('a', 1))
215/36: d.update((('a', 1)))
215/37: t1 = (('d', 4))
215/38: d.update(t1)
215/39: len(t)
215/40: len(t1)
215/41: t1
215/42: t1 = (t1)
215/43: t1
215/44: t
215/45: lst = list(t)
215/46: lst
215/47: d
215/48: d={}
215/49: d.update(lst)
215/50: d
215/51: t1 = list(t1)
215/52: t1
215/53: d.update(t1)
215/54: t1
215/55: t1 = [('d',4)]
215/56: t1
215/57: d.update(t1)
215/58: d
215/59: ls
215/60: cd examples/librispeech/
215/61: import preprocess
215/62: lex_dict = preprocess.lexicon_to_dict()
215/63: len(lex_dict)
215/64: data
215/65: transcript = 'he began a confused complaint against the wizard who had vanished behind the curtain on the left'
215/66: transcript = transcript.slit()
215/67: transcript = transcript.split()
215/68: transcript
215/69: phonemes = [lex_dict[word] for word in transcript]
215/70: phonemes
215/71: phonemes = [*lex_dict[word] for word in transcript]
215/72: phonemes = [phones for word in transcript for phones in lex_dict[word]]
215/73: phonemes
215/74: f
215/75:
with open(f) as fid:
    lines = (l.strip().lower().split() for l in fid)
    lines = (
                    (l[0], phones for word in l[1:] for phones in lex_dict[word]) 
                        for l in lines)
215/76: data = {}
215/77:
with open(f) as fid:
    lines = (l.strip().lower().split() for l in fid)
    lines = ((l[0], phones for word in l[1:] for phones in lex_dict[word]) for l in lines)
215/78:
with open(f) as fid:
    lines = (l.strip().lower().split() for l in fid)
    lines = ((l[0], phones for word in l[1:] for phones in lex_dict[word]) for l in lines)
215/79:
with open(f) as fid:
    lines = (l.strip().lower().split() for l in fid)
    lines = ((l[0], phones for word in l[1:] for phones in lex_dict[word]) for l in lines)
215/80:
with open(f) as fid:
    lines = (l.strip().lower().split() for l in fid)
    lines = ((l[0], phones for word in l[1:] for phones in lex_dict[word]) for l in lines)
215/81:
with open(f) as fid:
    lines = (l.strip().lower().split() for l in fid)
    lines = ((l[0], [phones for word in l[1:] for phones in lex_dict[word]]) for l in lines)
    data.update(lines)
215/82: data
216/1: pset = {}
216/2: pset.add(4)
216/3: pset = set()
216/4: pset.add(4)
216/5: pset
216/6: 4 not in pset
216/7: 4 in pset
216/8: ls
216/9: cd examples/librispeech/
216/10: import preprocess
216/11: import preprocess
216/12: import preprocess
216/13: lex_dict = preprocess.lexicon_to_dict()
216/14: preprocess.export_phones(lex_dict)
216/15: pset
216/16: pset.add(5)
216/17: pset.add(6)
216/18: pset
216/19: print(i for i in pset)
216/20: import preprocess
216/21: lex_dict = preprocess.lexicon_to_dict()
216/22: preprocess.export_phones(lex_dict)
218/1: import preprocess
219/1: import preprocess
219/2: lex_dict = preprocess.lexicon_to_dict()
219/3: preprocess.export_phones(lex_dict)
219/4: preprocess.export_phones(lex_dict)
220/1: import preprocess
220/2: lex_dict = preprocess.lexicon_to_dict()
220/3: preprocess.export_phones(lex_dict)
220/4:
with open("librispeech_lexicon_phone_set", 'r') as fid:
    phones = fid.read()
220/5: phones
220/6: split = phones.split()
220/7: split
221/1: import preprocess
221/2: lex_dict = preprocess.lexicon_to_dict()
221/3: preprocess.export_phones(lex_dict)
221/4:
with open("librispeech_lexicon_phone_set", 'r') as fid:
    phones = fid.read()
221/5: split = phones.split()
221/6: split
221/7: len(split)
221/8: lines = [["hello", "how", "are", "you"], ["test", "tart", "asdf"], ["gibberish", "adsfe", "123rn"]]
221/9: filter(lambda x: word_phoneme_dict[x] == "unk", word for line in lines for word in line[1:])
221/10: filtered = filter(lambda x: word_phoneme_dict[x] == "unk", word for line in lines for word in line[1:])
221/11: filtered = filter(lambda x: lex_dict[x] == "unk", word for line in lines for word in line[1:])
221/12: filtered = filter(lambda x: lex_dict[x] == "unk", [word for line in lines for word in line[1:]])
221/13: filtered
221/14: list(filtered)
221/15: filtered = filter(lambda x: lex_dict[x] == "unk", (word for line in lines for word in line[1:]))
221/16: list(filtered)
221/17: pset=set()
221/18: [1, 2, 3, 4, 1]
221/19: l = [1, 2, 3, 4, 1]
221/20: pset.add(l)
221/21: pset.add(*l)
221/22: pset.update(l)
221/23: pset
221/24: filtered = filter(lambda x: lex_dict[x] == "unk", (word for line in lines for word in line[1:]))
221/25: m = set(filtered)
221/26: m
221/27: lines = [["hello", "how", "are", "you"], ["test", "tart", "asdf"], ["gibberish", "adsfe", "123rn", "asdf"]]
221/28: filtered = filter(lambda x: lex_dict[x] == "unk", (word for line in lines for word in line[1:]))
221/29: m = set(filtered)
221/30: m
221/31: filtered = filter(lambda x: lex_dict[x] == "unk", (word for line in lines for word in line[1:]))
221/32: m = list(filtered)
221/33: m
221/34: pset
221/35: m = set(filtered)
221/36: m
221/37: filtered = filter(lambda x: lex_dict[x] == "unk", (word for line in lines for word in line[1:]))
221/38: m = set(filtered)
221/39: pset
221/40: pset.update(m)
221/41: pset
222/1: import preprocess
222/2: lex_dict = preprocess.lexicon_to_dict()
222/3: preprocess.export_phones(lex_dict)
223/1: import preprocess
223/2: lex_dict = preprocess.lexicon_to_dict()
223/3: preprocess.export_phones(lex_dict)
223/4:
with open("librispeech_lexicon_phone_set", 'r') as fid:
    phones = fid.read()
223/5: phones
223/6: split = phones.split()
223/7: split
223/8: len(split)
223/9: import os
223/10: path = 'home/dzubke/speech
223/11: path = 'home/dzubke/speech'
223/12: dr = "train-other-500"
223/13: os.path.json(path,dr)
223/14: os.path.join(path,dr)
223/15: prefix = os.path.join(path,dr)
223/16: prefix
223/17: os.path.dirname(prefix)
223/18: os.path.basename(prefix)
223/19: os.path.basename(prefix) + os.path.extsep + "json"
221/42:
def fun(start, end):
    return end - start > 3
221/43: fun(5, 10)
221/44: fun(5, 8)
221/45: fun(5, 9)
221/46: fun(5, 8.01)
221/47: fun(5, 8)
223/20: ls
223/21: cd ..
223/22: cd tedlium/
223/23: path = "TEDLIUM.152k.dic"
223/24: lex_dict = defaultdict(lambda: "unk")
223/25: from collections import defaultdict
223/26: lex_dict = defaultdict(lambda: "unk")
223/27:
with open(path, 'r') as fid:
    lexicon = (l.strip().lower().split() for l in fid)
223/28: lexicon
223/29: lex = list(lexicon = (l.strip().lower().split() for l in fid))
223/30:
with open(path, 'r') as fid:
    lexicon = [l.strip().lower().split() for l in fid]
223/31: lexicon
223/32: lexicon
223/33:
with open(path, 'r') as fid:
    lexicon = (l.strip().lower().split() for l in fid)
    for line in lexicon:
        word = line[0]
        phones = line[1:]
        lex_dict[word] = phones
223/34: len(lex_dict)
223/35: lex_dict.get('acetate')
221/48: dct = {'abe': ['a', 'b', 'e'], 'call': ['c', 'a', 'l', 'l'], 'a (2)': ['a']}
221/49: dct
221/50: import strings
221/51: import string
221/52: print(string.digits)
221/53: [k for k in dct.keys() if string.digits in k]
221/54: re
221/55: import re
221/56: dctt
221/57: dctt
221/58: dct
221/59: dct.keys()
221/60: re.search($\(\d\), dct.keys() )
221/61: re.search("$\(\d\)", dct.keys() )
221/62: re.search("$\(\d\)", i for i in dct.keys())
221/63: [re.search("$\(\d\)",k) for k in dct.keys()]
221/64: [re.search("\(\d\)$",k) for k in dct.keys()]
221/65: m = [1,1,1]
221/66: m*m
221/67: m*.m
221/68: m+m
221/69: dct
221/70: del dct['abe']
221/71: dct
221/72: dct = {'abe': ['a', 'b', 'e'], 'call': ['c', 'a', 'l', 'l'], 'a (2)': ['a']}
221/73: ndct = dict(filter(lambda elem: re.search("\(\d\)$", elem), dct))
221/74: ndct = dict(filter(lambda elem: re.search("\(\d\)$", elem), dct.keys()))
221/75: ndct = dict(filter(lambda elem: re.search("\(\d\)$", elem), dct.items()))
221/76: ndct = dict(filter(lambda elem: re.search("\(\d\)$", elem), dct.keys()))
221/77: ndct = {key: value for key, value in dct.items() if re.search("\(\d\)$", key)}
221/78: ndct
221/79: ndct = {key: value for key, value in dct.items() if not re.search("\(\d\)$", key)}
221/80: ndct
221/81: ls
221/82: cd ..
221/83: cd tedlium
221/84: import preprocess
221/85: ls
221/86: ted_lex_dict = preprocess.lexicon_to_dict("./")
224/1: cd ../tedlium
224/2: import preprocess as ted_preprocess
224/3: ted_lex_dict = ted_preprocess.lexicon_to_dict("./")
224/4: ted_lex_dict = ted_preprocess.lexicon_to_dict(ted_dir="./")
225/1: cd tedlium
225/2: import preprocess
226/1: cd tedlium
226/2: import preprocess
227/1: import wget
227/2: import sys
227/3: print(sys.path)
228/1: import wget
228/2: cd tedlium
228/3: import preprocess
228/4: import preprocess
228/5: lex_dict = preprocess.lexicon_to_dict("./")
228/6: lex_dict = preprocess.lexicon_to_dict("./")
228/7: import preprocess
228/8: lex_dict = preprocess.lexicon_to_dict("./")
228/9: import preprocess
228/10: lex_dict = preprocess.lexicon_to_dict("./")
229/1: import preprocess
229/2: lex_dict = preprocess.lexicon_to_dict("./")
230/1: import preprocess
230/2: lex_dict = preprocess.lexicon_to_dict("./")
230/3: lex_dict
230/4: len(lex_dict)
230/5: lex_dict["chinook"]
230/6: lex_dict["chinook(2)"]
230/7: import re
230/8: [key for key in lex_dict.keys() if re.search("\(\d\)$", key)]
230/9: lex_dict
230/10: print(dir())
230/11: lines = [['hello', 'how', 'are'], ['asdf', 'test', 'qwerty']]
230/12: [word for line in lines for word in line]
230/13: [word for line in lines for word in line if lex_dict[word]=='unk']
230/14: from collections import defaultdict
230/15: lex_dict = preprocess.lexicon_to_dict("./")
230/16: type(lex_dict)
230/17: lex_dict['asdf']
230/18: ddct=defaultdict()
230/19: type(ddct)
231/1: import preprocess
231/2: lex_dict = preprocess.lexicon_to_dict("./")
231/3: type(lex_dict)
231/4: dct = dict()
231/5: type(dct)
231/6: from collections import defaultdict
231/7: dct = defaultdict(lambda: "unk", dct)
231/8: type(dct)
231/9: dct['adf']
232/1: import preprocess
232/2: lex_dict = preprocess.lexicon_to_dict("./")
232/3: lines = [['hello', 'how', 'are'], ['asdf', 'test', 'qwerty']]
232/4: [word for line in lines for word in line if lex_dict[word]=='unk']
232/5: filter(lambda x:lex_dict[x] == "unk", word for line in lines for word in line)
232/6: (filter(lambda x:lex_dict[x] == "unk", word for line in lines for word in line))
232/7: filter(lambda x:lex_dict[x] == "unk", word for line in lines for word in line)
232/8: [word for line in lines for word in line lambda x: lex_dict[x] == "unk"]
232/9: filter(lambda x:lex_dict[x] == "unk", [word for line in lines for word in line])
232/10: list(filter(lambda x:lex_dict[x] == "unk", [word for line in lines for word in line]))
232/11: list(filter(lambda x:lex_dict[x] == "unk",  line for line in lines))
232/12: ddct
232/13: dct
232/14: dct = {1:2, 3:4, 5:6}
232/15:
for i in dct:
    print(i)
232/16: filter(lambda x:lex_dict[x] == "unk", [word for line in lines for word in line])
232/17: fil = filter(lambda x:lex_dict[x] == "unk", [word for line in lines for word in line])
232/18: list(fil)
232/19: fil = filter(lambda x:lex_dict[x] == "unk", [word for line in lines for word in line])
232/20: len(fil)
232/21: tset=set()
232/22: tlist=[1,2,3,3,3]
232/23: tset.update(tlist)
232/24: tset
232/25: sum(tlist)
233/1: count1, count2 = 5, 10
233/2: count = (count1, count2)
233/3: count += count
233/4: count
233/5: count += count
233/6: count1, count2 += count
234/1: import speech.utils.data_helpers
234/2: lex_dict = lexicon_to_dict("TEDLIUM.152k.dic", "tedlium")
234/3: lex_dict = data_helpers.lexicon_to_dict("TEDLIUM.152k.dic", "tedlium")
234/4: import speech.utils.data_helpers as dh
234/5: lex_dict = dh.lexicon_to_dict("TEDLIUM.152k.dic", "tedlium")
234/6: lex_dict["hello"]
234/7: lex_dict["asdf"]
235/1: import speech.utils.data_helpers as dh
235/2: lex_dict = dh.lexicon_to_dict("TEDLIUM.152k.dic", "tedlium")
235/3: lex_dict["hello"]
235/4: list("hello")
235/5: list(str("hello"))
235/6: s = "hello"
235/7: list(s)
235/8: s = 'hello'
235/9: list(s)
235/10: lex_dict
236/1: import speech.utils.data_helpers as dh
236/2: lex_dict = dh.lexicon_to_dict("TEDLIUM.152k.dic", "tedlium")
236/3: s = 'hello'
236/4: lex_dict[s]
237/1: x = ['1', '2', '15', '-7', '300']
237/2: sorted(x)
239/1: import preprocess
239/2: path = 'examples/tests/librispeech_test.txt'
239/3: path = 'examples/tests/'
239/4: output = preprocess.load_transcripts(path, use_phonemes=True)
239/5: output
239/6: pattern = os.path.join(path, "*/*/*.trans.txt")
239/7: import os
239/8: pattern = os.path.join(path, "*/*/*.trans.txt")
239/9: pattern
239/10: pattern
239/11: import glob
239/12: files = glob.glob(pattern)
239/13: files
239/14: path = 'examples/tests/'
239/15: files = glob.glob(pattern)
239/16: files
239/17: path = '../tests/'
239/18: pattern = os.path.join(path, "*/*/*.trans.txt")
239/19: files = glob.glob(pattern)
239/20: files
239/21: path = 'examples/tests/'
239/22: pattern = os.path.join(path, "*/*/*.trans.txt")
239/23: files = glob.glob(pattern)
239/24: files
239/25: path = '../tests/'
239/26: pattern = os.path.join(path, "*/*/*.trans.txt")
239/27: files = glob.glob(pattern)
239/28: files
239/29: output = preprocess.load_transcripts(path, use_phonemes=True)
239/30: import preprocess
239/31: output = preprocess.load_transcripts(path, use_phonemes=True)
239/32: import importlib
239/33: importlib.reload(preprocess)
239/34: output = preprocess.load_transcripts(path, use_phonemes=True)
239/35: importlib.reload(preprocess)
239/36: output = preprocess.load_transcripts(path, use_phonemes=True)
239/37: outputt
239/38: output
239/39: importlib.reload(preprocess)
239/40: output = preprocess.load_transcripts(path, use_phonemes=True)
239/41: output
239/42: importlib.reload(speech.utils.data_helpers)
239/43: %autoreload
239/44: %load_ext
239/45: %load_ext autoreload
239/46: %autoreload 2
239/47: output = preprocess.load_transcripts(path, use_phonemes=True)
239/48: output
239/49: %autoreload 1
239/50: output = preprocess.load_transcripts(path, use_phonemes=True)
239/51: output
239/52: importlib.reload(preprocess)
239/53: output = preprocess.load_transcripts(path, use_phonemes=True)
239/54: output
240/1: import preprocess
240/2: %hist
240/3: path = '../tests/'
240/4: output = preprocess.load_transcripts(path, use_phonemes=True)
240/5: output
240/6: json_path = "librispeech_test.json"
240/7:
with open(json_path, 'w') as fid:
    json.dump(output, fid)
240/8: import json
240/9:
with open(json_path, 'w') as fid:
    json.dump(output, fid)
240/10: output[0]
240/11:
with open(json_path, 'w') as fid:
    datum = {"text": output[0]['61-70968-0005']}
    json.dump(datum, fid)
240/12: unk_words_list, unk_words_dict = list(), dict()
240/13: unk_words_list
240/14: from speech.utils import data_helpers
240/15:         LEXICON_PATH = "librispeech-lexicon.txt"
240/16:         word_phoneme_dict = data_helpers.lexicon_to_dict(LEXICON_PATH, corpus_name="librispeech")
240/17:
with open(files) as fid:
    lines = [l.strip().lower().split() for l in fid]
    lines = ((l[0], [word_phoneme_dict[word] for word in l[1:]]) for l in lines)
    print(list(lines))
240/18: pattern = os.path.join(path, "*/*/*.trans.txt")
240/19: import os
240/20: pattern = os.path.join(path, "*/*/*.trans.txt")
240/21: import glob
240/22: files = glob.glob(pattern)
240/23:
with open(files) as fid:
    lines = [l.strip().lower().split() for l in fid]
    lines = ((l[0], [word_phoneme_dict[word] for word in l[1:]]) for l in lines)
    print(list(lines))
240/24: files
240/25:
with open(files) as fid:
    lines = [l.strip().lower().split() for l in fid]
240/26:
with open(files) as fid:
    lines = [l for l in fid]
240/27:
with open(files) as fid:
    lines = (l for l in fid)
240/28: files
240/29:
with open(files[0]) as fid:
    lines = [l.strip().lower().split() for l in fid]
    lines = ((l[0], [word_phoneme_dict[word] for word in l[1:]]) for l in lines)
    print(list(lines))
240/30: from collections import defaultdict
240/31: d = defaultdict(lambda: list())
240/32: d[0]
240/33:
d={}
with open(files[0]) as fid:
    lines = [l.strip().lower().split() for l in fid]
    lines = ((l[0], [word_phoneme_dict[word] for word in l[1:]]) for l in lines)
    d.update(lines)
    text=d.get('61-70968-0000')
    datum = {'text': text}
    print(datum)
240/34:
d={}
with open(files[0]) as fid:
    lines = [l.strip().lower().split() for l in fid]
    lines = ((l[0], preprocesss.transcript_to_phonemes(l[1:], word_phoneme_dict)) for l in lines)
    d.update(lines)
    text=d.get('61-70968-0000')
    datum = {'text': text}
    print(datum)
240/35:
d={}
with open(files[0]) as fid:
    lines = [l.strip().lower().split() for l in fid]
    lines = ((l[0], preprocess.transcript_to_phonemes(l[1:], word_phoneme_dict)) for l in lines)
    d.update(lines)
    text=d.get('61-70968-0000')
    datum = {'text': text}
    print(datum)
240/36:
d={}
with open(files[0]) as fid:
    lines = [l.strip().lower().split() for l in fid]
    lines = ((l[0], preprocess.transcript_to_phonemes(l[1:], word_phoneme_dict)) for l in lines)
    d.update(lines)
    text=d.get('61-70968-0005')
    datum = {'text': text}
    print(datum)
241/1: import preprocess
241/2: import preprocess
242/1: import preprocess
243/1: import preprocess
243/2: path = "../tests/"
243/3: output = preprocess.load_transcripts(path, use_phonemes=True)
243/4: output
243/5: l
243/6: l=[1,2,3]
243/7: l.extend(4)
243/8: l.extend([4])
243/9: l
243/10: l.extend([])
243/11: l
243/12: l.extend([])
243/13: l
243/14: l.extend([4])
243/15: l
243/16: l = ["tumble's", "khosala", "macklewain's", "zingiber", "quinci", "birdikins", "razetta", "balvastro", "brau", "dowle", "boolooroo", "scheiler", "telemetering", "gingle", "untrussing", "novatians", "gardar", "abalone's", "sneffels", "wahiti", "tarrinzeau", "breadhouse", "bergez", "troke", "officinale", "recuperations", "pinkies", "gwynplaine's", "quilter's", "burgoynes", "fibi", "lacquey's", "myrdals", "ossipon", "doma", "noirtier", "libano", "finnacta", "collander", "bozzle's", "homoiousios", "rangitata", "dhourra", "yulka", "vinos", "saknussemm", "d'avrigny", "docetes", "delectasti", "mulrady", "congal's", "irolg", "verloc", "heuchera", "impara", "daguerreotypist", "homoousios", "westmere", "chaba", "skint", "ambrosch", "beenie", "bambeday", "derivatively", "bennydeck", "magazzino", "weiser", "nicless", "sandyseal", "drouet's", "bennydeck's", "testbridge", "congal", "tishimingo", "macklewain", "vendhya", "mainhall", "sprucewood", "brandd", "corncakes", "sudvestr", "herbivore", "presty", "shoplets", "shampooer", "yundt", "yokul", "moling", "ganny", "hennerberg", "satisfier", "bush'", "stupirti", "bhunda", "synesius's", "buzzer's", "ruggedo's", "hurstwood", "darfhulva", "hardwigg", "delaunay's", "theosophies", "brion's", "canyou", "fjordungr", "glenarvan's", "olbinett", "shimerda", "trampe", "shimerdas", "parrishes", "bozzle", "rathskellers", "verloc's", "frierson's", "ossipon's", "riverlike"]
243/17: l
243/18: import importlib
243/19: importlib.reload(preprocess)
243/20: preprocess.unique_unknown_words()
243/21: r =5
243/22: importlib.reload(preprocess)
243/23: preprocess.unique_unknown_words()
243/24: importlib.reload(preprocess)
243/25: preprocess.unique_unknown_words()
243/26: importlib.reload(preprocess)
243/27: preprocess.unique_unknown_words()
243/28: importlib.reload(preprocess)
243/29: preprocess.unique_unknown_words()
243/30: importlib.reload(preprocess)
243/31: preprocess.unique_unknown_words()
243/32: importlib.reload(preprocess)
243/33: preprocess.unique_unknown_words()
243/34: importlib.reload(preprocess)
243/35: preprocess.unique_unknown_words()
243/36: importlib.reload(preprocess)
243/37: preprocess.unique_unknown_words()
243/38: importlib.reload(preprocess)
243/39: preprocess.unique_unknown_words()
243/40: importlib.reload(preprocess)
243/41: preprocess.unique_unknown_words()
243/42: importlib.reload(preprocess)
243/43: preprocess.unique_unknown_words()
243/44: importlib.reload(preprocess)
243/45: preprocess.unique_unknown_words()
243/46: importlib.reload(preprocess)
243/47: preprocess.unique_unknown_words()
243/48: importlib.reload(preprocess)
243/49: preprocess.unique_unknown_words()
243/50: importlib.reload(preprocess)
243/51: preprocess.unique_unknown_words()
243/52: importlib.reload(preprocess)
243/53: preprocess.unique_unknown_words()
243/54: importlib.reload(preprocess)
243/55: preprocess.unique_unknown_words()
243/56: importlib.reload(preprocess)
243/57: importlib.reload(preprocess)
243/58: preprocess.unique_unknown_words()
243/59: importlib.reload(preprocess)
243/60: preprocess.unique_unknown_words()
243/61: importlib.reload(preprocess)
243/62: preprocess.unique_unknown_words()
243/63: importlib.reload(preprocess)
243/64: preprocess.unique_unknown_words()
243/65: importlib.reload(preprocess)
243/66: preprocess.unique_unknown_words()
243/67: importlib.reload(preprocess)
243/68: preprocess.unique_unknown_words()
243/69: d
243/70: from collections import defaultdict
243/71: d = defaultdict(None)
243/72: d[0]
243/73: d = defaultdict(lambda: None)
243/74: d[0]
243/75: d[7]
243/76: d
243/77: dct = {}
243/78: d.get(0, None)
243/79: d
243/80: dct.get(0, None)
243/81: dct
243/82: dct.get("tree", None)
243/83: dct
243/84: preprocess.unique_unknown_words()
243/85: importlib.reload(preprocess)
243/86: preprocess.unique_unknown_words()
243/87: l = list()
243/88: l
243/89: l = list(l)
243/90: l
243/91: importlib.reload(preprocess)
243/92: preprocess.unique_unknown_words()
243/93: importlib.reload(preprocess)
243/94: preprocess.unique_unknown_words()
243/95: l
243/96: l.extend(None)
243/97: d
243/98: t=()
243/99: t
243/100: d.update(t)
243/101: d
243/102: t1=(1,2)
243/103: d.update(t1)
243/104: d.update((t1))
243/105: l
243/106: l=[[1,2,3],[2,3,4],[3,4,5]]
243/107: gen = ((m[0], m[1:]) for m in l)
243/108: type(gen)
243/109: d.update(gen)
243/110: d
243/111:
def filter_2(array):
    if array[0] ==2:
        return ()
    else:
        return (array[0], array[1:])
243/112: gen = (filter_2(m) for m in l)
243/113: d
243/114: d.update(gen)
243/115:
def filter_2(array):
    if array[0] ==2:
        return (,)
    else:
        return (array[0], array[1:])
243/116:
def filter_2(array):
    if array[0] ==2:
        return (None, None)
    else:
        return (array[0], array[1:])
243/117: gen = (filter_2(m) for m in l)
243/118: d.update(gen)
243/119: d
243/120: l = [1, 2, 3, 4, 5]
243/121: l1 = [3,4,5,6,7]
243/122: l - l1
243/123: s1 = {1, 2, 3, 4, 5}
243/124: type(s1)
243/125: s2 = {3,4,5,6,7}
243/126: s1 - s2
243/127: s2 - s1
243/128: s3 = s2 - s1
243/129: type(s3)
243/130: len(s3)
243/131: s2.difference(s1)
243/132: name = "TEDLIUM.152k.dic
243/133: name = "TEDLIUM.152k.dic"
243/134: name.split(sep=".")
243/135: name.split(sep=".")[0].lower()
243/136: lib_name = "librispeech-lexicon_extended.txt"
243/137: "-" in lib_name
243/138: "-" in name
243/139:
set1 = {"metzinger", "rsum", "vclav", "fifo", "60", "90s", "manya", "nosair", "1740s", "diabolic", "dallases", "seattlites", "multisurface", "helie", "edik", "fessing", "tactually", "watermaker", "sikang", "1800s", "deadness", "forebearers", "otoo", "7", "zx81", "vulnerabilites", "fiftys", "barelegged", "lakebed", "g5", "zenounes", 
"tightroping", "trocm", "detainer", "15", "navstar", "famlia", "electrotactile", "530s", "47s", "recalibrations", "reykjavk", "suhavi", "carroa", "superstrcuture", "aksum", "chaetomorpha", "oshea", "unman", "centella", "60s", "entrainment", "khiari", "selenide", "uachtarin", "fotokite", "precommitment", "jouska", "additively"}
243/140: digits ="0123"
243/141: [0-9] in digits
243/142: import re
243/143: re.search('/d', digits)
243/144: re.search('/d', "stre")
243/145: re.search('[0-9', "stre")
243/146: re.search(r'[0-9', "stre")
243/147:
s = "tim email is tim@somehost.com"
match = re.search(r'[\w.-]+@[\w.-]+', s)
243/148: match
243/149:
s = "tim email is tim@somehost.com"
match = re.search(r'[0-9]', digits)
243/150: match
243/151: digits
243/152: re.search(r'[0-9]', digits)
243/153: re.search(r'[0-9]', digits).group()
243/154: re.search(r'[0-9]', digits[1:]).group()
243/155: re.search(r'[0-9]', digits[2:]).group()
243/156: re.search(r'[0-9]', digits[3:]).group()
243/157: digits
243/158: re.search(r'[0-9]', 'stre').group()
243/159: re.search(r'[0-9]', 'stre')
243/160: not re.search(r'[0-9]', 'stre')
243/161: not re.search(r'[0-9]', '90s')
243/162: set1
243/163: [i for i in set1 if not re.search(r'[0-9]', i)]
243/164: list(filter(lambda x: re.search(r'[0-9]', x), set1))
243/165: list(filter(lambda x: not re.search(r'[0-9]', x), set1))
243/166: fil_set = filter(lambda x: len(x)<30, set1)
243/167: list(filter(lambda x: not re.search(r'[0-9]', x), fil_set))
243/168: len(list(filter(lambda x: not re.search(r'[0-9]', x), fil_set)))
243/169: fil_set = filter(lambda x: len(x)<30, set1)
243/170: len(list(filter(lambda x: not re.search(r'[0-9]', x), fil_set)))
243/171: len(list(filter(lambda x: not re.search(r'[0-9]', x), set1)))
243/172: set.add("asdklfnpoion934inrp913i4nrpoiandpoin394nrp913n4ir")
243/173: set1.add("asdklfnpoion934inrp913i4nrpoiandpoin394nrp913n4ir")
243/174: fil_set = filter(lambda x: len(x)<30, set1)
243/175: len(list(filter(lambda x: not re.search(r'[0-9]', x), fil_set)))
243/176: len(list(filter(lambda x: not re.search(r'[0-9]', x), set1)))
243/177: set1[-1]
243/178: l=[1,2,3,4,5,6]
243/179: l[-1]
243/180: l[-2]
243/181: l[-3]
243/182: l[-5]
243/183: l[-6]
243/184: len(set1)
243/185: set1.add("asdfadsfinpoinonasd;lfknpasiodnfplknd;lknadf")
243/186: len(list(filter(lambda x: not re.search(r'[0-9]', x), set1)))
243/187: fil_set = filter(lambda x: len(x)<30, set1)
243/188: len(list(filter(lambda x: not re.search(r'[0-9]', x), fil_set)))
243/189: set(filter(lambda x: not re.search(r'[0-9]', x), set1))
243/190: len(list(filter(lambda x: not re.search(r'[0-9]', x), set1)))
243/191: len(list(filter(lambda x: not re.search(r'[0-9]', x), set1)))
243/192: len(list(filter(lambda x: not re.search(r'[0-9]+', x), set1)))
243/193: len(list(filter(lambda x: not re.search(r'[0-9]*', x), set1)))
243/194: len(list(filter(lambda x: not re.search(r'[0-9]+', x), set1)))
243/195: len(set1)
243/196: len(list(filter(lambda x: not re.search(r'[0-9]+?+', x), set1)))
243/197: len(list(filter(lambda x: not re.search(r'[0-9]+?', x), set1)))
243/198: len(list(filter(lambda x: not re.search(r'[0-9]+', x), set1)))
243/199: set1
243/200: set1.add("adsf?")
243/201: len(list(filter(lambda x: not re.search(r'[0-9]+', x), set1)))
243/202: len(list(filter(lambda x: not re.search(r'[0-9]+?', x), set1)))
243/203: len(list(filter(lambda x: not re.search(r'[0-9]+?+', x), set1)))
243/204: len(list(filter(lambda x: not re.search(r'[0-9?]+', x), set1)))
243/205: len(list(filter(lambda x: not re.search(r'[0-9]+', x), set1)))
243/206: set1.add("*asdf")
243/207: len(list(filter(lambda x: not re.search(r'[0-9]+', x), set1)))
243/208: len(list(filter(lambda x: not re.search(r'[0-9?]+', x), set1)))
243/209: len(list(filter(lambda x: not re.search(r'[0-9?*]+', x), set1)))
243/210: len(list(filter(lambda x: not re.search(r'[0/-9?*]+', x), set1)))
243/211: len(list(filter(lambda x: not re.search(r'[0-9?*]+', x), set1)))
243/212: len(list(filter(lambda x: not re.search(r'[0\-9?*]+', x), set1)))
243/213: list(filter(lambda x: not re.search(r'[0\-9?*]+', x), set1))
243/214: list(filter(lambda x: not re.search(r'[0-9?*]+', x), set1))
243/215: len(list(filter(lambda x: not re.search(r'[0\-9?*]+', x), set1)))
243/216: len(list(filter(lambda x: not re.search(r'[0-9?*]+', x), set1)))
243/217: len(list(filter(lambda x: not re.search(r'[0-9?*&]+', x), set1)))
243/218: set1.add("asdf&")
243/219: len(list(filter(lambda x: not re.search(r'[0-9?*]+', x), set1)))
243/220: len(list(filter(lambda x: not re.search(r'[0-9?*&]+', x), set1)))
243/221: set1.add("asdf&asdf")
243/222: len(list(filter(lambda x: not re.search(r'[0-9?*]+', x), set1)))
243/223: len(list(filter(lambda x: not re.search(r'[0-9?*&]+', x), set1)))
243/224: len(list(filter(lambda x: not re.search(r'[0-9?*&\\]+', x), set1)))
243/225: set1.add("asdf\")
243/226: set1.add("asdf\ ")
243/227: len(list(filter(lambda x: not re.search(r'[0-9?*&]+', x), set1)))
243/228: len(list(filter(lambda x: not re.search(r'[0-9?*&\]+', x), set1)))
243/229: len(list(filter(lambda x: not re.search(r'[0-9?*&\]+', x), set1)))
243/230: len(list(filter(lambda x: not re.search(r'[0-9?*&\\]+', x), set1)))
243/231: len(list(filter(lambda x: not re.search(r'[0-9?*&\\=]+', x), set1)))
243/232: set1.add("asdf=")
243/233: len(list(filter(lambda x: not re.search(r'[0-9?*&\\]+', x), set1)))
243/234: len(list(filter(lambda x: not re.search(r'[0-9?*&\\=]+', x), set1)))
243/235: len(list(filter(lambda x: not re.search(r'[0-9?*&\\=@]+', x), set1)))
243/236: len(list(filter(lambda x: not re.search(r'[0-9?*&\\=@#]+', x), set1)))
243/237: len(list(filter(lambda x: not re.search(r'[0-9?*&\\=@#]', x), set1)))
243/238: len(list(filter(lambda x: not re.search(r'[0-9?*&\\=@#!#$%&()*+,\-./:;<=>?@\[\\\]^_‘{|}~]', x), set1)))
243/239: len(list(filter(lambda x: not re.search(r'[0-9?*&\\=@#!#$%&()*+,\-./:;<=>?@\[\\\]^_{|}~]', x), set1)))
243/240: import data_helpers
243/241: ls
243/242: cd ..
243/243: ls
243/244: import data_helpers
243/245: from speech.utils import data_helpers
243/246: data_helpers.unique_unknown_words("/Users/dustin/CS/consulting/firstlayerai/phoneme_classification/src/awni_speech/speech/examples/tedlium")
243/247: data_helpers.unique_unknown_words("/Users/dustin/CS/consulting/firstlayerai/phoneme_classification/src/awni_speech/speech/examples/tedlium")
243/248: import importlib
243/249: importlib.reload(data_helpers)
243/250: data_helpers.unique_unknown_words("/Users/dustin/CS/consulting/firstlayerai/phoneme_classification/src/awni_speech/speech/examples/tedlium")
243/251: importlib.reload(data_helpers)
243/252: data_helpers.unique_unknown_words("/Users/dustin/CS/consulting/firstlayerai/phoneme_classification/src/awni_speech/speech/examples/tedlium")
243/253: importlib.reload(data_helpers)
243/254: data_helpers.unique_unknown_words("/Users/dustin/CS/consulting/firstlayerai/phoneme_classification/src/awni_speech/speech/examples/tedlium")
243/255: importlib.reload(data_helpers)
243/256: data_helpers.unique_unknown_words("/Users/dustin/CS/consulting/firstlayerai/phoneme_classification/src/awni_speech/speech/examples/tedlium")
243/257: importlib.reload(data_helpers)
243/258: data_helpers.unique_unknown_words("/Users/dustin/CS/consulting/firstlayerai/phoneme_classification/src/awni_speech/speech/examples/tedlium")
243/259: importlib.reload(data_helpers)
243/260: data_helpers.unique_unknown_words("/Users/dustin/CS/consulting/firstlayerai/phoneme_classification/src/awni_speech/speech/examples/tedlium")
243/261: importlib.reload(data_helpers)
243/262: data_helpers.unique_unknown_words("/Users/dustin/CS/consulting/firstlayerai/phoneme_classification/src/awni_speech/speech/examples/tedlium")
243/263: importlib.reload(data_helpers)
243/264: data_helpers.unique_unknown_words("/Users/dustin/CS/consulting/firstlayerai/phoneme_classification/src/awni_speech/speech/examples/tedlium")
243/265: import torch
243/266: ls
243/267: cd ..
243/268: ls
243/269: ctc_path="examples/librispeech/ctc_config.json"
243/270:
with open(ctc_path, 'r') as fid:
    ctc_config =
243/271: import json
243/272:
with open(ctc_path, 'r') as fid:
    ctc_config = json.dump(fid)
243/273:
with open(ctc_path, 'r') as fid:
    ctc_config = json.load(fid)
243/274: ctc_config
243/275: ctc_config["model"]
243/276: import speech.models as models
243/277: import speech.loader as loader
243/278: data_cfg = ctc_config["data"]
244/1: a = 5
244/2: x = 4 is not None
244/3: x
245/1: [range(10)]
245/2: [*range(10)]
246/1: import numpy as np
246/2: n1 = np.ones(256, 257)
246/3: n1 = np.ones((256, 257))
246/4: n1.shape
246/5: n0 = np.zeros((256,257))
246/6: n0.shape
246/7: nc = np.concatenate((n0, n1), axis=0)
246/8: nc.shape
246/9: nc[0:10, 0]
246/10: nc[0:10, 256]
246/11: nc[0:10, 257]
246/12: nc[255:256, 0]
246/13: n0 = np.zeros((256))
246/14: n1 = np.ones((256))
246/15: nc = np.concatenate((n0, n1), axis=0)
246/16: nc[250:260]
246/17: n_array = [n0,n1]
246/18: nc = np.concatenate((*n_array), axis=0)
246/19: nc = np.concatenate(*n_array, axis=0)
246/20: *n_array
246/21: n_array
246/22: nc = np.concatenate(n_array, axis=0)
246/23: nc.shape
246/24: n2 = np.array([2]*256)
246/25: n2.shape
246/26: n2[0:5]
246/27: del n_array[0]
246/28: len(n_array)
246/29: n_array.append(n2)
246/30: n_array
246/31: from queue import Queue
246/32: q = Queue()
246/33: q.get()
246/34: q.put(5)
246/35: q.get()
246/36: q.get()
247/1: import numpy as np
247/2: help(np.random.rand())
247/3: help(np.random.rand)
247/4: np.random.rand()
247/5: np.random.rand()
247/6: np.random.rand()
247/7: import scipy
247/8: sample_rate, data = scipy.io.wavfile.read(/Users/dustin/Downloads/train/audio/_background_noise_/doing_the_dishes.wav)
247/9: sample_rate, data = scipy.io.wavfile.read("/Users/dustin/Downloads/train/audio/_background_noise_/doing_the_dishes.wav")
247/10: sample_rate, data = scipy.wavfile.read("/Users/dustin/Downloads/train/audio/_background_noise_/doing_the_dishes.wav")
247/11: scip.io
247/12: scipy.io
247/13: from scipy.io.wavefile import read
247/14: from scipy.io.wavfile import read
247/15: sample_rate, data = read("/Users/dustin/Downloads/train/audio/_background_noise_/doing_the_dishes.wav")
247/16: sample_rate, data = read("/Users/dustin/Downloads/train/audio/_background_noise_/pink_noise.wav")
247/17: sample_rate, data = read("/Users/dustin/Downloads/train/audio/_background_noise_/pink_noise.wav", mmap=True)
247/18: scipy.io.wavfile.write('/Users/dustin/Downloads/train/audio/_background_noise_/white_noise_2.wav', 16000, np.array(((acoustics.generator.noise(16000*60, color='white'))/3) * 32767).astype(np.int16))
248/1: import scipy
248/2: scipy.io.wavfile.write('/Users/dustin/Downloads/train/audio/_background_noise_/white_noise_2.wav', 16000, np.array(((acoustics.generator.noise(16000*60, color='white'))/3)* 32767).astype(np.int16))
248/3: from scipy.io.wavfile import write
248/4: write('/Users/dustin/Downloads/train/audio/_background_noise_/white_noise_2.wav', 16000, np.array(((acoustics.generator.noise(16000*60, color='white'))/3)* 32767).astype(np.int16))
248/5: import numpy as np
248/6: import acoustics
248/7: write('/Users/dustin/Downloads/train/audio/_background_noise_/white_noise_2.wav', 16000, np.array(((acoustics.generator.noise(16000*60, color='white'))/3)* 32767).astype(np.int16))
248/8: write('/Users/dustin/Downloads/train/audio/_background_noise_/pink_noise_2.wav', 16000, np.array(((acoustics.generator.noise(16000*60, color='pink'))/3)* 32767).astype(np.int16))
248/9: write('/Users/dustin/Downloads/train/audio/_background_noise_/violet_noise_2.wav', 16000, np.array(((acoustics.generator.noise(16000*60, color='violet'))/3)* 32767).astype(np.int16))
248/10: write('/Users/dustin/Downloads/train/audio/_background_noise_/brown_noise_2.wav', 16000, np.array(((acoustics.generator.noise(16000*60, color='brown'))/3)* 32767).astype(np.int16))
248/11: from scipy.io.wavfile import read
248/12: sr, data = read(str(/Users/dustin/Downloads/train/audio/yes/0a7c2a8d_nohash_0.wav))
248/13: sr, data = read(str(~/Downloads/train/audio/yes/0a7c2a8d_nohash_0.wav))
248/14: sr, data = read("~/Downloads/train/audio/yes/0a7c2a8d_nohash_0.wav")
248/15: sr, data = read(r"/Users/dustin/Downloads/train/audio/yes/0a7c2a8d_nohash_0.wav")
248/16: sr, data = read("/Users/dustin/Downloads/train/audio/yes/0a7c2a8d_nohash_0.wav")
248/17: whole, rem = divmod(60, 27)
248/18: whole
248/19: rem
248/20:
for i in range(0):
    print(i)
248/21:
for i in range(1):
    print(i)
248/22: l=[1,2,3,4,5,6,7,8,9]
248/23: l[:6]
252/1: import pandas as pd
252/2: list = [1,2,3,4]
252/3: pd.Dataframe(list)
252/4: array = [1,2,3,4]
252/5: pd.DataFrame(array)
252/6: df = pd.DataFrame(array)
252/7: type(df)
252/8: US_DATA_PATH:str = "/Users/dustin/CS/job_projects/prep/triplebyte/data/USvideos.csv"
252/9: us_df = pd.read_csv(data_path)
252/10: us_df = pd.read_csv(US_DATA_PATH)
252/11: us_df.describe
252/12: us_df.profile_report()
252/13: us_df.profile_report
253/1: US_DATA_PATH:str = "/Users/dustin/CS/job_projects/prep/triplebyte/data/USvideos.csv"
253/2: import pandas as pd
253/3: import pandas_profiling
253/4: us_df = pd.read_csv(US_DATA_PATH)
253/5: us_df.profile_report()
254/1: US_DATA_PATH:str = "/Users/dustin/CS/job_projects/prep/triplebyte/data/USvideos.csv"
254/2:
import pandas as pd
import pandas_profiling
254/3: import pandas as pd
254/4:
import pandas as pd
import pandas_profiling
255/1:
import pandas as pd
import pandas_profiling
256/1:
import pandas as pd
import pandas_profiling
257/1:
import pandas as pd
import pandas_profiling
258/1:
import pandas as pd
import pandas_profiling
258/2: US_DATA_PATH:str = "/Users/dustin/CS/job_projects/prep/triplebyte/data/USvideos.csv"
258/3: us_df = pd.read_csv(US_DATA_PATH)
258/4: us_df.profile_report()
258/5: us_df.describe
258/6:
import pandas as pd
import pandas_profiling
import sklearn
258/7: # make binary variable like greather than 1 million views
258/8: # process columns to be fed into log_reg like one hot representations
258/9: # feed trimmed and processed dataframe into regression
259/1: import numpy as np
259/2: np.array([1,2,3])
259/3: n = np.array([1,2,3])
259/4: n.size
259/5: n.shape
259/6: n = np.zeroes((5,5))
259/7: n = np.zeros((5,5))
259/8: n.size
259/9: n.shape
259/10: ls
259/11: from speech.utils.wave import array_from_wave
259/12: wav_file = "/Users/dustin/CS/consulting/firstlayerai/data/LibriSpeech/dev-clean/174/84280/174-84280-0000.wav"
259/13: play wav_file
259/14: import os
259/15: play_params = "play "+wav_file
259/16: os.system(play_params)
259/17: audio_data, samp_rate = array_from_wave(wav_file)
259/18: noise_prob=0.4
259/19: add_noise = np.random.binomial(1, noise_prob)
259/20: add_noise
259/21: add_noise = np.random.binomial(1, noise_prob)
259/22: add_noise
259/23: add_noise = np.random.binomial(1, noise_prob)
259/24: add_noise
259/25: noise_dir = "/Users/dustin/CS/consulting/firstlayerai/data/background_noise"
259/26: noise_levels = [0,0.5]
259/27: aug_data = inject_noise(audio_data, samp_rate, noise_dir, noise_levels)
259/28: from speech.utils.noise_inector import inject_noise
259/29: from speech.utils.noise_injector import inject_noise
259/30: from speech.utils.noise_injector import inject_noise
259/31: from speech.utils.noise_injector import inject_noise
259/32: aug_data = inject_noise(audio_data, samp_rate, noise_dir, noise_levels)
259/33: from librosa.util import find_files
259/34: files = find_files(noise_dir)
259/35: files[0:10]
259/36: from speech.utils.noise_injector import inject_noise
259/37: aug_data = inject_noise(audio_data, samp_rate, noise_dir, noise_levels)
259/38: import importlib
259/39: importlib.reload(inject_noise)
259/40: from speech.utils import noise_injector
259/41: aug_data = noise_injector.inject_noise(audio_data, samp_rate, noise_dir, noise_levels)
259/42: importlib.reload(noise_injector)
259/43: importlib.reload(noise_injector)
259/44: aug_data = noise_injector.inject_noise(audio_data, samp_rate, noise_dir, noise_levels)
259/45: noise_files = find_files(noise_dir)
259/46: noise_path = np.random.choice(noise_files)
259/47: noise_path
259/48: noise_level = np.random.uniform(*noise_levels)
259/49: noise_level
259/50: from speech.utils.wave import wav_duration
259/51: noise_len = wav_duration(noise_path)
259/52: noise_len
259/53: noise_files
259/54: pattern = os.path.join(noise_dir, "*.wav")
259/55: pattern
259/56: noise_files = glob.glob(pattern)
259/57: import glob
259/58: noise_files = glob.glob(pattern)
259/59: noise_files
259/60: noise_path = np.random.choice(noise_files)
259/61: noise_path
259/62: noise_level = np.random.uniform(*noise_levels)
259/63: noise_len = wav_duration(noise_path)
259/64: noise_len
259/65: data
259/66: print(__dir__())
259/67: __dir__()
259/68: %namespace
259/69: %who
259/70: tyep(audio_data)
259/71: type(audio_data)
259/72: data_len = len(audio_data) / samp_rate
259/73: data_len
259/74: import numpy as np
259/75: noise_start = np.random.rand() * (noise_len - data_len)
259/76: noise_start
259/77: noise_end = noise_start + data_len
259/78: noise_end
259/79: noise_dst = noise_injector.audio_with_sox(noise_path, sample_rate, noise_start, noise_end)
259/80: noise_dst = noise_injector.audio_with_sox(noise_path, samp_rate, noise_start, noise_end)
259/81: noise_dst.shape
259/82: assert len(data) == len(noise_dst)
259/83: assert len(audio_data) == len(noise_dst)
259/84: audio_data.shape
259/85: noise_energy = np.sqrt(noise_dst.dot(noise_dst) / noise_dst.size)
259/86: data_energy = np.sqrt(data.dot(data) / data.size)
259/87: data_energy = np.sqrt(audio_data.dot(audio_data) / audio_data.size)
259/88: audio_data.dtype
259/89: noise_dst.dtype
259/90: data_energy.dtype
259/91: noise_level
259/92: noise_level*noise_dst
259/93: noise_level*noise_dst*data_energy
259/94: noise_level*noise_dst
259/95: data_energy
259/96: noise_energy = np.sqrt(noise_dst.dot(noise_dst) / noise_dst.size)
259/97: noise_energy
259/98: noise_dst
259/99: noise_dst.size
259/100: noise_dst.dot(noise_dst)
259/101: np.sqrt(noise_dst.dot(noise_dst))
259/102: np.sqrt(np.abs(noise_dst.dot(noise_dst)))
259/103: noise_energy = np.sqrt(np.abs(noise_dst.dot(noise_dst)) / noise_dst.size)
259/104: noise_energy
259/105: data_energy = np.sqrt(np.abs(data.dot(data)) / data.size)
259/106: data_energy = np.sqrt(np.abs(audio_data.dot(audio_data)) / audio_data.size)
259/107: data_energy
259/108: noise_level * noise_dst * data_energy / noise_energy
259/109: audio_data += noise_level * noise_dst * data_energy / noise_energy
259/110: noise_dst.dtype
259/111: noise_vals = noise_level * noise_dst * data_energy / noise_energy
259/112: noise_vals.dtype
259/113: noise_vals
259/114: audio_data
259/115: noise_dst
259/116: np.max(audio_data)
259/117: np.max(noise_dst)
259/118: np.mean(noise_dst)
259/119: np.mean(audio_data)
259/120: noise_vals.mean()
259/121: noise_vals.sum()
259/122: audio_data.mean()
259/123: audio_data.sum()
259/124: aug_data = audio_data + noise_vals
259/125: aug_data
259/126: aug_data.mean()
259/127: audio_data.mean()
259/128: aug_data.sum()
259/129: audio_data.sum()
259/130: from scipy.io.wavfile import write
259/131: test_file_noise = "/Users/dustin/CS/consulting/firstlayerai/data"
259/132: write(test_file_noise, aug_data)
259/133: write(test_file_noise, samp_rate, aug_data)
259/134: test_file_noise = "/Users/dustin/CS/consulting/firstlayerai/data/"
259/135: write(test_file_noise, samp_rate, aug_data)
259/136: ls
259/137: write("~/", samp_rate, aug_data)
259/138: write(r"~/", samp_rate, aug_data)
259/139: write("./", samp_rate, aug_data)
259/140: test_file_noise = "/Users/dustin/CS/consulting/firstlayerai/data/test_file_noise.wav"
259/141: write(test_file_noise, samp_rate, aug_data)
259/142: noise_dst.dot(noise_dst)
259/143: data = audio_dat
259/144: data = audio_data
259/145: data.dot(data)
259/146: data_energy
259/147: noise_energy
259/148: noise_dst
259/149: noise_dst.sum()
259/150: data.sum()
259/151: from librosa.feature import rmse
259/152:  import librosa.feature.rmse as rmse
259/153: librosa
259/154: import librosa
259/155: librosa.feature.rmse(data)
260/1: wav_file = "/Users/dustin/CS/consulting/firstlayerai/data/LibriSpeech/dev-clean/251/136532/251-136532-0001.wav"
260/2: noise_dir = "/Users/dustin/CS/consulting/firstlayerai/data/background_noise"
260/3: from speech.utils.wave import array_from_wave
260/4: data, samp_rate = array_from_wave(wav_file)
260/5: noise_levels=(0, 0.5)
260/6: pattern = os.path.join(noise_dir, "*.wav")
260/7: import os
260/8: pattern = os.path.join(noise_dir, "*.wav")
260/9: import glob
260/10: noise_files = glob.glob(pattern)
260/11: noise_path = np.random.choice(noise_files)
260/12: import numpy as np
260/13: noise_path = np.random.choice(noise_files)
260/14: noise_path
260/15: noise_level = np.random.uniform(*noise_levels)
260/16: noise_level
260/17: from speech.utils.wave import array_from_wave, wav_duration
260/18:  noise_len = wav_duration(noise_path)
260/19: noise_len
260/20: data_len = len(data) / samp_rate
260/21: data_len
260/22: noise_start = np.random.rand() * (noise_len - data_len)
260/23: noise_start
260/24: noise_end = noise_start + data_len
260/25: noise_end
260/26: from speech.utils import noise_injector
260/27: noise_dst = noise_injector.audio_with_sox(noise_path, samp_rate, noise_start, noise_end)
260/28: noise_dst
260/29: import importlib
260/30: importlib.reload(noise_injector)
260/31: noise_dst = noise_injector.audio_with_sox(noise_path, samp_rate, noise_start, noise_end)
260/32: noise_dst
260/33: assert len(data) == len(noise_dst)
260/34: noise_dst.dot(noise_dst)
260/35: data.dot(data)
260/36: np.dot(data, data)
260/37: data
260/38: data.sum()
260/39: noise_dst.sum()
260/40: data.mean()
260/41: noise_dst.mean()
260/42:     noise_energy = np.sqrt(np.abs(noise_dst.dot(noise_dst)) / noise_dst.size)
260/43:     data_energy = np.sqrt(np.abs(data.dot(data)) / data.size)
260/44: noise_energy
260/45: data_energy
260/46:     data += noise_level * noise_dst * data_energy / noise_energy
260/47: noise_level*noise_dst*data_energy/noise_energy
260/48: noise_val = noise_level*noise_dst*data_energy/noise_energy
260/49: noise_val.mean()
260/50: aug_data = data + noise_val
260/51: aug_data.mean()
260/52: from scipy.io.wavfile import write
260/53: test_file = "/Users/dustin/CS/consulting/firstlayerai/data/test_file_noise.wav"
260/54: write(test_file, aug_data, samp_rate)
260/55: write(test_file, samp_rate, aug_data)
260/56: l = [1, 2, -4, 5, -7]
260/57: arr = np.array(l)
260/58: l.dot(l)
260/59: arr.dot(arr)
260/60: data.dot(data)
260/61: data
260/62: np.abs(data).dot(np.abs(data))
260/63: data.size
260/64: np.sum(data*data)
260/65: noise_dst
260/66: noise_dst.dot(noise_dst)
260/67: data*data
260/68: data*data.size
260/69: (data*data).size
260/70: data.size
260/71: arr
260/72: arr*arr
260/73: arr.dot(arr)
260/74: np.sum(arr*arr)
260/75: data.dot(data)
260/76: np.sum(data*data)
260/77: noise_dst.dot(noise_dst)
260/78: np.sum(noise_dst*noise*dst)
260/79: np.sum(noise_dst*noise_dst)
260/80: help(np.inner)
260/81: np.dot(data,data)
260/82: np.inner(data,data)
260/83: sum(data[:]*data[:])
260/84: np.inner(data, data)
260/85: data.shape
260/86: ndim(data)
260/87: np.ndim(data)
260/88:  np.tensordot(data, data, axes=(-1,-1))
260/89: data[:100]
260/90: data.max()
260/91: data.min()
260/92: data[:600]
260/93: noise_dst
260/94: data[:100].dot(data[:100])
260/95: sum(data[:100]*data[:100])
260/96: index=500
260/97: data[:index].dot(data[:index])
260/98: sum(data[:index]*data[:index])
260/99: index=1500
260/100: data[:index].dot(data[:index])
260/101: sum(data[:index]*data[:index])
260/102: index=10000
260/103: data[:index].dot(data[:index])
260/104: sum(data[:index]*data[:index])
260/105: index=5000
260/106: data[:index].dot(data[:index])
260/107: sum(data[:index]*data[:index])
260/108: index=3000
260/109: data[:index].dot(data[:index])
260/110: sum(data[:index]*data[:index])
260/111: index=1000
260/112: data[:index].dot(data[:index])
260/113: sum(data[:index]*data[:index])
260/114: index=2000
260/115: data[:index].dot(data[:index])
260/116: sum(data[:index]*data[:index])
260/117: index=2500
260/118: data[:index].dot(data[:index])
260/119: sum(data[:index]*data[:index])
260/120: index=2900
260/121: data[:index].dot(data[:index])
260/122: sum(data[:index]*data[:index])
260/123: data[2500:2900]
260/124: rge=2500:2900
260/125: out_index=2900
260/126: in_index=2500
260/127: sum(data[in_index:out_index]*data[in_index:out_index])
260/128: data[in_index:out_index].dot(data[in_index:out_index])
260/129: index=2900
260/130: sum(data[:index]*data[:index])
260/131: data[:index].dot(data[:index])
260/132: index=2500
260/133: sum(data[:index]*data[:index])
260/134: data[:index].dot(data[:index])
260/135: [i for i in data[:10]]
260/136: sum([i**2 for i in data[:index]])
260/137: sum(data[:index]*data[:index])
260/138: index=2900
260/139: sum([i**2 for i in data[:index]])
260/140: sum(data[:index]*data[:index])
260/141: data[:index].dot(data[:index])
260/142: np.dot(data[:index],data[:index])
260/143: np.inner(data[:index],data[:index])
260/144: [i for i in data if np.iscomplex(data[i])==True]
260/145: [i for i in data if np.isreal(data[i])==False]
260/146: np.iscomplexobj(data)
260/147: np.tensordot(data,data)
260/148: np.tensordot(data,data,axes=1)
260/149: from scipy.io.wavfile import read
260/150: sr, data_sci = read(wav_file)
260/151: sr
260/152: data_sci
260/153: np.tensordot(data_sci,data_sci,axes=1)
260/154: data_sci = data_sci.astype('float32') / 32767
260/155: np.tensordot(data_sci,data_sci,axes=1)
260/156: np.dot(data_sci,data_sci)*32767*32767
260/157: sr, data_sci = read(wav_file)
260/158: np.dot(data_sci,data_sci)
260/159: data_sci.dtype
260/160: data_sci.max()
260/161: sq_data_sci = data_sci*data_sci
260/162: sq_data_sci.dtype
260/163: sum(sq_data_sci)
260/164: data_sci.dtype
260/165: data_sci.shape
260/166: sq_data_sci = data_sci*data_sci
260/167: sq_data_sci
260/168: sum(sq_data_sci)
260/169: sq_data_sci_64 = sq_data_sci.astype('float64')
260/170: sum(sq_data_sci_64)
260/171: sq_data_sci_64 = data_sci*data_sci
260/172: sum(sq_data_sci_64)
260/173: sq_data_sci_64.dtype
260/174: data_sci_64 = data_sci.astype('float64')
260/175: sq_data_sci_64 = data_sci_64*data_sci_64
260/176: sq_data_sci_64.dtype
260/177: sum(sq_data_sci_64)
260/178: data.mean()
260/179: data_sci_64.mean()
260/180: data_sci_64.dtype
260/181: data.dtype
260/182: data.max()
260/183: data_sci_64.max()
260/184: data_sci_64.dot(data_sci_64)
260/185: data.dot(data)
260/186: noise_dst.dtype
260/187: noise_dst_64 = noise_dst.astype('float64')
260/188: noise_dst_64.type
260/189: noise_dst_64.dtype
260/190: noise_energy_64 = np.sqrt(noise_dst_64.dot(noise_dst_64) / noise_dst.size_64)
260/191: noise_energy_64 = np.sqrt(noise_dst_64.dot(noise_dst_64) / noise_dst_64.size)
260/192: noise_energy_64
260/193: data_energy_64 = np.sqrt(data_sci_64.dot(data_sci_64) / data_sci_64.size)
260/194: data_energy_64
260/195: noise_level * noise_dst_64 * data_energy_64 / noise_energy_64
260/196: data_sci_64 += noise_level * noise_dst_64 * data_energy_64 / noise_energy_64
260/197: filename_64 = "/Users/dustin/CS/consulting/firstlayerai/data/test_file_noise_64.wav"
260/198: write(filename_64, samp_rate, data_sci_64)
260/199: noise_level
260/200: noise_level =0.1
260/201: write(filename_64, samp_rate, data_sci_64)
260/202: data_sci_64 += noise_level * noise_dst_64 * data_energy_64 / noise_energy_64
260/203: write(filename_64, samp_rate, data_sci_64)
260/204: noise_level =0.4
260/205: data_sci_64 += noise_level * noise_dst_64 * data_energy_64 / noise_energy_64
260/206: write(filename_64, samp_rate, data_sci_64)
260/207: filename_64 = "/Users/dustin/CS/consulting/firstlayerai/data/test_file_noise_64_nl0.4.wav"
260/208: filename_64_04 = "/Users/dustin/CS/consulting/firstlayerai/data/test_file_noise_64_nl0.4.wav"
260/209: write(filename_64_04, samp_rate, data_sci_64)
260/210: noise_level =0.26
260/211: filename_64_26 = "/Users/dustin/CS/consulting/firstlayerai/data/test_file_noise_64_nl26.wav"
260/212: % hist -g array
260/213: %hist -g array
260/214: data, samp_rate = array_from_wave(wav_file)
260/215: data_64 = data.astype('float64')
260/216: data_test_64 = data_64 + noise_level * noise_dst_64 * data_energy_64 / noise_energy_64
260/217: noise_level
260/218: filename_64_26 = "/Users/dustin/CS/consulting/firstlayerai/data/test_file_noise_64_nl26.wav"
260/219: write(filename_64_26, samp_rate, data_test_64)
260/220: noise_level = 0.4
260/221: filename_64_40 = "/Users/dustin/CS/consulting/firstlayerai/data/test_file_noise_64_nl40.wav"
260/222: data_test_64_26 = data_64 + noise_level * noise_dst_64 * data_energy_64 / noise_energy_64
260/223: noise_level=0.26
260/224: data_test_64_26 = data_64 + noise_level * noise_dst_64 * data_energy_64 / noise_energy_64
260/225: write(filename_64_26, samp_rate, data_test_64_26)
260/226: noise_level=0.40
260/227: data_test_64_40 = data_64 + noise_level * noise_dst_64 * data_energy_64 / noise_energy_64
260/228: filename_64_40 = "/Users/dustin/CS/consulting/firstlayerai/data/test_file_noise_64_nl40.wav"
260/229: write(filename_64_40, samp_rate, data_test_64_40)
260/230: noise_level = 0.0
260/231: filename_64_0 = "/Users/dustin/CS/consulting/firstlayerai/data/test_file_noise_64_nl0.wav"
260/232: data_test_64_0 = data_64 + noise_level * noise_dst_64 * data_energy_64 / noise_energy_64
260/233: write(filename_64_0, samp_rate, data_test_64_0)
260/234: noise_level = 0.1
260/235: filename_64_10 = "/Users/dustin/CS/consulting/firstlayerai/data/test_file_noise_64_nl10.wav"
260/236: data_test_64_10 = data_64 + noise_level * noise_dst_64 * data_energy_64 / noise_energy_64
260/237: write(filename_64_10, samp_rate, data_test_64_10)
260/238: filename_64_50 = "/Users/dustin/CS/consulting/firstlayerai/data/test_file_noise_64_nl50.wav"
260/239: noise_level=0.5
260/240: data_test_64_50 = data_64 + noise_level * noise_dst_64 * data_energy_64 / noise_energy_64
260/241: write(filename_64_50, samp_rate, data_test_64_50)
260/242: noise_level=0.6
260/243: data_test_64_60 = data_64 + noise_level * noise_dst_64 * data_energy_64 / noise_energy_64
260/244: filename_64_60 = "/Users/dustin/CS/consulting/firstlayerai/data/test_file_noise_64_nl60.wav"
260/245: write(filename_64_60, samp_rate, data_test_64_60)
260/246: noise_level=0.8
260/247: data_test_64_80 = data_64 + noise_level * noise_dst_64 * data_energy_64 / noise_energy_64
260/248: filename_64_80 = "/Users/dustin/CS/consulting/firstlayerai/data/test_file_noise_64_nl80.wav"
260/249: write(filename_64_80, samp_rate, data_test_64_80)
260/250: noise_level=1.0
260/251: filename_64_100 = "/Users/dustin/CS/consulting/firstlayerai/data/test_file_noise_64_nl100.wav"
260/252: data_test_64_100 = data_64 + noise_level * noise_dst_64 * data_energy_64 / noise_energy_64
260/253: write(filename_64_100, samp_rate, data_test_64_100)
260/254: data_test_64_100.max()
260/255: data_test_64_80.max()
260/256: data_test_64_80.min()
260/257: data_test_64_80.dtype
260/258: data_test_64_100_16.astype('int16')
260/259: data_test_64_100_16 = data_test_64_100.astype('int16')
260/260: data_test_64_100_16.dtype
260/261: data_test_64_100_16.max()
260/262: data_test_64_100_16.min()
260/263: filename_64_100_16 = "/Users/dustin/CS/consulting/firstlayerai/data/test_file_noise_64_nl100_16.wav"
260/264: write(filename_64_100_16, samp_rate, data_test_64_100_16)
260/265: data_test_64_0
260/266: data_test_64_0 = data_test_64_0.dtype('int16')
260/267: data_test_64_0 = data_test_64_0.astype('int16')
260/268: write(filename_64_0, samp_rate, data_test_64_0)
260/269: data_test_64_10 = data_test_64_10.astype('int16')
260/270: write(filename_64_10, samp_rate, data_test_64_10)
260/271: data_test_64_26 = data_test_64_26.astype('int16')
260/272: write(filename_64_26, samp_rate, data_test_64_26)
260/273: data_test_64_40 = data_test_64_40.astype('int16')
260/274: write(filename_64_40, samp_rate, data_test_64_40)
260/275: data_test_64_50 = data_test_64_50.astype('int16')
260/276: write(filename_64_50, samp_rate, data_test_64_50)
260/277: data_test_64_60 = data_test_64_60.astype('int16')
260/278: write(filename_64_60, samp_rate, data_test_64_60)
260/279: data_test_64_80 = data_test_64_80.astype('int16')
260/280: write(filename_64_80, samp_rate, data_test_64_80)
260/281: data_test_64_100 = data_test_64_100.astype('int16')
260/282: write(filename_64_100, samp_rate, data_test_64_100)
260/283: noise_len = wav_duration(noise_path)
260/284: noise_len
260/285: data_len = len(data) / samp_rate
260/286: data_len
260/287: noise_start = np.random.rand() * (noise_len - data_len)
260/288: noise_end = noise_start + data_len
260/289: noise_start
260/290: noise_end
260/291: noise_dst = audio_with_sox(noise_path, sample_rate, noise_start, noise_end)
260/292: noise_dst = noise_injector.audio_with_sox(noise_path, sample_rate, noise_start, noise_end)
260/293: noise_dst = noise_injector.audio_with_sox(noise_path, samp_rate, noise_start, noise_end)
260/294: noise_dst = noise_dst.astype('float64')
260/295: len(noise_dst)
260/296: len(data)
260/297: data = data.astype('float64')
260/298: assert len(data) == len(noise_dst)
261/1:
import pandas as pd
import pandas_profiling
import sklearn
261/2:
# data from https://www.kaggle.com/datasnaek/youtube-new
US_DATA_PATH:str = "/Users/dustin/CS/job_projects/prep/triplebyte/data/USvideos.csv"
261/3: us_df = pd.read_csv(US_DATA_PATH)
261/4: us_df.profile_report()
261/5: us_df///
261/6: us_df
261/7: us_df['over_1M']=us_df.apply(lambda x: x>1000000, us_df['views'])
261/8: us_df['over_1M']=us_df.apply('views'>1000000)
261/9:
def binary_threshold(df, column_name, threshold):
    if df[column_name]>threshold:
        return 1
    else: 
        return 0
261/10:
def binary_threshold(df):
    threshold = 1000000
    column_name = 'views'
    if df[column_name]>threshold:
        return 1
    else: 
        return 0
261/11: us_df['over_1M']=us_df.apply(binary_threshold, axis=1)
261/12:
#label column
label_col = ["over_1M"]

#feature columns
categorical_col=["video_id", "channel_title", "category_id", "tags", ""]
numeric_col=["likes", "dislikes", "comment_count", ""]
text_cols=["title", "thumbnail_link"]
datetime_cols = ["trending_date","publish_time",""]
boolean_cols = ["comments_disabled", "ratings_disabled", 
not_useful_cols = [video_error_or_removed"]
261/13:
#label column
label_col = ["over_1M"]

#feature columns
categorical_col=["video_id", "channel_title", "category_id", "tags", ""]
numeric_col=["likes", "dislikes", "comment_count", ""]
text_cols=["title", "thumbnail_link"]
datetime_cols = ["trending_date","publish_time",""]
boolean_cols = ["comments_disabled", "ratings_disabled", 
not_useful_cols = [video_error_or_removed"]
261/14:
#label column
label_col = ["over_1M"]

#feature columns
all_cols_dict = {
categorical_col: ["video_id", "channel_title", "category_id", "tags"],
numeric_col: ["likes", "dislikes", "comment_count"],
text_cols: ["title", "thumbnail_link", "description"],
datetime_cols: ["trending_date","publish_time"],
boolean_cols: ["comments_disabled", "ratings_disabled"],
not_useful_cols: ["video_error_or_removed"]
}

feature_dict = {
categorical_col: ["video_id", "channel_title", "category_id", "tags"],
numeric_col: ["likes", "dislikes", "comment_count"],
}
261/15:
#label column
label_col = ["over_1M"]

#feature columns
all_cols_dict = {
"categorical_cols": ["video_id", "channel_title", "category_id", "tags"],
"numeric_cols": ["likes", "dislikes", "comment_count"],
"text_cols": ["title", "thumbnail_link", "description"],
datetime_cols: ["trending_date","publish_time"],
boolean_cols: ["comments_disabled", "ratings_disabled"],
not_useful_cols: ["video_error_or_removed"]
}

feature_dict = {
categorical_col: ["video_id", "channel_title", "category_id", "tags"],
numeric_col: ["likes", "dislikes", "comment_count"],
}
261/16:
#label column
label_col = ["over_1M"]

#feature columns
all_cols_dict = {
"categorical_cols": ["video_id", "channel_title", "category_id", "tags"],
"numeric_cols": ["likes", "dislikes", "comment_count"],
"text_cols": ["title", "thumbnail_link", "description"],
"datetime_cols": ["trending_date","publish_time"],
"boolean_cols": ["comments_disabled", "ratings_disabled", "video_error_or_removed"]
}

feature_dict = {
"categorical_col": ["video_id", "channel_title", "category_id", "tags"],
"numeric_col": ["likes", "dislikes", "comment_count"]
}
261/17:
#convert categorical to one-hot-encodings
feature_col_list = []
for value_list in feature_dict.values():
    feature_col_list.extend(value_list)
    
feature_df = us_df[feature_col_list]
261/18: feature_df
261/19: feature_df[video_id].type
261/20: feature_df['video_id'].type
261/21: feature_df['video_id'].dtype
261/22: feature_df['video_id'].dtypes
261/23:
for label, content in feature_df.items():
    print(f"{label} is type: {feature_df[label].dtype}")
261/24:
# create one-hot-encodings
pd.get_dummies(feature_df, prefix="OHE", prefix_sep='_', columns=feature_dict.get("categorical_col"))
261/25: feature_df
261/26:
# create one-hot-encodings
data_df = pd.get_dummies(feature_df, prefix="OHE", prefix_sep='_', columns=feature_dict.get("categorical_col"))
261/27: data_df.describe
261/28: data_df.info
261/29: len(data_df)
261/30: data_df.shape
261/31:
print(f"feature_df shape: {feature_df.shape}")
print(f"data_df shape: {data_df.shape}")
261/32:
import pandas as pd
import pandas_profiling
from sklearn.linear_model import LogisticRegression
261/33: # feed trimmed and processed dataframe into regression
261/34:
# make dataframe of feature columns
feature_col_list = []
for value_list in feature_dict.values():
    feature_col_list.extend(value_list)
    
feature_df = us_df[feature_col_list]
label_df = us_df["over_1M"]
261/35: clf = LogisticRegression(pentalty='l2', random_state=0).fit(data_df, label_df)
261/36: clf = LogisticRegression(penalty='l2', random_state=0).fit(data_df, label_df)
261/37:
import pandas as pd
import pandas_profiling
from sklearn.linear_model import LogisticRegression
261/38: clf.type()
261/39: type(clf)
262/1: import torch
262/2: 0310_model_path = "/Users/dustin/CS/consulting/firstlayerai/phoneme_classification/src/awni_speech/speech/examples/librispeech/models/ctc_models/20200305/20200310/model"
262/3: model_path_0310 = "/Users/dustin/CS/consulting/firstlayerai/phoneme_classification/src/awni_speech/speech/examples/librispeech/models/ctc_models/20200305/20200310/model"
262/4: model_0310 = torch.load(model_path_0310, torch.device('cpu'))
262/5: model_0310.parameters()
262/6: tensor = torch.tensor([1,2,3,4])
262/7: 4 in tensor
262/8: 5 in tensor
263/1: %hist
263/2: %history
263/3: %history ~1/1-100
263/4: tensor
263/5: %paste
263/6: import torch
263/7: model_path_0310 = "/Users/dustin/CS/consulting/firstlayerai/phoneme_classification/src/awni_speech/speech/examples/librispeech/models/ctc_models/20200305/20200310/model"
263/8: model_0310 = torch.load(model_path_0310, torch.device('cpu'))
263/9: model_0310
263/10: tensor = torch.tensor([1,2,3,4])
263/11: 0 in tensor
263/12: tensor = torch.tensor([0,1,2,3,4])
263/13: 0 in tensor
263/14: import numpy as np
263/15: tensor = torch.tensor([0,1,2,3,4, np.nan])
263/16: tensor != tensor
263/17: np.sum(tensor != tensor)
263/18: torch.sum(tensor != tensor)
263/19: tensor_nonan = torch.tensor([0,1,2,3,4])
263/20: torch.sum(tensor_nonan != tensor_nonan)
263/21: (tensor != tensor).any()
263/22: (tensor_nonan != tensor_nonan).any()
263/23: (tensor_nonan != tensor_nonan).any()==0
263/24:
if (tensor_nonan != tensor_nonan).any():
    print(hello)
263/25:
if (tensor != tensor).any():
    print(hello)
263/26:
if (tensor != tensor).any():
    print("hello")
263/27: model_0305.parameters()
263/28: model_0310.parameters()
263/29:
for i in model_0310.parameters():
    print(i)
263/30:
for name, contents in model.stat_dict().items():
    print(name, contents)
263/31:
for name, contents in model_3010.stat_dict().items():
    print(name, contents)
263/32:
for name, contents in model_0310.stat_dict().items():
    print(name, contents)
263/33:
for name, contents in model_0310.state_dict().items():
    print(name, contents)
263/34: import randomm
263/35: import random
263/36: random.randint(0,0)
263/37: random.randint(0,0)
263/38: model_path_0308 = "/Users/dustin/CS/consulting/firstlayerai/phoneme_classification/src/awni_speech/speech/examples/librispeech/models/ctc_models/20200305/20200308"
263/39: from speech.utils.io impmort load
263/40: from speech.utils.io import load
263/41: model_0308, preproc_0308 = load(model_path_0308)
263/42: ls_testaudio = "/Users/dustin/CS/consulting/firstlayerai/data/LibriSpeech/dev-clean/174/84280/174-84280-0000.wav"
263/43: ls_input, ls_label = preproc_0308.preprocess(ls_testaudio, )
263/44: ls_testtext = ["hh", "aw", "w", "iy", "m", "ah", "s", "t", "s", "ih", "m", "p", "l", "ah", "f", "ay"]
263/45: ls_input, ls_label = preproc_0308.preprocess(ls_testaudio, ls_testtext)
263/46: from speech.utils import wave
263/47: audio_data, samp_rate = wave.array_from_wave(ls_testaudio)
263/48: from speech import loader
263/49: inputs = loader.log_specgram_from_data(audio_data, samp_rate, 32, 16)
263/50: inputs = (inputs - preproc_0308.mean) / preproc_0308.std
263/51: targets = preproc_0308.encode(ls_testtext)
263/52: inupts.shape
263/53: inputs.shape
263/54: inputs_zero = inputs
263/55: inputs_zero[:,:] = 0
263/56: inputs_zero.shape
263/57: np.sum(inputs_zero)
263/58: np.sum(inputs)
263/59: inputs = loader.log_specgram_from_data(audio_data, samp_rate, 32, 16)
263/60: inputs = (inputs - preproc_0308.mean) / preproc_0308.std
263/61: inputs.sum()
263/62: inputs_copy = inputs.copy()
263/63: inputs_copy = 0
263/64: inputs_copy.sum()
263/65: inputs.sum()
263/66: inputs_zero.shape
263/67: ctc_config_0305_path = "/Users/dustin/CS/consulting/firstlayerai/phoneme_classification/src/awni_speech/speech/examples/librispeech/models/ctc_models/20200305/20200308/ctc_config.json"
263/68: import json
263/69:
with open(ctc_config_0305_path, 'r') as fid:
    ctc_config_0305 = json.load(fid)
263/70: ctc_config_0305
263/71: ctc_config_0305 = config
263/72: config = ctc_config_0305
263/73: config
263/74:
    opt_cfg = config["optimizer"]
    data_cfg = config["data"]
    preproc_cfg = config["preproc"]
    model_cfg = config["model"]
263/75:
    optimizer = torch.optim.SGD(model.parameters(),
                    lr=opt_cfg["learning_rate"],
                    momentum=opt_cfg["momentum"])
263/76:
    optimizer_0308 = torch.optim.SGD(model_0308.parameters(),
                    lr=opt_cfg["learning_rate"],
                    momentum=opt_cfg["momentum"])
263/77: optimizer_0308.parameters()
263/78: optimizer_0308.state_dict()
263/79: optimizer.zero_grad()
263/80: optimizer_0308.zero_grad()
263/81: optimizer_0308.state_dict()
263/82: import functions.ctc as ctc
263/83: out, rnn_args = model_0308.forward_impl(inputs_zero)
263/84: inputs = toorch.IntTensor(inputs)
263/85: inputs = torch.IntTensor(inputs)
263/86: label = torch.IntTensor([27])
263/87: inputs_zero = toorch.IntTensor(inputs_zero)
263/88: inputs_zero = torch.IntTensor(inputs_zero)
263/89: out, rnn_args = model_0308.forward_impl(inputs_zero)
263/90: inputs_zero = inputs_zero.unsqeeze(1)
263/91: inputs_zero = inputs_zero.unsqueeze(1)
263/92: inputs_zero.shape
263/93: inputs_zero = inputs_zero.squeeze()
263/94: inputs_zero.shape
263/95: inputs_zero = inputs_zero.unsqueeze(0)
263/96: inputs_zero.shape
263/97: out, rnn_args = model_0308.forward_impl(inputs_zero)
263/98: import importlib
263/99: imports_zero.shape
263/100: inputs_zero.shape
263/101: inputs_zero = inputs_copy.copy()
263/102: inputs_zero = inputs.copy()
263/103: inputs_zero.detach().cpu().numpy()
263/104: inputs_zero =  inputs_zero.detach().cpu().numpy()
263/105: labels = [27]
263/106: batch = ((inputs_zero, labels))
263/107: model_0308.collate(*batch)
263/108: labels = [[27]]
263/109: batch = ((inputs_zero, labels))
263/110: model_0308.collate(*batch)
263/111: x,y,x_lens,y_lens = model_0308.collate(*batch)
263/112: out, rnn_args = model_0308.forward_impl(x)
263/113: loss_fn = ctc.CTCLoss()
263/114: loss = loss_fn(out, y, x_lens, y_lens)
263/115: loss.item()
263/116: loss.backward()
263/117: import torch.nn as nn
263/118: grad_norm = nn.utils.clip_grad_norm_(model_0308.parameters(), 200)
263/119: loss = loss.item()
263/120: grad_norm
263/121: optimizer.step()
263/122: optimizer_0308.step()
263/123: grad_norm
263/124: loss
263/125: model_0308.state_dict()
263/126: inputs_zero
263/127: inputs_zero.sum()
263/128: model_0308.conv.9.weight.grad
263/129: model_0308.conv9.weight.grad
263/130: print(model_0308.conv.9.weight.grad)
263/131: print(model_0308."conv.9.weight".grad)
263/132:  model_0308.state_dict_keys()
263/133:  model_0308.state_dict()
263/134: %who
263/135: model_0308_copy = torch.load(model_path_0308, torch.device('cpu'))
263/136: model_0308_copy = torch.load(model_path_0308+"/best_model", torch.device('cpu'))
263/137:
for name, content in model_0308.state_dict().items(): 
    print(content.grad)
263/138:
for name, content in model_0308.state_dict().items(): 
    print(content)
263/139:
for name, content in model_0308.state_dict().items(): 
    print(content.grad())
263/140:
for name, content in model_0308.state_dict().items(): 
    print(content.grad)
263/141: model_0308
263/142: model_0308.parameters()
263/143: model_0308.state_dict()
263/144: model_0308.parameters()
263/145: i = 0
263/146:
for param in model_0308.parameters():
    if i==0:
        print(param)
    i++
263/147:
for param in model_0308.parameters():
    if i==0:
        print(param)
    i+=1
263/148:
def check_nan(x):
    return (x!=x).any()
263/149:
for param in model_0308.parameters():
    print(check_nan(param))
263/150:
for param in model_0308.parameters():
    if check_nan(param):
        print("found nan")
263/151:
for param in model_0308.parameters():
    if not check_nan(param):
        print("not found nan")
263/152: (check_nan(param) for param in model_0308.parameters()).any()
263/153: [check_nan(param) for param in model_0308.parameters()].any()
263/154: round(7.28293, 2)
263/155:
array = [10, 16,  1, 16,  1,  8, 16, 19, 32, 16, 27,  1, 16, 11, 16,  6,  9, 12,
        16, 31, 36,  3, 22,  1, 19, 18,  7, 29, 13, 37, 13, 37, 28, 14, 25, 23,
        17, 19, 25, 32, 19, 14, 25, 19,  9, 31, 10, 16, 12,  9, 35,  1, 12, 21,
         2,  9, 35, 31,  3, 25, 12,  2, 32, 10,  7, 19, 13, 37, 14,  2,  1, 20,
         3, 25,  9, 22, 36,  8, 24, 21, 18, 22,  3, 19,  1, 20, 25, 37, 34,  4,
        31, 20, 10, 16, 15, 17, 19, 18, 28, 14, 25, 38,  9, 36, 38,  9,  1, 19,
        18, 32, 36, 16,  1, 19, 15, 32, 10,  9,  1, 14, 25,  9, 19,  9, 29, 22,
        16,  5,  9, 35, 17, 27, 18, 32,  2,  3, 38,  6,  9, 11, 10, 16,  1, 19,
         4, 19,  9, 22,  7, 12, 19,  1,  9, 22, 31,  9, 12,  4, 19,  1, 16, 28,
         7, 12, 19,  9, 22, 38, 13, 37, 36, 16, 22, 22,  4, 11, 18, 10,  7, 19,
         9, 29, 14,  2,  9, 19,  6,  0, 31,  1, 19, 25,  8, 12, 13, 37, 15,  2,
         8, 22, 31, 12,  0, 31,  8, 31, 37, 34, 16,  1, 19, 16,  1, 19, 16, 10,
        16, 28,  0,  2, 31,  9, 19,  4,  2,  1, 20,  2, 16, 12, 16,  2, 32, 28,
        14, 25, 15, 20,  5, 36,  8, 28, 25,  4, 29, 16, 29, 28,  4,  2, 10, 16,
         1, 36, 16, 11, 38, 21, 17, 27, 18,  8, 36,  4,  7, 31, 38, 18, 13, 18,
        29,  6, 18, 25,  8, 24,  9, 28, 20, 22,  2, 32, 38, 32, 12,  0, 31,  6,
         9, 22, 16, 13,  4,  2, 14, 25, 16, 38,  3, 25, 27, 18, 31, 23,  4, 36,
        24, 18, 38,  7, 24,  1, 10,  9,  1,  9, 12,  1, 24,  2,  4, 22, 29, 38,
         6,  8, 38,  6, 16, 22, 36, 32, 19,  1,  1, 20, 28, 13, 37, 24, 25,  9,
        22,  1, 19, 16, 22, 36, 17, 22,  1, 20, 30, 16,  2, 32,  1, 16, 36,  6,
        32, 38,  7, 27, 28, 21, 22, 31, 38,  6,  9, 11,  9, 29,  9, 12,  1, 17,
         2, 31,  9, 22, 22, 20, 16, 10, 18,  1, 17, 12, 30, 16, 22, 16, 27, 10,
        16,  1, 21,  5, 12, 16, 22, 19, 25, 32, 14,  2,  1, 20, 16, 35, 12, 16,
         2, 34,  3, 22,  6,  9, 10, 15, 17,  5,  7, 22, 31, 24,  7, 19,  1, 32,
        28, 25, 32, 12,  6, 16, 22, 19,  9, 31, 10, 16, 30,  3, 24,  1, 16, 27,
        10, 16,  6,  0, 31,  6, 18, 12, 18, 29, 38, 21,  9, 29, 13, 14, 25, 36,
        16, 10, 18, 29, 25, 37, 36, 16, 19,  9, 29, 16, 36, 30, 32,  1, 17, 31,
        10, 16, 14,  5, 18, 29,  2,  4, 15, 18, 29,  6, 18, 12, 16, 19, 30, 14,
        25, 19, 15,  8, 10, 16,  1, 16, 31, 16, 22,  9, 22, 19, 25, 37, 33, 16,
        22, 16, 27, 16, 36,  4, 31, 29, 18, 27, 16, 22, 19, 16, 34, 17, 22, 16,
         2, 36, 16, 22, 19, 16,  1, 32, 13, 37,  1, 18, 38, 32,  9, 29,  1, 16,
        36, 16, 22, 28, 14, 25, 11, 16, 22, 16, 19, 38, 37, 31, 16, 29, 22,  3,
        19, 38,  7, 27, 15, 25, 17, 31, 17, 27, 18, 32, 31,  4, 16,  5, 21, 29,
        16, 22, 22, 14,  9, 35, 12, 17, 25, 29, 16,  5, 21, 29, 16, 22, 36, 14,
        25, 19, 16,  2,  1,  3, 25, 20, 29, 16,  6,  4, 19, 38,  9, 29, 25,  9,
        19, 18, 22, 19, 16, 12,  3, 22, 30, 16,  1, 22, 16,  1, 16, 27, 10, 16,
         1,  4, 36,  2,  7, 22, 31,  9, 22, 10, 16,  2,  7,  1, 28,  9, 28, 19,
        32, 13, 18, 29, 10, 16, 31,  9,  1, 12, 16, 27, 18, 32, 16, 27, 15,  9,
         2, 31,  9, 35,  1, 19, 20, 22, 12, 20,  2, 22,  7, 11, 18, 16,  2, 23,
         7,  1, 14, 25, 26,  2,  2,  7, 22, 31, 36,  4, 36,  4, 12, 10, 16,  7,
        22, 13, 37, 16,  2, 25, 17, 22, 19, 14, 25, 25, 26, 16,  2, 19, 32]
263/156: len(array)
263/157: % hist
263/158: %hist
263/159: inputs_zero.shape
263/160: np.expand_dims(inputs_zero, axis=0).shape
263/161: from speech.utils import noise_injector
263/162: array_from_wave
263/163: from speech.utils import wave
263/164: audio_file
263/165: %who
263/166: audio_data
263/167: ls_testaudio
263/168:
data = noise_injector.inject_noise(audio_data, samp_rate, 
)
263/169: noise_dir = "/Users/dustin/CS/consulting/firstlayerai/data/background_noise"
263/170: data = noise_injector.inject_noise(audio_data, samp_rate, noise_dir, (0,0.5))
263/171: noise_path = "/Users/dustin/CS/consulting/firstlayerai/data/background_noise/Amusement_Park_Ambiance-SoundBible-250228572_extended.wav"
263/172: data = noise_injector.inject_noise_sample(audio_data, samp_rate, noise_path, (0,0.5))
263/173: data = noise_injector.inject_noise_sample(audio_data, samp_rate, noise_path, 0.5)
263/174: importlib.reload(noise_injector)
263/175: data = noise_injector.inject_noise_sample(audio_data, samp_rate, noise_path, 0.5)
263/176: audio_data.size
263/177: audio_data.shape
263/178: audio_data[:-1].size
263/179: diff = 1
263/180: audio_data[:-diff].size
263/181: diff=5
263/182: audio_data[:-diff].size
263/183: np.concatenate(audio_data, np.zeros((diff)), axis=0).size
263/184: np.concatenate((audio_data, np.zeros((diff))), axis=0).size
263/185: np.concatenate((audio_data, np.zeros((diff))), axis=0).size
263/186: probs = np.array([0,1,2,3])
263/187: np.log(probs)
263/188: probs = np.array([np.nan,1,2,3])
263/189: np.log(probs)
263/190: from speech.utils import process_noise
263/191: ls
263/192: from speech.utils import process_noise
263/193: from speech.utils import process_noise
263/194: from speech.utils import process_noise
263/195: audio_dir = "/Users/dustin/CS/consulting/firstlayerai/data/background_noise"
263/196: process_noise.resample(audio_dir, 16000)
263/197: from speech.utils import process_noise
263/198: process_noise.resample(audio_dir, 16000)
263/199: importlib.reload(process_noise)
263/200: process_noise.resample(audio_dir, 16000)
263/201: importlib.reload(process_noise)
263/202: process_noise.resample(audio_dir, 16000)
263/203: importlib.reload(process_noise)
263/204: process_noise.resample(audio_dir, 16000)
263/205: importlib.reload(process_noise)
263/206: process_noise.resample(audio_dir, 16000)
263/207: importlib.reload(process_noise)
263/208: process_noise.resample(audio_dir, 16000)
263/209: importlib.reload(process_noise)
263/210: process_noise.resample(audio_dir, 16000)
263/211: importlib.reload(process_noise)
263/212: process_noise.resample(audio_dir, 16000)
263/213: importlib.reload(process_noise)
263/214: process_noise.resample(audio_dir, 16000)
263/215: audio_data.shape
263/216: test_noise_data = np.concatenate((audio_data, audio_data))
263/217: test_noise_data.shape
263/218: noise_start = np.random.rand() * (test_noise_data.size - audio_data.size)
263/219: noise_start
263/220: np.random.rand()
263/221: (test_noise_data.size - audio_data.size)
263/222: np.random.rand()
263/223: noise_start = np.random.rand() * (test_noise_data.size - audio_data.size)
263/224: noise_start
263/225: noise_start = int(np.random.rand() * (test_noise_data.size - audio_data.size))
263/226: noise_start
263/227: noise_end = noise_start + audio_data.size
263/228: noise_end
263/229: noise_end - noise_start
263/230: test_noise_data[noise_start:noise_end].size
263/231: path ="/Users/dustin/CS/consulting/firstlayerai/data/LibriSpeech/manifest.csv"
263/232:
with (path, 'w') as fid:
    fid.write(f"{noise_end}, {noise_start}\n")
263/233:
with open(path, 'w') as fid:
    fid.write(f"{noise_end}, {noise_start}\n")
263/234:
with open(path, 'w') as fid:
    fid.write(f"{noise_end}, {noise_start}\n")
    fid.write(f"{noise_end}, {noise_start}\n") 
    fid.write(f"{noise_end}, {noise_start}\n")
264/1: s = str
264/2: s
264/3: type(s)
264/4: s = str()
264/5: type(s)
264/6: s
264/7: s+"hello"
264/8: s.append(" tree")
264/9: s.add(" tree")
264/10: arr = [1,2,3,4]
264/11: " ".join(arr)
264/12: " ".join(*arr)
264/13: *arr
264/14: " ".join(map(lambda x: str(x), arr))
264/15: " ".join(map(str(), arr))
264/16: " ".join(map(str, arr))
264/17: [0]*10
265/1:
import pandas as pd
import pandas_profiling
from sklearn.linear_model import LogisticRegression
265/2:
# data from https://www.kaggle.com/datasnaek/youtube-new
US_DATA_PATH:str = "/Users/dustin/CS/job_projects/prep/triplebyte/data/USvideos.csv"
265/3: us_df = pd.read_csv(US_DATA_PATH)
265/4: us_df.profile_report()
265/5:
def download_read_data()):
    
    # data from https://www.kaggle.com/datasnaek/youtube-new
    US_DATA_PATH:str = "/Users/dustin/CS/job_projects/prep/triplebyte/data/USvideos.csv" 
    us_df = pd.read_csv(US_DATA_PATH)
download_read_data()
265/6: download_read_data()
265/7:
def download_read_data():
    # data from https://www.kaggle.com/datasnaek/youtube-new
    US_DATA_PATH:str = "/Users/dustin/CS/job_projects/prep/triplebyte/data/USvideos.csv" 
    us_df = pd.read_csv(US_DATA_PATH)
265/8: download_read_data()
265/9:
def explore_data():
    us_df.profile_report()
265/10: explore_data()
265/11:
def process_data():
    us_df['over_1M']=us_df.apply(binary_threshold, axis=1)
    
    
    #label column
    label_col = ["over_1M"]

    #feature columns
    all_cols_dict = {
    "categorical_cols": ["video_id", "channel_title", "category_id", "tags"],
    "numeric_cols": ["likes", "dislikes", "comment_count"],
    "text_cols": ["title", "thumbnail_link", "description"],
    "datetime_cols": ["trending_date","publish_time"],
    "boolean_cols": ["comments_disabled", "ratings_disabled", "video_error_or_removed"]
    }

    feature_dict = {
    "categorical_col": ["video_id", "channel_title", "category_id", "tags"],
    "numeric_col": ["likes", "dislikes", "comment_count"]
    }
265/12:
def binary_threshold(df):
    threshold = 1000000
    column_name = 'views'
    if df[column_name]>threshold:
        return 1
    else: 
        return 0
265/13: process_data()
265/14: print(all_cols_dict)
265/15: print(all_cols_dict)
265/16: all_cols_dict
265/17: process_data()
265/18: all_cols_dict
265/19:
# data from https://www.kaggle.com/datasnaek/youtube-new
US_DATA_PATH:str = "/Users/dustin/CS/job_projects/prep/triplebyte/data/USvideos.csv"
265/20: us_df = pd.read_csv(US_DATA_PATH)
265/21: us_df.profile_report()
265/22: # make binary variable like greather than 1 million views
265/23:
def binary_threshold(df):
    threshold = 1000000
    column_name = 'views'
    if df[column_name]>threshold:
        return 1
    else: 
        return 0
265/24: us_df['over_1M']=us_df.apply(binary_threshold, axis=1)
265/25: # process columns to be fed into log_reg like one hot representations
265/26:
#label column
label_col = ["over_1M"]

#feature columns
all_cols_dict = {
"categorical_cols": ["video_id", "channel_title", "category_id", "tags"],
"numeric_cols": ["likes", "dislikes", "comment_count"],
"text_cols": ["title", "thumbnail_link", "description"],
"datetime_cols": ["trending_date","publish_time"],
"boolean_cols": ["comments_disabled", "ratings_disabled", "video_error_or_removed"]
}

feature_dict = {
"categorical_col": ["video_id", "channel_title", "category_id", "tags"],
"numeric_col": ["likes", "dislikes", "comment_count"]
}
265/27:
# make dataframe of feature columns
feature_col_list = []
for value_list in feature_dict.values():
    feature_col_list.extend(value_list)
    
feature_df = us_df[feature_col_list]
label_df = us_df["over_1M"]
265/28:
# review data types of feature_df
for label, content in feature_df.items():
    print(f"{label} is type: {feature_df[label].dtype}")
265/29:
# create one-hot-encodings
data_df = pd.get_dummies(feature_df, prefix="OHE", prefix_sep='_', columns=feature_dict.get("categorical_col"))
265/30:
print(f"feature_df shape: {feature_df.shape}")
print(f"data_df shape: {data_df.shape}")
265/31: # feed trimmed and processed dataframe into regression
265/32: clf = LogisticRegression(penalty='l2', random_state=0).fit(data_df, label_df)
265/33: type(clf)
265/34: ## CREATE FUNCTION CALLED FEATURE EXTRACTOR AND MAKE FUNCTION FOR EACH BIG STEP. INSTEAD OF MAKING COMMENTS HAVE THE FUNCTION BE COMMENTS
265/35: # interpret results
265/36:
test_size = 0.1
dev_size = 0.1
X_train, X_dev, X_test, y_train, y_dev, y_test = train_dev_test_split(feature_df, label_df, dev_size=dev_size, test_sie=test)_zzE)
265/37:
test_size = 0.1
dev_size = 0.1

X_train, X_dev, X_test, y_train, y_dev, y_test = train_dev_test_split(feature_df, label_df, dev_size=dev_size, test_size=test_size)
265/38:
import pandas as pd
import pandas_profiling
from sklearn.linear_model import LogisticRegression
sklearn.model_selection import train_test_split
265/39:
import pandas as pd
import pandas_profiling
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
265/40:
print(f"feature_df shape: {feature_df.shape}")
print(f"data_df shape: {data_df.shape}")
265/41:
def train_dev_test_split(feature_df, label_df, dev_size, test_size):
    """
    splits the data into train, dev, and test sets
    """
    
    X_train_tmp, X_test, Y_train_tmp, Y_test = train_test_split(feature_df, label_df, test_size=test_size)
    
    true_dev_size = dev_size /(1-test_size)
    
    X_train, X_dev, Y_train, Y_dev = train_test_split(feature_df, label_df, test_size=true_dev_size)
    
    return X_train, X_dev, X_test, y_train, y_dev, y_test
265/42:
test_size = 0.1
dev_size = 0.1

X_train, X_dev, X_test, y_train, y_dev, y_test = train_dev_test_split(feature_df, label_df, dev_size=dev_size, test_size=test_size)
265/43:
def train_dev_test_split(feature_df, label_df, dev_size, test_size):
    """
    splits the data into train, dev, and test sets
    """
    
    X_train_tmp, X_test, y_train_tmp, y_test = train_test_split(feature_df, label_df, test_size=test_size)
    
    true_dev_size = dev_size /(1-test_size)
    
    X_train, X_dev, y_train, y_dev = train_test_split(X_train_tmp, y_train_tmp, test_size=true_dev_size)
    
    return X_train, X_dev, X_test, y_train, y_dev, y_test
265/44:
test_size = 0.1
dev_size = 0.1

X_train, X_dev, X_test, y_train, y_dev, y_test = train_dev_test_split(feature_df, label_df, dev_size=dev_size, test_size=test_size)
265/45:
test_size = 0.1
dev_size = 0.1

X_train, X_dev, X_test, y_train, y_dev, y_test = train_dev_test_split(feature_df, label_df, dev_size=dev_size, test_size=test_size)



print(f"{X_train.shape}")
265/46:
test_size = 0.1
dev_size = 0.1

X_train, X_dev, X_test, y_train, y_dev, y_test = train_dev_test_split(feature_df, label_df, dev_size=dev_size, test_size=test_size)



print(f"{X_train.shape}. {X_dev.shape}. {X_test.shape}.")
265/47:
test_size = 0.1
dev_size = 0.1

X_train, X_dev, X_test, y_train, y_dev, y_test = train_dev_test_split(feature_df, label_df, dev_size=dev_size, test_size=test_size)



print(f"{X_train.shape}. {X_dev.shape}. {X_test.shape}. {y_train.shape}.")
265/48:
test_size = 0.1
dev_size = 0.1

X_train, X_dev, X_test, y_train, y_dev, y_test = train_dev_test_split(feature_df, label_df, dev_size=dev_size, test_size=test_size)



print(f"{X_train.shape}. {X_dev.shape}. {X_test.shape}. {y_train.shape}. {y_dev.shape}.")
265/49:
test_size = 0.1
dev_size = 0.1

X_train, X_dev, X_test, y_train, y_dev, y_test = train_dev_test_split(feature_df, label_df, dev_size=dev_size, test_size=test_size)



print(f"{X_train.shape}. {X_dev.shape}. {X_test.shape}. {y_train.shape}. {y_dev.shape}. {y_test.shape}.")
265/50: clf = LogisticRegression(penalty='l2', random_state=0).fit(X_train, y_train)
265/51: type(clf)
265/52:
test_size = 0.1
dev_size = 0.1

X_train, X_dev, X_test, y_train, y_dev, y_test = train_dev_test_split(feature_df, label_df, dev_size=dev_size, test_size=test_size)



print(f"{X_train.shape}. {X_dev.shape}. {X_test.shape}. {y_train.shape}. {y_dev.shape}. {y_test.shape}.")
265/53: # feed trimmed and processed dataframe into regression
265/54: clf = LogisticRegression(penalty='l2', random_state=0).fit(X_train, y_train)
265/55:
# create one-hot-encodings
featre_df = pd.get_dummies(feature_df, prefix="OHE", prefix_sep='_', columns=feature_dict.get("categorical_col"))
265/56:
print(f"feature_df shape: {feature_df.shape}")
print(f"data_df shape: {data_df.shape}")
265/57:
# create one-hot-encodings
data_df = pd.get_dummies(feature_df, prefix="OHE", prefix_sep='_', columns=feature_dict.get("categorical_col"))
265/58:
print(f"feature_df shape: {feature_df.shape}")
print(f"data_df shape: {data_df.shape}")
265/59:
# create one-hot-encodings
feature_df = pd.get_dummies(data_df, prefix="OHE", prefix_sep='_', columns=feature_dict.get("categorical_col"))
265/60:
# make dataframe of feature columns
feature_col_list = []
for value_list in feature_dict.values():
    feature_col_list.extend(value_list)
    
data_df = us_df[feature_col_list]
label_df = us_df["over_1M"]
265/61:
# review data types of feature_df
for label, content in data_df.items():
    print(f"{label} is type: {data_df[label].dtype}")
265/62:
# create one-hot-encodings
feature_df = pd.get_dummies(data_df, prefix="OHE", prefix_sep='_', columns=feature_dict.get("categorical_col"))
265/63:
print(f"feature_df shape: {feature_df.shape}")
print(f"data_df shape: {data_df.shape}")
265/64:
def train_dev_test_split(feature_df, label_df, dev_size, test_size):
    """
    splits the data into train, dev, and test sets
    """
    
    X_train_tmp, X_test, y_train_tmp, y_test = train_test_split(feature_df, label_df, test_size=test_size)
    
    true_dev_size = dev_size /(1-test_size)
    
    X_train, X_dev, y_train, y_dev = train_test_split(X_train_tmp, y_train_tmp, test_size=true_dev_size)
    
    return X_train, X_dev, X_test, y_train, y_dev, y_test
265/65:
test_size = 0.1
dev_size = 0.1

X_train, X_dev, X_test, y_train, y_dev, y_test = train_dev_test_split(feature_df, label_df, dev_size=dev_size, test_size=test_size)



print(f"{X_train.shape}. {X_dev.shape}. {X_test.shape}. {y_train.shape}. {y_dev.shape}. {y_test.shape}.")
265/66: # feed trimmed and processed dataframe into regression
265/67: clf = LogisticRegression(penalty='l2', random_state=0).fit(X_train, y_train)
265/68: type(clf)
265/69:
# interpret results
clf.score(X_dev, y_dev)
265/70:
print(f"feature_df shape: {feature_df.shape}")
print(f"data_df shape: {data_df.shape}")
feature_df.profile_report()
265/71:
print(f"feature_df shape: {feature_df.shape}")
print(f"data_df shape: {data_df.shape}")
265/72:
# interpret results

#Returns the mean accuracy on the given test data and labels.
clf.score(X_dev, y_dev)
dev_pred = clf.predict(X_dev)
print(dev_pred.iloc[:10])
265/73:
# interpret results

#Returns the mean accuracy on the given test data and labels.
clf.score(X_dev, y_dev)
dev_pred = clf.predict(X_dev)
print(dev_pred[:10])
265/74:
# interpret results

#Returns the mean accuracy on the given test data and labels.
clf.score(X_dev, y_dev)
dev_pred = clf.predict(X_dev)
print(dev_pred[:100])
265/75:
import pandas as pd
import pandas_profiling
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import roc_auc_score
265/76: print(y_dev[:10])
265/77:

dev_probs = clf.predict_proba(y_dev)
print(f"dev_probs: {dev_probs}")
roc_auc_score(dev_probs, y)
265/78:
dev_probs = clf.predict_proba(X_dev)
print(f"dev_probs: {dev_probs}")
roc_auc_score(dev_probs, y)
265/79:
dev_probs = clf.predict_proba(X_dev)
print(f"dev_probs: {dev_probs}")
roc_auc_score(dev_probs, y_dev)
265/80:
dev_probs = clf.predict_proba(X_dev)
print(f"dev_probs: {dev_probs.shape}")
roc_auc_score(dev_probs, y_dev)
265/81:
dev_probs = clf.predict_proba(X_dev)
print(f"dev_probs: {dev_probs.shape}")
raise NotImplementedError
roc_auc_score(dev_probs, y_dev)
265/82:
dev_probs = clf.predict_proba(X_dev)
print(f"dev_probs: {dev_probs.shape}")
raise NotImplementedError("dev_probs shape is wrong:\n 
                          https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_auc_score.html")
roc_auc_score(dev_probs, y_dev)
265/83:
dev_probs = clf.predict_proba(X_dev)
print(f"dev_probs: {dev_probs.shape}")
raise NotImplementedError("dev_probs shape is wrong:
                          https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_auc_score.html")
roc_auc_score(dev_probs, y_dev)
265/84:
dev_probs = clf.predict_proba(X_dev)
print(f"dev_probs: {dev_probs.shape}")
raise NotImplementedError("dev_probs shape is wrong: https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_auc_score.html")
roc_auc_score(dev_probs, y_dev)
266/1: l = ['zh', 'ah', 'hh', 'k']
266/2: l.split()
267/1: import Levenshtein as Lev
267/2: import Levenshtein as Lev
267/3: import python-levenshtein as lev
267/4: import levenshtein as lev
267/5: import Levenshtein as lev
267/6: from Levenshtein import *
268/1: import Levenshtein as Lev
268/2: Ref =['dh', 'ah', 'hh', 'ow', 'l', 'th', 'ih', 'ng', 'w', 'aa', 'z', 'ah', 't', 'r', 'ay', 'f', 'ah', 'l', 'aa', 'd']
268/3: Hyp=['dh', 'ah', 'v', 'ow', 'th', 'ih', 'ng', 'w', 'aa', 'z', 'ih', 't', 'ay', 'f', 'ah', 'l', 'aa', 'n']
268/4: Lev.distance(Ref,Hyp)
268/5: Lev.distance("".join(Ref),"".join(Hyp))
268/6: len(Ref)
268/7: 6/20
268/8: len("".join(Ref))
268/9: 6/32
268/10: import editdistance
268/11: editdistance.eval(Ref, Hyp)
268/12: editdistance.eval("".join(Ref),"".join(Hyp))
268/13: b = set(Ref+Hyp)
268/14: b
268/15: word2char = dict(zip(b, range(len(b))))
268/16: word2char
268/17: ref1 = [chr(word2char[w]) for w in Ref]
268/18: hyp1 = [chr(word2char[w]) for w in Hyp]
268/19: Lev.distance(''.join(ref1), ''.join(hyp1))
268/20: editdistance.eval(Ref, Hyp)
268/21: len(Ref)
268/22:
def LevED(Ref,Hyp):
    b=set(Ref+Hyp)
    word2char = dict(zip(b, range(len(b))))
    w1 = [chr(word2char[w]) for w in Ref]
    w2 = [chr(word2char[w]) for w in Hyp]
    return Lev.distance(''.join(w1), ''.join(w2))
268/23: Ref: ['hh', 'w', 'ay', 'd', 'ow', 'n', 'y', 'uw', 'm', 'uw', 'v', 'dh', 'ah', 'p', 'ow', 'n', 'iy'] ['w', 'ay', 't', 'ow', 'n', 'ih', 'm', 'uw', 'v', 'dh', 'ah', 'p', 'ow', 'n', 'iy']
268/24: Ref=['hh', 'w', 'ay', 'd', 'ow', 'n', 'y', 'uw', 'm', 'uw', 'v', 'dh', 'ah', 'p', 'ow', 'n', 'iy']
268/25: Hyp=['w', 'ay', 't', 'ow', 'n', 'ih', 'm', 'uw', 'v', 'dh', 'ah', 'p', 'ow', 'n', 'iy']
268/26: LevED(Ref,Hyp)
268/27: len(Ref)
268/28: editdistance.eval(Ref, Hyp)
268/29: d1 =['f', 'ao', 'r', 'ae', 'm', 'b', 'r', 'ao', 'sh', 'm', 'ay', 'm', 'aa', 'm', 'ah', 'k', 'ah', 'm', 'hh', 'iy', 'r']
268/30: d2=['f', 'ao', 'r', 'ae', 'm', 'b', 'r', 'er', 'ah', 'sh', 'm', 'ay', 'm', 'aa', 'n', 'ah', 'k', 'ah', 'm', 'hh', 'iy', 'r']
268/31: LevED(d1,d2)
268/32: len(d1)
268/33: d1 = ['m', 'ao', 'r', 'n', 'ih', 'ng', 'p', 'r', 'eh', 'r', 'z', 'w', 'er', 'l', 'ao', 'ng', 'g', 'er', 'dh', 'ae', 'n', 'y', 'uw', 'zh', 'ah', 'w', 'ah', 'l']
268/34: d2 = ['m', 'ao', 'r', 'n', 'ih', 'ng', 'p', 'r', 'eh', 'r', 'z', 'w', 'er', 'l', 'ao', 'ng', 'g', 'er', 'dh', 'ae', 'n', 'y', 'uw', 'zh', 'ah', 'w', 'ah', 'l']
268/35: len(d1)
268/36: LevED(d1,d2)
268/37: d2 = ['m', 'ao', 'eh', 'n', 'ih', 'ng', 'p', 'r', 'eh', 'r', 'z', 'w', 'er', 'l', 'ao', 'ng', 'd', 'er', 'n', 'dh', 'ih', 'n', 'y', 'uw', 'zh', 'ah', 'ah', 'l']
268/38: LevED(d1,d2)
268/39: len(d1)
268/40: s = ''
268/41: if s: print("hello")
268/42: s
268/43: m='m'
268/44: if m: print(4)
268/45: if s: print(4)
269/1:
dev_probs = clf.predict_proba(X_dev)
print(f"dev_probs: {dev_probs.shape}")
roc_auc_score(dev_probs, y_dev)
269/2:
import pandas as pd
import pandas_profiling
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import roc_auc_score
269/3:
import pandas as pd
import pandas_profiling
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import roc_auc_score
269/4:
# data from https://www.kaggle.com/datasnaek/youtube-new
US_DATA_PATH:str = "/Users/dustin/CS/job_projects/prep/triplebyte/data/USvideos.csv"
269/5: us_df = pd.read_csv(US_DATA_PATH)
269/6: us_df.profile_report()
269/7: us_df['over_1M']=us_df.apply(binary_threshold, axis=1)
269/8: # process columns to be fed into log_reg like one hot representations
269/9:
#label column
label_col = ["over_1M"]

#feature columns
all_cols_dict = {
"categorical_cols": ["video_id", "channel_title", "category_id", "tags"],
"numeric_cols": ["likes", "dislikes", "comment_count"],
"text_cols": ["title", "thumbnail_link", "description"],
"datetime_cols": ["trending_date","publish_time"],
"boolean_cols": ["comments_disabled", "ratings_disabled", "video_error_or_removed"]
}

feature_dict = {
"categorical_col": ["video_id", "channel_title", "category_id", "tags"],
"numeric_col": ["likes", "dislikes", "comment_count"]
}
269/10:
# make dataframe of feature columns
feature_col_list = []
for value_list in feature_dict.values():
    feature_col_list.extend(value_list)
    
data_df = us_df[feature_col_list]
label_df = us_df["over_1M"]
269/11:
# review data types of feature_df
for label, content in data_df.items():
    print(f"{label} is type: {data_df[label].dtype}")
269/12:
# create one-hot-encodings
feature_df = pd.get_dummies(data_df, prefix="OHE", prefix_sep='_', columns=feature_dict.get("categorical_col"))
269/13:
print(f"feature_df shape: {feature_df.shape}")
print(f"data_df shape: {data_df.shape}")
269/14:
def train_dev_test_split(feature_df, label_df, dev_size, test_size):
    """
    splits the data into train, dev, and test sets
    """
    
    X_train_tmp, X_test, y_train_tmp, y_test = train_test_split(feature_df, label_df, test_size=test_size)
    
    true_dev_size = dev_size /(1-test_size)
    
    X_train, X_dev, y_train, y_dev = train_test_split(X_train_tmp, y_train_tmp, test_size=true_dev_size)
    
    return X_train, X_dev, X_test, y_train, y_dev, y_test
269/15:
test_size = 0.1
dev_size = 0.1

X_train, X_dev, X_test, y_train, y_dev, y_test = train_dev_test_split(feature_df, label_df, dev_size=dev_size, test_size=test_size)



print(f"{X_train.shape}. {X_dev.shape}. {X_test.shape}. {y_train.shape}. {y_dev.shape}. {y_test.shape}.")
269/16:
def binary_threshold(df):
    threshold = 1000000
    column_name = 'views'
    if df[column_name]>threshold:
        return 1
    else: 
        return 0
269/17: us_df['over_1M']=us_df.apply(binary_threshold, axis=1)
269/18: # process columns to be fed into log_reg like one hot representations
269/19:
#label column
label_col = ["over_1M"]

#feature columns
all_cols_dict = {
"categorical_cols": ["video_id", "channel_title", "category_id", "tags"],
"numeric_cols": ["likes", "dislikes", "comment_count"],
"text_cols": ["title", "thumbnail_link", "description"],
"datetime_cols": ["trending_date","publish_time"],
"boolean_cols": ["comments_disabled", "ratings_disabled", "video_error_or_removed"]
}

feature_dict = {
"categorical_col": ["video_id", "channel_title", "category_id", "tags"],
"numeric_col": ["likes", "dislikes", "comment_count"]
}
269/20:
# make dataframe of feature columns
feature_col_list = []
for value_list in feature_dict.values():
    feature_col_list.extend(value_list)
    
data_df = us_df[feature_col_list]
label_df = us_df["over_1M"]
269/21:
# review data types of feature_df
for label, content in data_df.items():
    print(f"{label} is type: {data_df[label].dtype}")
269/22:
# create one-hot-encodings
feature_df = pd.get_dummies(data_df, prefix="OHE", prefix_sep='_', columns=feature_dict.get("categorical_col"))
269/23:
print(f"feature_df shape: {feature_df.shape}")
print(f"data_df shape: {data_df.shape}")
269/24:
def train_dev_test_split(feature_df, label_df, dev_size, test_size):
    """
    splits the data into train, dev, and test sets
    """
    
    X_train_tmp, X_test, y_train_tmp, y_test = train_test_split(feature_df, label_df, test_size=test_size)
    
    true_dev_size = dev_size /(1-test_size)
    
    X_train, X_dev, y_train, y_dev = train_test_split(X_train_tmp, y_train_tmp, test_size=true_dev_size)
    
    return X_train, X_dev, X_test, y_train, y_dev, y_test
269/25:
test_size = 0.1
dev_size = 0.1

X_train, X_dev, X_test, y_train, y_dev, y_test = train_dev_test_split(feature_df, label_df, dev_size=dev_size, test_size=test_size)



print(f"{X_train.shape}. {X_dev.shape}. {X_test.shape}. {y_train.shape}. {y_dev.shape}. {y_test.shape}.")
270/1:
import pandas as pd
import pandas_profiling
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import roc_auc_score
270/2:
# data from https://www.kaggle.com/datasnaek/youtube-new
US_DATA_PATH:str = "/Users/dustin/CS/job_projects/prep/triplebyte/data/USvideos.csv"
270/3: us_df = pd.read_csv(US_DATA_PATH)
270/4: us_df.profile_report()
270/5: # make binary variable like greather than 1 million views
270/6:
def binary_threshold(df):
    threshold = 1000000
    column_name = 'views'
    if df[column_name]>threshold:
        return 1
    else: 
        return 0
270/7: us_df['over_1M']=us_df.apply(binary_threshold, axis=1)
270/8: # process columns to be fed into log_reg like one hot representations
270/9:
#label column
label_col = ["over_1M"]

#feature columns
all_cols_dict = {
"categorical_cols": ["video_id", "channel_title", "category_id", "tags"],
"numeric_cols": ["likes", "dislikes", "comment_count"],
"text_cols": ["title", "thumbnail_link", "description"],
"datetime_cols": ["trending_date","publish_time"],
"boolean_cols": ["comments_disabled", "ratings_disabled", "video_error_or_removed"]
}

feature_dict = {
"categorical_col": ["video_id", "channel_title", "category_id", "tags"],
"numeric_col": ["likes", "dislikes", "comment_count"]
}
270/10:
# make dataframe of feature columns
feature_col_list = []
for value_list in feature_dict.values():
    feature_col_list.extend(value_list)
    
data_df = us_df[feature_col_list]
label_df = us_df["over_1M"]
270/11:
# review data types of feature_df
for label, content in data_df.items():
    print(f"{label} is type: {data_df[label].dtype}")
270/12:
# create one-hot-encodings
feature_df = pd.get_dummies(data_df, prefix="OHE", prefix_sep='_', columns=feature_dict.get("categorical_col"))
270/13:
print(f"feature_df shape: {feature_df.shape}")
print(f"data_df shape: {data_df.shape}")
270/14:
def train_dev_test_split(feature_df, label_df, dev_size, test_size):
    """
    splits the data into train, dev, and test sets
    """
    
    X_train_tmp, X_test, y_train_tmp, y_test = train_test_split(feature_df, label_df, test_size=test_size)
    
    true_dev_size = dev_size /(1-test_size)
    
    X_train, X_dev, y_train, y_dev = train_test_split(X_train_tmp, y_train_tmp, test_size=true_dev_size)
    
    return X_train, X_dev, X_test, y_train, y_dev, y_test
270/15:
test_size = 0.1
dev_size = 0.1

X_train, X_dev, X_test, y_train, y_dev, y_test = train_dev_test_split(feature_df, label_df, dev_size=dev_size, test_size=test_size)



print(f"{X_train.shape}. {X_dev.shape}. {X_test.shape}. {y_train.shape}. {y_dev.shape}. {y_test.shape}.")
270/16: # feed trimmed and processed dataframe into regression
270/17: clf = LogisticRegression(penalty='l2', random_state=0).fit(X_train, y_train)
270/18:
# interpret results

#Returns the mean accuracy on the given test data and labels.
clf.score(X_dev, y_dev)
dev_pred = clf.predict(X_dev)
print(dev_pred[:100])
270/19: print(y_dev[:10])
270/20:
dev_probs = clf.predict_proba(X_dev)
print(f"dev_probs: {dev_probs.shape}")
roc_auc_score(dev_probs, y_dev)
270/21:
# interpret results

#Returns the mean accuracy on the given test data and labels.
print(clf.score(X_dev, y_dev))
dev_pred = clf.predict(X_dev)
print(dev_pred[:100])
270/22:
# interpret results

#Returns the mean accuracy on the given test data and labels.
print(f"accuracy score: {clf.score(X_dev, y_dev)}")
dev_pred = clf.predict(X_dev)
print(dev_pred[:100])
270/23:
dev_probs = clf.predict_proba(X_dev)
print(f"dev_probs: {dev_probs.shape}")
roc_auc_score(dev_probs, y_dev)
270/24:
dev_probs = clf.predict_proba(X_dev)
print(f"dev_probs: {dev_probs.shape}")
270/25:
dev_probs = clf.predict_proba(X_dev)
print(f"dev_probs: {dev_probs[:10, :]}")
270/26:
dev_probs = clf.predict_proba(X_dev)
print(f"dev_probs: {dev_probs[:10, :]}")
print(clf.classes_)
270/27:
dev_probs = clf.predict_proba(X_dev)
print(f"dev_probs: {dev_probs[:10, :-1]}")
print(clf.classes_)
270/28:
dev_probs = clf.predict_proba(X_dev)
print(f"dev_probs: {dev_probs[:10, :-1]}")
print(f"dev_probs: {dev_probs[:10, :]}")
print(clf.classes_)
270/29:
dev_probs = clf.predict_proba(X_dev)
print(f"dev_probs: {dev_probs[:10, 1]}")
print(f"dev_probs: {dev_probs[:10, :]}")
print(clf.classes_)
270/30: roc_auc_score(dev_probs[:,1], y_dev)
270/31:
dev_probs = clf.predict_proba(X_dev)
print(f"dev_probs: {dev_probs[:10, 1]}")
print(f"dev_probs: {dev_probs[:10, :]}")
print(clf.classes_)
print(f"y_dev: {y_dev})
270/32:
dev_probs = clf.predict_proba(X_dev)
print(f"dev_probs: {dev_probs[:10, 1]}")
print(f"dev_probs: {dev_probs[:10, :]}")
print(clf.classes_)
print(f"y_dev: {y_dev.shape}"")
270/33:
dev_probs = clf.predict_proba(X_dev)
print(f"dev_probs: {dev_probs[:10, 1]}")
print(f"dev_probs: {dev_probs[:10, :]}")
print(clf.classes_)
print(f"y_dev: {y_dev.shape}")
270/34:
dev_probs = clf.predict_proba(X_dev)
print(f"dev_probs: {dev_probs[:10, 1]}")
print(f"dev_probs: {dev_probs.shape}")
print(clf.classes_)
print(f"y_dev: {y_dev.shape}")
270/35: roc_auc_score(dev_probs[:,1].squeeze(), y_dev)
270/36:
dev_probs = clf.predict_proba(X_dev)
print(f"dev_probs: {dev_probs[:10, 1]}")
print(f"dev_probs: {dev_probs[:,1].shape}")
print(clf.classes_)
print(f"y_dev: {y_dev.shape}")
270/37: roc_auc_score(dev_probs[:,1], y_dev)
270/38: tuple = roc_curve(y_test, clf.predict_proba(X_test)[:,1])
270/39:
import pandas as pd
import pandas_profiling
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import roc_auc_score, roc_curve
270/40: tuple = roc_curve(y_test, clf.predict_proba(X_test)[:,1])
270/41: false_positive_rate, true_positive_rate, thresholds = roc_curve(y_test, clf.predict_proba(X_test)[:,1])
270/42: auc(false_positive_rate, true_positive_rate)
270/43:
import pandas as pd
import pandas_profiling
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import roc_auc_score, roc_curve, auc
270/44: auc(false_positive_rate, true_positive_rate)
270/45: roc_auc_score(clf.predict(X_dev), y_dev)
270/46: roc_auc_score(y_dev, clf.predict_proba(X_dev))
270/47: roc_auc_score(y_dev, clf.predict_proba(X_dev)[:,1])
270/48: false_positive_rate, true_positive_rate, thresholds = roc_curve(y_test, clf.predict_proba(X_test)[:,1])
270/49: auc(false_positive_rate, true_positive_rate)
270/50: roc_auc_score(y_test, clf.predict_proba(X_test)[:,1])
270/51: plot_precision_recall_curve(clf, X_dev, y_dev)
270/52:
import pandas as pd
import pandas_profiling
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import roc_auc_score, roc_curve, auc, plot_precision_recall_curve
270/53:
import pandas as pd
import pandas_profiling
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import roc_auc_score, roc_curve, auc, plot_precision_recall_curve, plot_roc_curve
270/54: plot_precision_recall_curve(clf, X_dev, y_dev)
270/55: plot_roc_curve(clf, X_dev, y_dev)
270/56:
import pandas as pd
import pandas_profiling
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import roc_auc_score, roc_curve, auc, plot_precision_recall_curve, plot_roc_curve, plot_confusion_matrix
270/57:
import pandas as pd
import pandas_profiling
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import roc_auc_score, roc_curve, auc, plot_precision_recall_curve, plot_roc_curve, plot_confusion_matrix
270/58: plot_confusion_matrix(clf, X_dev, y_dev)
268/46: path = "1-drz-test-20191202.wv"
268/47: audio_ext = [".wav", ".wv"]
268/48: list(filter(lambda x: x in path, audio_ext))
268/49: path1="hell.wav"
268/50: list(filter(lambda x: x in path1, audio_ext))
271/1: #us_df.profile_report()
272/1: import pandas_profiling
272/2: import requests
272/3: import editdistance
272/4: import editdistance
272/5: import editdistance
272/6: import pandas_profiling
272/7:
dataset_path = "/Users/dustin/Downloads/sleep.csv"
sleep_dataset = pd.read_csv(dataset_path)
272/8: import pandas as pd
272/9:
dataset_path = "/Users/dustin/Downloads/sleep.csv"
sleep_dataset = pd.read_csv(dataset_path)
272/10: print(sleep_dataset)
272/11: print(type(sleep_dataset["sleep"]))
272/12: print(type(sleep_dataset["sleep"][0]))
272/13: print(type(sleep_dataset["sleep"].iloc[0]))
272/14: def calc_sleep_hours(sleep_start_sr, sleep_end_sr):
272/15:
sleep_start = "2016-01-01 05:28:00+00:00"
sleep_end = "2016-01-01 12:02:00+00:00"
print(sleep_start[12])
272/16:
sleep_start = "2016-01-01 05:28:00+00:00"
sleep_end = "2016-01-01 12:02:00+00:00"
print(sleep_start[11])
272/17:
sleep_start = "2016-01-01 05:28:00+00:00"
sleep_end = "2016-01-01 12:02:00+00:00"
print(sleep_start[11:19])
272/18:
import datetime

import pandas as pd
272/19: datetime.strptime(sleep_start)
272/20:
from datetime import datetime
import pandas as pd
272/21: datetime.strptime(sleep_start)
272/22: date = datetime.strptime("2016-01-01 05:28:00+00:00", "%y/%m/%d %H:%M:%S")
272/23: date = datetime.strptime("2016-01-01 05:28:00+00:00", "%Y-%M-%D %H:%M:%S")
272/24: date = datetime.strptime("2016-01-01 05:28:00+00:00", "%Y-%M-%d %H:%M:%S")
272/25: date = datetime.strptime("2016-01-01 05:28:00+00:00", "%Y-%MM-%d %H:%M:%S")
272/26: date = datetime.strptime("2016-01-01 05:28:00+00:00", "%Y-%m-%d %H:%M:%S")
272/27: date = datetime.strptime("2016-01-01 05:28:00", "%Y-%m-%d %H:%M:%S")
272/28:
date = datetime.strptime("2016-01-01 05:28:00", "%Y-%m-%d %H:%M:%S")
date1 = datetime.strptime("2016-01-01 12:02:00", "%Y-%m-%d %H:%M:%S")
272/29: date1-date
272/30: [i for i in sleep_dataset["sleep"].]
272/31: [i for i in sleep_dataset["sleep"]
272/32: [i for i in sleep_dataset["sleep"]]
272/33: len(sleep_dataset["sleep"])
272/34: len("2016-01-01 05:28:00")
272/35:
date = datetime.strptime("2016-01-01 05:28:00", "%Y-%m-%d %H:%M:%S")
date1 = datetime.strptime("2016-01-01 12:02:00", "%Y-%m-%d %H:%M:%S")

date2 = datetime.strptime(sleep_start_sr.iloc[0][:19], "%Y-%m-%d %H:%M:%S")
272/36:
date = datetime.strptime("2016-01-01 05:28:00", "%Y-%m-%d %H:%M:%S")
date1 = datetime.strptime("2016-01-01 12:02:00", "%Y-%m-%d %H:%M:%S")

date2 = datetime.strptime(sleep_dataset["sleep_start"].iloc[0][:19], "%Y-%m-%d %H:%M:%S")
272/37:
date = datetime.strptime("2016-01-01 05:28:00", "%Y-%m-%d %H:%M:%S")
date1 = datetime.strptime("2016-01-01 12:02:00", "%Y-%m-%d %H:%M:%S")

date2 = datetime.strptime(sleep_dataset["sleep"].iloc[0][:19], "%Y-%m-%d %H:%M:%S")
272/38:
date = datetime.strptime("2016-01-01 05:28:00", "%Y-%m-%d %H:%M:%S")
date1 = datetime.strptime("2016-01-01 12:02:00", "%Y-%m-%d %H:%M:%S")

date2 = datetime.strptime(sleep_dataset["sleep"].iloc[0][:19], "%Y-%m-%d %H:%M:%S")
print(date2)
272/39: (date1-date).seconds
272/40: (date1-date).seconds/3600
272/41: sleep_dataset["sleep_hours"]
272/42: sleep_dataset["sleep_hours"] = 0
272/43: len(sleep_dataset["sleep_hours"])
272/44: sleep_dataset = calc_sleep_hours(sleep_dataset)
272/45:
def calc_sleep_hours(sleep_dataset):
    sleep_dataset["sleep_hours"] = 0*
    
    assert len(sleep_start_sr)==len(sleep_end_sr), "series are not same length"
    
    for i range(len(sleep_start_sr)):
        start_time = datetime.strptime(sleep_dataset["sleep"].iloc[i][:19], "%Y-%m-%d %H:%M:%S")
        end_time = datetime.strptime(sleep_dataset["wake"].iloc[i][:19], "%Y-%m-%d %H:%M:%S")
        
        sleep_hour = end_time - start_time
        sleep_hour = sleep_hour.seconds/3600
        sleep_dataset["sleep_hours"].iloc[i] = sleep_hour
        
    return sleep_dataset
272/46: sleep_dataset = calc_sleep_hours(sleep_dataset)
272/47:
def calc_sleep_hours(sleep_dataset):
    sleep_dataset["sleep_hours"] = 0
    
    assert len(sleep_start_sr)==len(sleep_end_sr), "series are not same length"
    
    for i range(len(sleep_start_sr)):
        start_time = datetime.strptime(sleep_dataset["sleep"].iloc[i][:19], "%Y-%m-%d %H:%M:%S")
        end_time = datetime.strptime(sleep_dataset["wake"].iloc[i][:19], "%Y-%m-%d %H:%M:%S")
        
        sleep_hour = end_time - start_time
        sleep_hour = sleep_hour.seconds/3600
        sleep_dataset["sleep_hours"].iloc[i] = sleep_hour
        
    return sleep_dataset
272/48: sleep_dataset = calc_sleep_hours(sleep_dataset)
272/49:
def calc_sleep_hours(sleep_dataset):
    sleep_dataset["sleep_hours"] = 0
    
    assert len(sleep_start_sr)==len(sleep_end_sr), "series are not same length"
    
    for i in range(len(sleep_start_sr)):
        start_time = datetime.strptime(sleep_dataset["sleep"].iloc[i][:19], "%Y-%m-%d %H:%M:%S")
        end_time = datetime.strptime(sleep_dataset["wake"].iloc[i][:19], "%Y-%m-%d %H:%M:%S")
        
        sleep_hour = end_time - start_time
        sleep_hour = sleep_hour.seconds/3600
        sleep_dataset["sleep_hours"].iloc[i] = sleep_hour
        
    return sleep_dataset
272/50: sleep_dataset = calc_sleep_hours(sleep_dataset)
272/51:
def calc_sleep_hours(sleep_dataset):
    sleep_dataset["sleep_hours"] = 0
    
    
    for i in range(len(sleep_start_sr)):
        start_time = datetime.strptime(sleep_dataset["sleep"].iloc[i][:19], "%Y-%m-%d %H:%M:%S")
        end_time = datetime.strptime(sleep_dataset["wake"].iloc[i][:19], "%Y-%m-%d %H:%M:%S")
        
        sleep_hour = end_time - start_time
        sleep_hour = sleep_hour.seconds/3600
        sleep_dataset["sleep_hours"].iloc[i] = sleep_hour
        
    return sleep_dataset
272/52: sleep_dataset = calc_sleep_hours(sleep_dataset)
272/53:
def calc_sleep_hours(sleep_dataset):
    sleep_dataset["sleep_hours"] = 0
    
    
    for i in range(len(sleep_dataset["sleep"])):
        start_time = datetime.strptime(sleep_dataset["sleep"].iloc[i][:19], "%Y-%m-%d %H:%M:%S")
        end_time = datetime.strptime(sleep_dataset["wake"].iloc[i][:19], "%Y-%m-%d %H:%M:%S")
        
        sleep_hour = end_time - start_time
        sleep_hour = sleep_hour.seconds/3600
        sleep_dataset["sleep_hours"].iloc[i] = sleep_hour
        
    return sleep_dataset
272/54: sleep_dataset = calc_sleep_hours(sleep_dataset)
273/1: import numpy as np
273/2:
import numpy as np
import pandas as pd
273/3: data_path = "/Users/dustin/Downloads/trees.csv"
273/4: dataset = pd.read_csv(data_path)
273/5: dataset
273/6: x_train =dataset[x0:x3]
273/7: dataset = dataset.to_numpy()
273/8: type(dataset)
273/9: X_train = dataset[0:3][:500]
273/10: X_train.shape
273/11: X_train = dataset[:500][0:3]
273/12: X_train.shape
273/13: dataset = dataset.to_numpy()
273/14: dataset = pd.read_csv(data_path)
273/15: dataset
273/16: dataset = dataset.to_numpy()
273/17: type(dataset
273/18: type(dataset)
273/19: X_train = dataset[:500][0:3]
273/20: X_train.shape
273/21: dataset = pd.read_csv(data_path)
273/22: dataset
273/23: dataset = dataset.to_numpy()
273/24: type(dataset)
273/25: dataset.shape
273/26: X_train = dataset[:500][:5]
273/27: dataset.shape
273/28: X_train.shape
273/29: X_train = dataset[:][:5]
273/30: X_train.shape
273/31: dataset = dataset.to_numpy()
273/32: dataset = pd.read_csv(data_path)
273/33: dataset
273/34: dataset = dataset.to_numpy()
273/35: X_train = dataset[:][:5]
273/36: X_train.shape
273/37: dataset[:500].shape
273/38: dataset[:500][:5].shape
273/39: dataset[:500][:4].shape
273/40: dataset[:500][:5].shape
273/41: X_data = dataset[:4]
273/42: X_train.shape
273/43: dataset = pd.read_csv(data_path)
273/44: dataset
273/45: y_train = dataset['y'].iloc[:500]
273/46: y_train
273/47: y_train.shape
273/48:
y_train = dataset['y'].iloc[:500]
y_test = dataset['y'].iloc[500:]
273/49: y_train.shape
273/50:
y_train.shape
y_test.shape
273/51:
train_dataset = dataset.ioc[:500]
test_dataset = dataset.iloc[500:]
273/52:
train_dataset = dataset[:500]
test_dataset = dataset[500:]
273/53: train_dataset.shape
273/54:
class Leaf():
    
    def __init__(self, label):
        
        self.label=label
273/55:
class Node():
    def __init__(self, column, split_value, left_subtree, right_subtree):
        self.column = column
        self.split_value = split_value
        self.left_subtree = left_subtree
        self.right_subtree = right_subtree
273/56:
n = Leaf(1)
type(n)
273/57:
n = Leaf(1)
type(n)=="__main__.Leaf"
273/58:
n = Leaf(1)
type(n)==__main__.Leaf
273/59:
n = Leaf(1)
isInstance(n, Leaf)
273/60:
n = Leaf(1)
isinstance(n, Leaf)
273/61: dataset[:4, :].shape
273/62: dataset.iloc[:4, :].shape
273/63: dataset.iloc[:, :$].shape
273/64: dataset.iloc[:, :4].shape
273/65: dataset.iloc[:, :].shape
273/66:
X_train = dataset.iloc[:500, :4]
X_test = dataset.iloc[500:, :4]
y_train = dataset['y'].iloc[:500]
y_test = dataset['y'].iloc[500:]
273/67: X_train.shape
273/68: X_train[1,:]
273/69: X_train.iloc[1,:]
273/70: X_train.iloc[1,:]["x1"]
273/71:
def predict(root, features):
    
    if isinstance(root, Leaf): 
        return root.label
    else: 
        if features.iloc[root.column] <= root.split_value:
            predict(root.left_subtree, features)
        else:
            predict(root.right_subtree, features)
273/72: tree = Node(0, 5.6, Leaf(0), Node(2, 5.0, Leaf(1), Leaf(2)))
273/73: predict(tree, features.iloc[:,1])
273/74: predict(tree, X_train.iloc[:,1])
273/75: print(predict(tree, X_train.iloc[:,1]))
273/76:
def predict(root, features):
    
    if isinstance(root, Leaf): 
        return root.label
    else: 
        if features.iloc[root.column] <= root.split_value:
            return predict(root.left_subtree, features)
        else:
            return predict(root.right_subtree, features)
273/77: print(predict(tree, X_train.iloc[:,1]))
273/78: print(predict(tree, X_train.iloc[:,0]))
273/79: print(predict(tree, X_train.iloc[:,1]))
273/80: print(predict(tree, X_train.iloc[:,1]))
268/51: import torch
268/52: t1 = torch.arange(100)
268/53: t2 = torch.arange(100, reversed=True)
268/54: help(range)
268/55: list(range(10, -1))
268/56: help(range)
268/57: list(range(0,10, -1))
268/58: list(range(10))
268/59: list(range(0,10))
268/60: list(range(0,10,2))
268/61: list(range(0,10,-1))
268/62: list(range(0,100))
268/63: l = list(range(0,10))
268/64: l.reverse()
268/65: l
268/66: list(range(10,-1,-1))
268/67: list(range(10,-1,1))
268/68: list(range(10,-1,-2))
268/69: t2 = torch.tensor(range(100,0,-1))
268/70: t2[:10]
268/71: t1[:10]
268/72: t3 = torch.tensor(t2 - t1)
268/73: t[:10]
268/74: t3[:10]
268/75: t4 = t2 - t1
268/76: t4 == t3
268/77: torch.sum(t4==t3)
268/78: torch.sum([1,2,3,4])
268/79: torch.sum(torch.tensor([1,2,3,4]))
268/80: t4.dtype
268/81: t5 = (t2 - t1).to(torch.float64)
268/82: t5
268/83: t4
268/84: t4.dtype
268/85: t5.dtype
268/86: torch.sum(t4==t5)
268/87: np
268/88: import numpy as np
268/89: np
268/90: arr = np.zeros((257,40))
268/91: save_path = '~/Downloads/'
268/92: np.save(save_path, arr)
268/93: save_path = '~/Downloads/np_test'
268/94: np.save(save_path, arr)
268/95: save_path = '/Users/dustin/Downloads/np_test'
268/96: np.save(save_path, arr)
268/97: arr_list = []
268/98: arr_list.append(arr)
268/99: arr_list.append(arr)
268/100: arr_list.append(arr)
268/101: arr_list.len()
268/102: len(arr_list)
268/103: np.save(save_path, arr_list)
268/104: vocab
268/105: import json
268/106: label_path="/Users/dustin/CS/consulting/firstlayerai/phoneme_classification/src/awni_speech/naren/deepspeech.pytorch/labels.json"
268/107:
with open(label_path, 'r') as fid:
    labels = json.load(fid)
268/108: labels
268/109: len(labels)
268/110: preproc
268/111: import speech
268/112: ls
268/113: cd ..
268/114: ls
268/115: cd ..
268/116: cd speech/
268/117: import speech
268/118: preproc_path="/Users/dustin/CS/consulting/firstlayerai/phoneme_classification/src/awni_speech/speech/examples/librispeech/models/ctc_models/20200314"
268/119: model, preproc = speech.load(preproc_path)
274/1: import speech
274/2: preproc_path = "/Users/dustin/CS/consulting/firstlayerai/phoneme_classification/src/awni_speech/speech/examples/librispeech/models/ctc_models/20200314"
274/3: model, preproc = speech.load(preproc_path)
275/1: import speech
275/2: preproc_path = "/Users/dustin/CS/consulting/firstlayerai/phoneme_classification/src/awni_speech/speech/examples/librispeech/models/ctc_models/20200314"
275/3: model, preproc = speech.load(preproc_path)
275/4: str(eval("preproc."+"preprocessor))
275/5: str(eval("preproc."+"preprocessor"))
275/6: print(preproc)
275/7: val = 26.184940338134766
275/8: f"val: {val}"
275/9: f"val: {val:0.2f}"
275/10: f"val: {val:0.3f}"
275/11: f"val: {val:0.3s}"
275/12: f"val: {val:0.3f}"
275/13: f"val: {val:0.3i}"
275/14: f"val: {val:0.3f}"
275/15: preproc
275/16: print(preproc)
275/17: preproc_path
275/18: preproc0311_path = '/Users/dustin/CS/consulting/firstlayerai/phoneme_classification/src/awni_speech/speech/examples/librispeech/models/ctc_models/20200310/20200311'
275/19: model0311, preproc0311 = speech.load(preproc0311_path)
275/20: print(preproc0311)
275/21: print(model0311)
275/22: import math
275/23: x = float('nan')
275/24: math.isnan(x)
275/25: math.isnan(5)
275/26: math.isnan('nan')
275/27: math.isnan(x)
275/28: print(preproc)
275/29: no_aug_path = "/Users/dustin/CS/consulting/firstlayerai/phoneme_classification/src/awni_speech/speech/examples/librispeech/models/ctc_models/20200312"
275/30: preproc_no_aug = speech.loadd(no_aug_path)
275/31: preproc_no_aug = speech.load(no_aug_path)
275/32: model_no_aug, preproc_no_aug = speech.load(no_aug_path)
275/33: print(preproc_no_aug)
275/34: no_aug_path = "/Users/dustin/CS/consulting/firstlayerai/phoneme_classification/src/awni_speech/speech/examples/librispeech/models/ctc_models/20200310"
275/35: model_no_aug, preproc_no_aug = speech.load(no_aug_path)
275/36: print(preproc_no_aug)
275/37: no_aug_path = "/Users/dustin/CS/consulting/firstlayerai/phoneme_classification/src/awni_speech/speech/examples/librispeech/models/ctc_models/20200305"
275/38: model_no_aug, preproc_no_aug = speech.load(no_aug_path)
275/39: print(preproc_no_aug)
275/40: import importlib
275/41: importlib.reload(speech)
275/42: model_no_aug, preproc_no_aug = speech.load(no_aug_path)
275/43: print(preproc_no_aug)
276/1: import speech
276/2: no_aug_path = "/Users/dustin/CS/consulting/firstlayerai/phoneme_classification/src/awni_speech/speech/examples/librispeech/models/ctc_models/20200305"
276/3: model_no_aug, preproc_no_aug = speech.load(no_aug_path)
276/4: print(preproc_no_aug)
276/5: no_aug_path = "/Users/dustin/CS/consulting/firstlayerai/phoneme_classification/src/awni_speech/speech/examples/librispeech/models/ctc_models/20200314"
276/6: model_no_aug, preproc_no_aug = speech.load(no_aug_path)
276/7: print(preproc_no_aug)
276/8: print(model_no_aug.state_dict())
276/9: print(model_no_aug.state_dict().keys())
276/10: ls
276/11: ls
276/12: cd ..
276/13: ls
276/14: cd naren/deepspeech.pytorch/
276/15: from model import DeepSpeech
276/16: state_dict = model_no_aug.state_dict()
276/17: state_dict
276/18: type(state_dict)
276/19: type(state_dict.keys())
276/20: state_dict.keys()
276/21: new_name = 'conv.seq_module.0.weight'
276/22: state_dict.get('conv.0.weight')
276/23: '' is None
276/24: if '': print(hello)
276/25: import torch
276/26: model = torch.load('/home/dzubke/awni_speech/speech/examples/librispeech/models/ctc_models/20200314/20200323/best_model')
277/1: import torch
278/1: import torch
278/2: t1 = torch.zeros(1,100, 21, 11)
278/3: t1.transpose(2,3).shape
278/4: t1.transpose(-2,-1).shape
278/5: log=True
278/6: if log print("hello")
278/7: print("hello) if log
278/8: print("hello") if log
278/9: print("hello") if log else None
278/10: print("hello") if !log else None
278/11: print("hello") if not log else None
278/12: print("hello") if not log else
278/13: print("hello") if not log else None
278/14: if log: print("hello")
278/15: if not log: print("hello")
278/16: logger=None
278/17: use_log = True if logger is not None else use_log=False
278/18: use_log = (True if logger is not None else False)
278/19: use_log
278/20: logger='a'
278/21: use_log = (True if logger is not None else False)
278/22: use_log
278/23: use_log = (logger is not None)
278/24: use_log
279/1: ls
279/2: import speech
279/3: import pickle
279/4: preproc
283/1: ca_data_path = "~/CS/job_projects/prep/triplebyte/data/california-housing-prices.csv"
283/2:
art_train_path = "~/CS/job_projects/prep/triplebyte/data/house_prices/train.csv"
art_test_path = "~/CS/job_projects/prep/triplebyte/data/house_prices/test.csv"
283/3:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
283/4: ca_data_path = "~/CS/job_projects/prep/triplebyte/data/california-housing-prices.csv"
283/5:
art_train_path = "~/CS/job_projects/prep/triplebyte/data/house_prices/train.csv"
art_test_path = "~/CS/job_projects/prep/triplebyte/data/house_prices/test.csv"
283/6: ca_data = pd.read_csv(ca_data_path)
283/7: ca_data.describe
283/8:
import pandas as pd
import pandas-profiling
import numpy as np
import matplotlib.pyplot as plt
283/9:
import pandas as pd
import pandas_profiling
import numpy as np
import matplotlib.pyplot as plt
283/10: profile = ProfileReport(ca_data, title='Pandas Profiling Report', html={'style':{'full_width':True}})
283/11:
import pandas as pd
from pandas_profiling import ProfileReport
import numpy as np
import matplotlib.pyplot as plt
283/12: profile = ProfileReport(ca_data, title='Pandas Profiling Report', html={'style':{'full_width':True}})
283/13:
import pandas as pd
from pandas_profiling import ProfileReport
import numpy as np
import matplotlib.pyplot as plt
283/14: profile = ProfileReport(ca_data, title='Pandas Profiling Report', html={'style':{'full_width':True}})
283/15:
with open(ca_data_path, newline='') as fid:
    reader = csv.reader(fid, delimiter=' ', quotechar='|')
283/16:
# standard library
import csv
# third-party libraries
import pandas as pd
from pandas_profiling import ProfileReport
import numpy as np
import matplotlib.pyplot as plt
283/17:
with open(ca_data_path, newline='') as fid:
    reader = csv.reader(fid, delimiter=' ', quotechar='|')
283/18:
art_train_path = "/Users/dustin/CS/job_projects/prep/triplebyte/data/house_prices/train.csv"
art_test_path = "/Users/dustin/CS/job_projects/prep/triplebyte/data/house_prices/test.csv"
283/19:
with open(ca_data_path, newline='') as fid:
    reader = csv.reader(fid, delimiter=' ', quotechar='|')
283/20: ca_data_path = "/Users/dustin/CS/job_projects/prep/triplebyte/data/california-housing-prices.csv"
283/21:
with open(ca_data_path, newline='') as fid:
    reader = csv.reader(fid, delimiter=' ', quotechar='|')
283/22: profile.to_notebook_iframe()
283/23: ca_data.profile_report()
283/24: art_train = pd.read_csv(art_train_path)
283/25:
art_train = pd.read_csv(art_train_path)
art_test = pd.read_csv(art_test_path)
283/26: art_train.pandas_profiling()
283/27: art_train.profile_report()
283/28: art_test.profile_report()
284/1:
# standard library
import csv
# third-party libraries
import pandas as pd
from pandas_profiling import ProfileReport
import numpy as np
import matplotlib.pyplot as plt
284/2: ca_data_path = "/Users/dustin/CS/job_projects/prep/triplebyte/data/california-housing-prices.csv"
284/3:
art_train_path = "/Users/dustin/CS/job_projects/prep/triplebyte/data/house_prices/train.csv"
art_test_path = "/Users/dustin/CS/job_projects/prep/triplebyte/data/house_prices/test.csv"
284/4: ca_data = pd.read_csv(ca_data_path)
284/5:
art_train = pd.read_csv(art_train_path)
art_test = pd.read_csv(art_test_path)
284/6: art_train.profile_report()
279/5: l = [1,2,3,4]
279/6: l1=[,3,4,5,6]
279/7: l1=[3,4,5,6]
279/8: set(l+l1)
279/9: set(l,l1)
279/10: d = {'test':1, 'tree': 2,  'bread':3}
279/11: 'test' in d
279/12: 'test' not in d
279/13: 'tst' not in d
279/14: d.update({'twist': 4})
279/15: d
279/16: import speech.utils.data_helpers
279/17: conda info -e
279/18: ls
279/19: cd examples/tests
279/20: from  corpus_compare import combine_lexicons
279/21: lib_lex_path = "~/CS/consulting/firstlayerai/phoneme_classification/src/awni_speech/speech/examples/librispeech/librispeech-lexicon_extended.txt"
279/22: ted_lex_path = "~/CS/consulting/firstlayerai/phoneme_classification/src/awni_speech/speech/examples/tedlium/TEDLIUM.162k.dic"
279/23: lib_dict = data_helpers.lexicon_to_dict(lib_lex_path, "librispeech")
279/24: from speech.utils import data_helpers
279/25: lib_dict = data_helpers.lexicon_to_dict(lib_lex_path, "librispeech")
279/26: ted_lex_path = "/Users/dustin/CS/consulting/firstlayerai/phoneme_classification/src/awni_speech/speech/examples/tedlium/TEDLIUM.162k.dic"
279/27: ted_lex_path = "/Users/dustin/CS/consulting/firstlayerai/phoneme_classification/src/awni_speech/speech/examples/tedlium/TEDLIUM.162k.dic"
279/28: lib_dict = data_helpers.lexicon_to_dict(lib_lex_path, "librispeech")
279/29: lib_lex_path = "/Users/dustin/CS/consulting/firstlayerai/phoneme_classification/src/awni_speech/speech/examples/librispeech/librispeech-lexicon_extended.txt"
279/30: lib_lex_path
279/31: lib_dict = data_helpers.lexicon_to_dict(lib_lex_path, "librispeech")
279/32: ted_dict = data_helpers.lexicon_to_dict(ted_lex_path, "tedlium")
279/33: len(lib_dict)
279/34: len(ted_dict)
279/35: combo_dict = combine_lexicons(lib_dict, ted_dict)
279/36: d.keys()
279/37: type(d.keys() )
279/38: d.keys() + d.keys()
279/39: list(d.keys()) + list(d.keys())
279/40: import importlib
279/41: importlib.reload(combine_lexicons)
279/42: importlib.relod(corpus_compare)
279/43: importlib.reload(corpus_compare)
279/44: import corpus_compare
279/45: combo_dict = corpus_compare.combine_lexicons(lib_dict, ted_dict)
279/46: importlib.reload(corpus_compare)
279/47: combo_dict = corpus_compare.combine_lexicons(lib_dict, ted_dict)
279/48: importlib.reload(corpus_compare)
279/49: combo_dict = corpus_compare.combine_lexicons(lib_dict, ted_dict)
279/50: combo_dict = corpus_compare.combine_lexicons(lib_dict, ted_dict)
279/51: True |  False
279/52: True &  False
279/53: False |  False
279/54: importlib.reload(corpus_compare)
279/55: combo_dict, diff_values = corpus_compare.combine_lexicons(lib_dict, ted_dict)
279/56: importlib.reload(corpus_compare)
279/57: combo_dict, diff_values = corpus_compare.combine_lexicons(lib_dict, ted_dict)
279/58: diff_values.get("sandwich")
279/59: cmudict_path = "/Users/dustin/CS/consulting/firstlayerai/phoneme_classification/src/awni_speech/speech/examples/cmudict-0.7b.txt"
279/60: cmu_dict = data_helpers.lexicon_to_dict(cmudict_path, "cmudict")
279/61: importlib.reload(data_helpers)
279/62: cmu_dict = data_helpers.lexicon_to_dict(cmudict_path, "cmudict")
279/63:
with open(cmudict_path, 'r') as fid:
    lexicon=[l for l in fid]
279/64:
with open(cmudict_path, 'r') as fid:
    lexicon=[l for l in fid]
279/65:
with open(cmudict_path, 'r') as fid:
    lexicon=[l for l in fid]
279/66:
with open(cmudict_path, 'r') as fid:
    lexicon=[l for l in fid]
279/67:
with open(cmudict_path, 'rb') as fid:
    lexicon=[l for l in fid]
279/68: lexicon
279/69:
with open(cmudict_path, 'r') as fid:
    lexicon=[l for l in fid]
279/70:
 with open(cmudict_path, 'rb') as fid:
        lexicon = (l.strip().lower().split() for l in fid)
279/71: lexicon
279/72: list(lexicon)[:100]
279/73: list(lexicon)
279/74:
 with open(cmudict_path, 'rb') as fid:
        lexicon = [l.strip().lower().split() for l in fid]
279/75: lexicon[:100]
279/76:
 with open(cmudict_path, 'r') as fid:
        lexicon = [l.strip().lower().split() for l in fid]
279/77:
 with open(cmudict_path, encoding="utf-8") as fid:
        lexicon = [l.strip().lower().split() for l in fid]
279/78:
 with open(cmudict_path, 'r' ,encoding="utf-8") as fid:
        lexicon = [l.strip().lower().split() for l in fid]
279/79:
 with open(cmudict_path, 'r' ,encoding="ISO-8859-1") as fid:
        lexicon = [l.strip().lower().split() for l in fid]
279/80: lexicon
279/81:
 with open(ted_lex_path, 'r' ,encoding="ISO-8859-1") as fid:
        lexicon = [l.strip().lower().split() for l in fid]
279/82: lexicon
279/83:
 with open(libsp_lex_path, 'r' ,encoding="ISO-8859-1") as fid:
        lexicon = [l.strip().lower().split() for l in fid]
279/84:
 with open(libsp_path, 'r' ,encoding="ISO-8859-1") as fid:
        lexicon = [l.strip().lower().split() for l in fid]
279/85:
 with open(lib_lex_path, 'r' ,encoding="ISO-8859-1") as fid:
        lexicon = [l.strip().lower().split() for l in fid]
279/86: lexicon
279/87: importlib.reload(data_helpers)
279/88: history -g lexicon_to_dict
279/89: cmu_dict = data_helpers.lexicon_to_dict(cmudict_path, "cmudict")
279/90: history -g combine_lexicons
279/91: combo_dict, diff_values = corpus_compare.combine_lexicons(cmu_dict, ted_dict)
279/92: diff_values
279/93: combo_dict, diff_values = corpus_compare.combine_lexicons(cmu_dict, lib_dict)
279/94: combo_dict, diff_values = corpus_compare.combine_lexicons(cmu_dict, ted_dict)
279/95: master_path = "/Users/dustin/CS/consulting/firstlayerai/phoneme_classification/src/awni_speech/speech/examples/master_dict.txt"
279/96:
with open(master_path, 'w') as fid:
    for key,val in combo_dict.items():
        fid.write(f"{key} {val}\n")
279/97: sort_keys = sorted(combo_dict.keys())
279/98: sort_keys[:100]
279/99:
with open(master_path, 'w') as fid:
    for key in sort_keys:
        fid.write(f"{key} {*combo_dict.get(key)}\n")
279/100:
with open(master_path, 'w') as fid:
    for key in sort_keys:
        fid.write(f"{key} {" ".join(combo_dict.get(key))}\n")
279/101:
with open(master_path, 'w') as fid:
    for key in sort_keys:
        fid.write(f"{key} {' '.join(combo_dict.get(key))}\n")
286/1: import csv
286/2: dev_path = "/Users/dustin/CS/consulting/firstlayerai/phoneme_classification/src/awni_speech/speech/examples/common-voice/dev.tsv"
286/3:
with open(dev_path) as fid:
    reader = csv.reader(fid, delimiter='\t')
    for row in reader:
        print(row)
286/4:
with open(dev_path) as fid:
    reader = csv.reader(fid, delimiter='\t')
    print(type(reader))
286/5:
with open(dev_path) as fid:
    data = list(csv.reader(fid, delimiter='\t'))
286/6: len(data)
286/7: data[0]
286/8: data[1]
286/9: len(data)
286/10: len(data[1:])
286/11: header, data = data[0], data[1:]
286/12: len(data)
286/13: data[0]
286/14: header
286/15: len(data)
286/16: accent_set = set()
286/17:
for line in data:
    accent_set.add(line[6])
286/18: accent_set
286/19: accent_set = set()
286/20:
for line in data:
    accent_set.add(line[7])
286/21: accent_set
286/22: x = 7
286/23: 5 <= 7
286/24: 5 <= 7 <=9
286/25: 5 <= 7 <=4
286/26: hist -g open
286/27: import os
286/28: os.path.basename(dev_path)
286/29: base = os.path.basename(dev_path)
286/30: os.path.splitext(base)[0]
286/31: os.path.splitext(dev_path)[0]
286/32: os.path.dirname(dev_path)
286/33: text = "\"\"Express\"\" was sent to investigate the area, finding nothing."
286/34: text
286/35: re.sub('[^A-Za-z0-9]+', '', text)
286/36: import re
286/37: re.sub('[^A-Za-z0-9]+', '', text)
286/38: re.sub('[^A-Za-z0-9]+ ', '', text)
286/39: re.sub('[^A-Za-z0-9] +', '', text)
286/40: re.sub('[^A-Za-z0-9 ]+', '', text)
286/41: re.sub('[^A-Za-z0-9 \']+', '', text)
286/42: text +='
286/43: text +=\'
286/44: text
286/45: text2= "testing this string's capabilitie's ''"
286/46: re.sub('[^A-Za-z0-9 ]+', '', text2)
286/47: re.sub('[^A-Za-z0-9 \']+', '', text2)
286/48: re.sub('[^A-Za-z0-9 \']+', '', text2)
286/49: text
286/50: re.sub('[^A-Za-z0-9 \']+', '', text)
286/51: re.sub('[^A-Za-z0-9 \']+', '', text).split()
286/52: re.sub('[^A-Za-z0-9 \']+', '', text2).split()
286/53: re.sub('[^A-Za-z0-9 \']+', '', text).lower().split()
286/54: re.sub('[^A-Za-z0-9 \']+', '', '"The party operates a weekly newspaper, ""unsere Zeit""."').lower().split()
286/55: re.sub('[^A-Za-z0-9 \']+', '', '"Why didn\'t I come home for supper?"').lower().split()
286/56: re.sub('[^A-Za-z0-9 \']+', '', "Why didn't I come home for supper?").lower().split()
286/57: re.sub('[^A-Za-z0-9 \']+', '', '"""We\'re refugees from the tribal wars, and we need money,"" the other figure said."').lower().split()
286/58: str('!"#$%&()*+, -./:;<=>?@[\]^_`{|}~')
286/59: punct = str('!"#$%&()*+, -./:;<=>?@[\]^_`{|}~')
286/60: trans = re.sub('[^A-Za-z0-9 \']+', '', '"""We\'re refugees from the tribal wars, and we need money,"" the other figure said."').lower().split()
286/61: trans
286/62: punt in trans
286/63: punct in trans
286/64: punct in '!'
286/65: any(punct in '!')
286/66: any([p in '!' for p in punct])
286/67: any([p in trans for p in punct])
286/68: any([p in 'test!' for p in punct])
286/69: trans.append('test!')
286/70: any([p in trans for p in punct])
286/71: file_name = /Users/dustin/CS/consulting/firstlayerai/data/common_voice/en
286/72: file_name = "/Users/dustin/CS/consulting/firstlayerai/data/common_voice/en"
286/73: import os
286/74: file_name=os.path.join(file_name,"dev.tsv")
286/75: file_name
286/76: basename = os.path.basename(file_name)
286/77: basename
286/78: set_name = os.path.splitext(basename)[0]
286/79: set_name
286/80: import glob
286/81: file_name = "/Users/dustin/CS/consulting/firstlayerai/data/common_voice/en"
286/82: patterh = os.path.join(file_name, "*/*.mp3")
286/83: pattern = os.path.join(file_name, "*/*.mp3")
286/84: pattern
286/85: files = glob.glob(pattern)
286/86: len(files)
287/1: test_path = "/Users/dustin/CS/consulting/firstlayerai/data/common_voice/en/invalidated.tsv"
287/2: import os
287/3: os.exists(test_path)
287/4: os.path.exists(test_path)
287/5: mp3_name = "common_voice_en_18422970.mp3"
287/6: label_name = "dev.tsv"
287/7: cv_dir = "/Users/dustin/CS/consulting/firstlayerai/data/common_voice/en
287/8: cv_dir = "/Users/dustin/CS/consulting/firstlayerai/data/common_voice/en"
287/9: label_fn = os.path.join(cv_dir, label_name)
287/10: label_fn
287/11: irname = os.path.dirname(label_fn)
287/12: basename = os.path.basename(label_fn)
287/13: set_name = os.path.splitext(basename)[0]
287/14: dirname = os.path.dirname(label_fn)
287/15: dirname
287/16: basename
287/17: set_name
287/18: json_path = os.path.join(dirname, set_name+".json")
287/19: json_path
287/20: mp3_file = os.path.join(dirname,"clips",audio_name)
287/21: mp3_file = os.path.join(dirname,"clips",mp3_name)
287/22: mp3_file
287/23: base, mp3_ext = os.path.splitext(mp3_file)
287/24: base
287/25: wav_file = base + os.path.extsep + "wav"
287/26: wav_file
287/27:
not os.path.exists(wav_file
)
287/28: command = ['ffmpeg', '-y', '-i', '/Users/dustin/CS/consulting/firstlayerai/data/common_voice/en/clips/common_voice_en_18351787.mp3', '-ar', '16000', '-f', 'wav', '/Users/dustin/CS/consulting/firstlayerai/data/common_voice/en/clips/common_voice_en_18351787.wav']
287/29: " ".jion(command)
287/30: " ".join(command)
287/31: l = []
287/32: boo = not l
287/33: boo
287/34: l1 = [1,2,3,3]
287/35: boo = not l
287/36: boo
287/37: boo = not l1
287/38: boo
287/39: boo = not (l1 )
287/40: boo
287/41: boo = not (l )
287/42: bboo
287/43: boo
287/44: boo = (l )
287/45: boo
287/46: boo = bool(l)
287/47: boo
287/48: boo = bool(l1)
287/49: boo
287/50: command = ['ffmpeg', '-y', '-i', '/Users/dustin/CS/consulting/firstlayerai/data/common_voice/en/clips/common_voice_en_18422503.mp3', '-ar', '16000', '-f', 'wav', '/Users/dustin/CS/consulting/firstlayerai/data/common_voice/en/clips/common_voice_en_18422503.wav']
287/51: " ".join(command)
289/1: import pandas as pd
289/2: data_path = "/Users/dustin/CS/job_projects/prep/triplebyte/data/pjm-energy/pjm_hourly_est.csv"
289/3: data = pd.read_csv(data_path)
289/4: data.head()
289/5: data.iloc[0,0]
289/6: data.iloc[0,1]
289/7: data.iloc[1,0]
289/8: data.iloc[2,0]
289/9: data.iloc[3,0]
289/10: data.head()
289/11: type(data.iloc[0,0])
289/12: strptime(data.iloc[0,0], "Y-m-d")
289/13: date.strptime(data.iloc[0,0], "Y-m-d")
289/14:
import pandas as pd
from datetime import date
289/15: date.strptime(data.iloc[0,0], "Y-m-d")
289/16:
import pandas as pd
import datetime
289/17: datetime.strptime(data.iloc[0,0], "Y-m-d")
289/18: datetime.strptime(data.iloc[0,0], "Y-m-d")
289/19: datetime.strptime("21/11/06 16:30", "%d/%m/%y %H:%M")
289/20:
import pandas as pd
import datetime
289/21: datetime.strptime("21/11/06 16:30", "%d/%m/%y %H:%M")
289/22:
import pandas as pd
from datetime import datetime
289/23: datetime.strptime("21/11/06 16:30", "%d/%m/%y %H:%M")
289/24: datetime.strptime(data.iloc[0,0], "Y-m-d")
289/25: datetime.strptime(data.iloc[0,0], "%Y-%m-%d %H:%M:%S")
289/26: data[date] = datetime.strptime(data["Datetime"], "%Y-%m-%d %H:%M:%S")
289/27:
def convert_datetimem(date_string:str):
    return datetime.strptime(date_string, "%Y-%m-%d %H:%M:%S")
289/28:
def convert_datetime(date_string:str):
    return datetime.strptime(date_string, "%Y-%m-%d %H:%M:%S")
289/29: data[date] = data["Datetime"].apply(convert_datetime)
289/30: data["date"] = data["Datetime"].apply(convert_datetime)
289/31: data.head()
289/32: type(data["date"])
289/33: type(data["date"].iloc[0])
289/34: data["date"].iloc[0] - data["date"].iloc[0]
289/35: data["date"].iloc[0] - data["date"].iloc[1]
289/36: data["date"].iloc[1]  - data["date"].iloc[0]
289/37: data["date"].iloc[0] - data["date"].iloc[1]
289/38: data["date"].iloc[1] - data["date"].iloc[0]
289/39: data["Dateime"] = data["Datetime"].apply(convert_datetime)
289/40: data.head()
289/41: data.head()
289/42: data.head()
289/43: date.drop(label="Dateime")
289/44: data.head()
289/45: date.drop(label="Dateime", axis=1)
289/46: data.drop(label="Dateime")
289/47: data.drop("Dateime")
289/48: data.drop("Dateime", axis=1)
289/49: data["Datetime"] = data["Datetime"].apply(convert_datetime)
289/50: data.head()
289/51: data.head()
289/52:
def convert_datetime(date_string:str):
    return datetime.strptime(date_string, "%Y-%m-%d %H:%M:%S")
289/53: data["Dateime"].head()
289/54: data.drop("Dateime", axis=1)
289/55: data.head()
289/56: data = data.drop("Dateime", axis=1)
289/57: data.head()
289/58: data.dtypes
289/59: data = data.drop(colum="date")
289/60: data = data.drop(column="date")
289/61: data = data.drop(columns="date")
289/62: data.head()
289/63: data.dtypes
289/64: data.head()
289/65: data.iloc[1,0:5]
289/66: data.iloc[0:5,1]
289/67: data.iloc[0:5,1].sum()
289/68: data.iloc[0:5,1].mean/()
289/69: data.iloc[0:5,1].mean()
289/70: data.iloc[0:5,1].std()
289/71: data.loc[0:5,"PJM_Load"].std()
289/72: data.loc[0:5,"PJM_Load"].sum()
289/73: data.loc[0:5,"PJM_Load"].mean()
289/74: data.loc[:,"PJM_Load"].mean()
287/52: from datetime import datetime
287/53: start = datetime.strptime('2020-03-31 16:49', '%Y-%m-%d %H:%M')
287/54: datetime.now()
287/55: datetime.now() - start
287/56: start
287/57: print(datetime.now() - start)
287/58: diff = datetime.now() - start
287/59: print(diff)
287/60: diff.seconds
287/61: diff.seconds/3600
287/62: .4383*60
287/63: diff.milliseconds
287/64: diff.seconds
287/65: diff.total_seconds
287/66: diff.total_seconds()
287/67: diff.microseconds
287/68: diff
287/69: from speech.utils import data_helpers
287/70: unk_path = "/Users/dustin/CS/consulting/firstlayerai/phoneme_classification/src/awni_speech/speech/examples/common-voice/unk_word_stats"
287/71: data_helpers.unique_unknown_words(unk_path)
287/72: unk_path = "/Users/dustin/CS/consulting/firstlayerai/phoneme_classification/src/awni_speech/speech/examples/common-voice/"
287/73: data_helpers.unique_unknown_words(unk_path)
287/74: tgz_path = "/Users/dustin/CS/consulting/firstlayerai/data/voxforge/archive/_r2h-20100822-kmh.tgz"
287/75: import tarfile
287/76:
with tarfile.open(tgz_path) as tf:
    tf.extractall(path="/Users/dustin/CS/consulting/firstlayerai/data/voxforge/archive")
287/77: LEX_URL ="http://www.repository.voxforge1.org/downloads/SpeechCorpus/Trunk/Lexicon/VoxForge.tgz"
287/78: import urllib.request
287/79: save_path = "/Users/dustin/CS/consulting/firstlayerai/data/voxforge"
287/80: urllib.request.urlretrieve(LEX_URL, filename=save_path)
287/81: urllib.request.urlretrieve(LEX_URL, filename=save_path+"tar")
287/82: urllib.request.urlretrieve(LEX_URL, filename=save_path+"/lexicon.tar.gz")
287/83: save_ddir = "/Users/dustin/CS/consulting/firstlayerai/data"
287/84: save_dir = "/Users/dustin/CS/consulting/firstlayerai/data"
287/85: EXT = ".tar.gz"
287/86: save_path = os.path.join(save_dir, "lexicon"+EXT)
287/87: LEX_URL ="http://www.repository.voxforge1.org/downloads/SpeechCorpus/Trunk/Lexicon/VoxForge.tgz"
287/88: urllib.request.urlretrieve(LEX_URL, filename=save_path)
287/89:
with tarfile.open(save_path) as tf:
    tf.extractall(path=save_dir)
287/90:
with tarfile.open(save_path) as tf:
    tf.extractall(path="/Users/dustin/CS/consulting/firstlayerai")
287/91: urllib.request.urlretrieve(LEX_URL, filename=save_path)
287/92:
with tarfile.open(save_path) as tf:
    tf.extractall(path=save_dir)
287/93:
with tarfile.open(save_path) as tf:
    tf.extractall(path=save_dir)
287/94: os.remove(save_path)
287/95: pattern = "*.tgz"
287/96: save_dir
287/97: save_dir = os.path.join(output_dir,"voxforge")
287/98: save_dir = os.path.join(save_dir,"voxforge")
287/99: save_dir
287/100: tar_path = os.join.path(save_dir,"archive",pattern)
287/101:     tar_path = os.path.join(save_dir,"archive",pattern)
287/102: tar_files = glob.glob(tar_path)
287/103: import glob
287/104: tar_files = glob.glob(tar_path)
287/105: len(tar_files)
287/106: sample_dir = os.path.join(save_dir,"archive")
287/107: sample_dir
287/108:
for tar_file in tar_files:
    with tarfile.open(tar_file) as tf:
        tf.extractall(path=sample_dir)
    os.remove(tar_file)
287/109: arr = list("hello", "[hello]", "he', 'l','l']
287/110: arr = list("hello", "[hello]", "he", 'l','l']
287/111: arr = list("hello", "[hello]", "he", 'l','l']
287/112: arr = list("hello", "[hello]", "he", 'l','l')
287/113: arr = ["hello", "[hello]", "he", 'l','l']
287/114: word, phones = arr[0], arr[2:]
287/115: word
287/116: phones
287/117: from speech.utils import data_helpers
287/118: data_helpers.UNK_WORD_TOKEN
287/119: m = 5 if True else: 1
287/120: m = 5 if: True else: 1
287/121: m = 5 if True else 1
287/122: m
287/123: m = 5 if False else 1
287/124: m
287/125: m = 5 if '' else 1
287/126: m
287/127: arr
287/128: arr = list()
287/129: tup = (5,4)
287/130: arr.append(tup)
287/131: arr
287/132: arr.append(tup)
287/133: arr
287/134: arr.append(tup)
287/135: arr
287/136: ls
287/137: cd examples/
287/138: import preprocess
287/139: preprocess.unique_unknown_words("/Users/dustin/CS/consulting/firstlayerai/data/common_voice/en")
287/140: data_path = "/Users/dustin/CS/consulting/firstlayerai/data/voxforge/archive"
287/141: pattern = "*"
287/142: import glob
287/143: import os
287/144: pattern_path = os.path.join(data_path, pattern)
287/145: pattern_path
287/146: sample_dirs = glob.glob(pattern_path)
287/147: len(sample_dirs)
287/148: sample_dirs[0]
287/149: sample_dir = sample_dirs[0]
287/150:  text_path = os.path.join(sample_dir, "etc", "prompts-original")
287/151:
with open(text_path, 'r') as fid:
    for line in fid:
        line = line.split()
        audio_name, transcript = line[0], line[1:]
        audio_path = os.path.join(sample_dir, "wav", audio_name+".wav")
        transcript = " ".join(transcript)
        print((audio_path, transcript))
287/152: type(sample_dirs)
287/153: text_path = "/Users/dustin/CS/consulting/firstlayerai/data/voxforge/archive/anonymous-20100529-fcn/etc/prompts-original"
287/154:
with open(text_path, 'r') as fid:
    for line in fid:
        print(line.split())
287/155:
with open(text_path, 'r') as fid:
    for line in fid:
        line
287/156:
with open(text_path, 'r') as fid:
    for line in fid:
        print(line)
287/157:
with open(text_path, 'r') as fid:
    for line in fid:
        print(len(line))
287/158:
with open(text_path, 'r') as fid:
    for line in fid:
        print(len(line.strip()))
287/159:
with open(text_path, 'r') as fid:
    for line in fid:
        print(line.strip().split())
287/160: text ="cmu_us_slt_arctic/mfc/arctic_a0017"
287/161: text.split(/)
287/162: text.split('/')
287/163: text.split('/')[-1]
287/164: text.split('/')[-1].split('_')[-1]
287/165: text.split('/_')[-1]
287/166: text.split('/',)[-1]
287/167: text.split('/','_')[-1]
287/168: import re
287/169: re.split('[/_]', text)
287/170: re.split(r'[/_]', text)
287/171: re.split(r'[/_]', 'a0017')
287/172: re.split(r'[/_]', 'a0017')[-1]
290/1: from datetime import datetime
290/2: start = datetime.strptime('2020-03-31 16:49', '%Y-%m-%d %H:%M')
290/3: datetime.now() - start
290/4: print(datetime.now() - start)
291/1: label_path = "/Users/dustin/CS/consulting/firstlayerai/data/tatoeba/tatoeba_audio_eng/sentences_with_audio.csv"
291/2: import csv
291/3: import os
291/4:
with open(label_path) as fid:
    reader = list(csv.reader(fid, delimiter='\t'))
    print(reader[0])
291/5:
with open(label_path) as fid:
    reader = list(csv.reader(fid, delimiter='\t'))
    print(reader[0:5])
291/6:
with open(label_path) as fid:
    reader = list(csv.reader(fid, delimiter='\t'))
    print(reader[151269:151271])
291/7:
with open(label_path) as fid:
    reader = list(csv.reader(fid, delimiter='\t'))
    print(reader[151273:151279])
291/8: import sys
291/9:
with open(label_path) as fid:
    reader = list(csv.reader(fid, delimiter='\t'))
    print(sys.getsizeof(reader))
291/10:
with open(label_path) as fid:
    reader = csv.reader(fid, delimiter='\t')
    print(sys.getsizeof(reader))
291/11:
with open(label_path) as fid:
    reader = list(csv.reader(fid, delimiter='\t'))
    print(reader[0])
291/12: speakers = ["CK", "Deliaan", "Pencil", "Susuan1430"]
291/13:  ("CK", "6122904") in [("CK", "6122904")]
291/14: ls
291/15: from speech.utils import noise_injector.py
291/16: from speech.utils import noise_injector
291/17: import numpy as np
291/18: np.arange(100)
291/19: arr = np.arange(100)
291/20: noise_injector.inject_
291/21: noise_path = "/Users/dustin/CS/consulting/firstlayerai/data/background_noise/new_audio/resampled/388338__uminari__short-walk-thru-a-noisy-street-in-a-mexico-city.wav"
291/22: noise_injector.inject_noise_sample(arr, 16000, noise_path, 0.5)
291/23: noise_injector.inject_noise_sample(arr, 16000, noise_path, 0)
291/24: noise_injector.inject_noise_sample(arr, 16000, noise_path, 0.4)
291/25: from speech.utils import wave
291/26: audio_data, samp_rate = wave
291/27: wave_file = "/home/dzubke/awni_speech/data/common-voice/clips/common_voice_en_19711124.wv"
291/28: audio_data, samp_rate = wave.array_from_wave(wave_file)
292/1: audio_file = "/Users/dustin/CS/consulting/firstlayerai/data/background_noise/new_audio/388338__uminari__short-walk-thru-a-noisy-street-in-a-mexico-city.wav"
292/2: ls
292/3: from speech.utils import convert
292/4: ls
293/1: from speech.utils import convert
293/2: audio_file = "/Users/dustin/CS/consulting/firstlayerai/data/background_noise/new_audio/388338__uminari__short-walk-thru-a-noisy-street-in-a-mexico-city.wav"
293/3: convert.convert_2channels(audio_file)
293/4:
with wave.open(audio_file, 'rb') as wid:
    wid.getnchannels()
293/5: import wave
293/6:
with wave.open(audio_file, 'rb') as wid:
    wid.getnchannels()
293/7:
with wave.open(audio_file, 'rb') as wid:
    print(wid.getnchannels())
293/8:
with wave.open(audio_file, 'r') as wid:
    print(wid.getnchannels())
293/9:
wid = wave.open(audio_file, 'r')
print(wid.getnchannels())   
wid.close()
293/10: audio_file
293/11:
wid = wave.open(audio_file)
print(wid.getnchannels())   
wid.close()
293/12:
wid = wave.open(audio_file)
print(wid.getsampwidth())   
wid.close()
293/13:
wid = wave.open(audio_file, mode='rb')
print(wid.getsampwidth())   
wid.close()
293/14: subprocess.check_output(["soxi", audio_file])
293/15: import subprocess
293/16: subprocess.check_output(["soxi", audio_file])
293/17: cmd = subprocess.check_output(["soxi", audio_file])
293/18: cmd.split("\n")
293/19: str(cmd).split("\n")
293/20: str(cmd)
293/21: cmd.decode("utf-8")
293/22: cmd.decode("utf-8").split("\n")
293/23: cmd.decode("utf-8").strip().split("\n")
293/24: cmd.decode("utf-8").strip().split("\n").split(":")
293/25: cmd.decode("utf-8").strip().split("\n")[0]
293/26: cmd.decode("utf-8").strip().split("\n")[1]
293/27: cmd.decode("utf-8").strip().split("\n")[1].split(':')
293/28: cmd.decode("utf-8").strip().split("\n")[1].split(':').strip()
293/29: cmd.decode("utf-8").strip().split("\n")[1].split(':')[1].strip()
293/30: int(cmd.decode("utf-8").strip().split("\n")[1].split(':')[1].strip())
293/31: type(cmd)
293/32: import importlib
293/33: improtlib.reload(convert)
293/34: importlib.reload(convert)
293/35: importlib.reload(convert)
293/36: num_chan = convert.parse_soxi_out(cmd)
293/37: num_chan
293/38: os.rename(audio_file, "/tmp/convet_2channels_audio.wav")
293/39: import os
293/40: os.rename(audio_file, "/tmp/convet_2channels_audio.wav")
293/41: audio_file
293/42: convert.to_wave("/tmp/convet_2channels_audio.wav", audio_file)
293/43: audio_file = "/Users/dustin/CS/consulting/firstlayerai/data/background_noise/new_audio/405419__logancircle2__hare-krishnas.mp3"
293/44: convert.convert_2channels(audio_file)
293/45: audio_file ="/Users/dustin/CS/consulting/firstlayerai/data/background_noise/new_audio/232341__sovy__background-sound.wav"
293/46: convert.convert_2channels(audio_file)
291/29: ls
291/30: cd examples/
291/31: import dataset
291/32: lib100 = dataset.Libsp100Dataset()
291/33: lib100.corpus_namme
291/34: lib100.corpus_name
291/35: lib100.path
291/36: lib100.corpus_name[0].upper()
291/37: test = "libsp"
291/38: test[0] = test[0].upper()
291/39: test[0] = test[0].upper()
291/40: test = "libsp"
291/41: test.capitalize()
291/42: test
291/43: test = test.capitalize()
291/44: test
291/45: test = test.capitalize()
291/46: test
291/47: dataset_name = "Librispeech100"
291/48: from speech import dataset_info
291/49: dataset = eval("dataset_info."+dataset_name+"Dataset")
291/50: dataset = eval("dataset_info."+dataset_name+"Dataset")()
291/51: import importlib
291/52: importlib.reload(dataset_info)
291/53: dataset = eval("dataset_info."+dataset_name+"Dataset")()
291/54: dataset.corpus_name
291/55: import random
291/56: random.randrange(5, 10)
291/57: random.randrange(5, 10)
291/58: random.randrange(5, 10)
291/59: random.randrange(5, 10)
291/60: random.randrange(5, 10)
291/61: random.randrange(5, 10)
291/62: random.randrange(5, 10)
291/63: random.randrange(5, 10)
291/64: random.randrange(5, 10)
291/65: random.randrange(5, 10)
291/66: random.randrange(5, 10)
291/67: "TedliumTest".capitalize()
295/1:
import pandas as pd
from datetime import datetime
295/2: data_path = "/Users/dustin/CS/job_projects/prep/triplebyte/data/pjm-energy/pjm_hourly_est.csv"
295/3: data = pd.read_csv(data_path)
295/4: data.head()
295/5:
def convert_datetime(date_string:str):
    return datetime.strptime(date_string, "%Y-%m-%d %H:%M:%S")
295/6: data.dtypes
295/7:
def convert_datetime(date_string:str):
    return datetime.strptime(date_string, "%Y-%m-%d %H:%M:%S")
295/8: data["Datetime"] = data["Datetime"].apply(convert_datetime)
295/9: data["Dateime"].head()
295/10: data["Datetime"].head()
295/11: data.dtypes
295/12: data["Datetime"].iloc[1] - data["Datetime"].iloc[0]
295/13: data.head()
295/14:
for i, row in enumerate(data):
    if i <5:
295/15:
for i, row in enumerate(data):
    if i <5:
        print(row)
295/16:
for i, row in enumerate(data):
    if i <16:
        print(row)
295/17:
for i, row in enumerate(data):
    if i <20:
        print(row)
295/18:
for i, row in enumerate(data):
    if i <100:
        print(row)
295/19:
i=0
for row in data:
    if i<5:
        print(row)
    i++
295/20:
i=0
for row in data:
    if i<5:
        print(row)
    i+=1
295/21:
i=0
for row in data.iterrows():
    if i<5:
        print(row)
    i+=1
295/22:
i=0
for row in data.iterrows():
    if i<5:
        print(type(row))
    i+=1
295/23:
i=0
for row in data.iterrows():
    if i<5:
        print(row)
    i+=1
295/24:
i=0
for row in data.iterrows():
    if i<4:
        print(type(row))
        print(row)
        print(len(row))
    i+=1
295/25:
i=0
for row in data.iterrows():
    if i<4:
        print(type(row))
        print(row)
        print(len(row[0]))
    i+=1
295/26:
i=0
for row in data.iterrows():
    if i<4:
        print(type(row))
        print(row)
        print(len(row))
    i+=1
295/27:
i=0
for row in data.iterrows():
    if i<2:
        print(f"type: {type(row)}\n")
        print(f"row: {row}\n)
        print(f"total len: {len(row)}\n")
        print(f"first in row{row[0]}\n")
        print(f"second in row{row[1]}\n")
        print(f"{}\n")
        
        
    i+=1
295/28:
i=0
for row in data.iterrows():
    if i<2:
        print(f"type: {type(row)}\n")
        print(f"row: {row}\n")
        print(f"total len: {len(row)}\n")
        print(f"first in row{row[0]}\n")
        print(f"second in row{row[1]}\n")
        print(f"{}\n")
        
        
    i+=1
295/29:
i=0
for row in data.iterrows():
    if i<2:
        print(f"type: {type(row)}\n")
        print(f"row: {row}\n")
        print(f"total len: {len(row)}\n")
        print(f"first in row{row[0]}\n")
        print(f"second in row{row[1]}\n")
        print(f"{"i"}\n")
        
        
    i+=1
295/30:
i=0
for row in data.iterrows():
    if i<2:
        print(f"type: {type(row)}\n")
        print(f"row: {row}\n")
        print(f"total len: {len(row)}\n")
        print(f"first in row{row[0]}\n")
        print(f"second in row{row[1]}\n")
        print(f"{i}\n")
        
        
    i+=1
295/31:
i=0
for row in data.itertuples():
    if i<2:
        print(f"type: {type(row)}\n")
        print(f"row: {row}\n")
        print(f"total len: {len(row)}\n")
        print(f"first in row: {row[0]}\n")
        print(f"second in row: {row[1]}\n")
        print(f"{i}\n")
        
        
    i+=1
295/32: data.query(`PJM_Load`>28500)
295/33: data.query(PJM_Load>28500)
295/34: data.query(@PJM_Load>28500)
295/35: data.query("PJM_Load>28500")
295/36: data.query("PJM_Load is NaN")
295/37: data.query("PJM_Load.isna")
295/38: data.query("PJM_Load.isna()")
295/39: data.query("PJM_Load.notna()")
295/40: data.query("PJM_Load.notna()")[PJM_Load]
295/41: data.query("PJM_Load.notna()")["PJM_Load"]
295/42: data.query("PJM_Load.notna()")
295/43: data.query("PJM_Load.notna()")["Datetime","PJM_Load"]
295/44: data.query("PJM_Load.notna()").loc["Datetime","PJM_Load"]
295/45: data.query("PJM_Load.notna()").loc["PJM_Load"]
295/46: data.query("PJM_Load.notna()")["PJM_Load"]
295/47:
i=0
for row in data.itertuples():
    if i<2:
        print(f"type: {type(row)}\n")
        print(f"row: {row}\n")
        print(f"total len: {len(row)}\n")
        print(f"first in row: {row[0]}\n")
        print(f"second in row: {row[1]}\n")
        print(f"{i}\n")
        
        
    i+=1
295/48: data.query("PJM_Load.notna()").get("Datetime","PJM_Load")
295/49: data.query("PJM_Load.notna()")
295/50: data.query("PJM_Load.notna()").get("Datetime","PJM_Load")
295/51: data.query("PJM_Load.notna()").get("Datetime")
295/52: data.query("PJM_Load.notna()").get("Datetime","PJM_Load")
295/53: data.columns
295/54: data.query("PJM_Load.notna()").get("Datetime","PJM_Load")
295/55: data.columns[0]
295/56: data.columns[0:3]
295/57: data.query("PJM_Load.notna()")[["Datetime","PJM_Load"]]
295/58: PJM_Load = data.query("PJM_Load.notna()")[["Datetime","PJM_Load"]]
295/59: PJM_Load.mean()
295/60: data.query("PJM_Load.notna()").loc(["Datetime","PJM_Load"])
295/61: data.query("PJM_Load.notna()").loc("Datetime","PJM_Load")
295/62: data.query("PJM_Load.notna()").loc(["Datetime","PJM_Load"])
295/63: data.query("PJM_Load.notna()").loc([["Datetime","PJM_Load"])
295/64: data.query("PJM_Load.notna()").loc([["Datetime","PJM_Load"]])
295/65: data.query("PJM_Load.notna()").loc(:,["Datetime","PJM_Load"])
295/66: data.query("PJM_Load.notna()").loc(1:3,["Datetime","PJM_Load"])
295/67: data.loc(1:3,["Datetime","PJM_Load"])
295/68: data.loc[1:3,["Datetime","PJM_Load"]]
295/69: data.query("PJM_Load.notna()").loc[:,["Datetime","PJM_Load"]]
295/70: data.query("PJM_Load.notna()").iloc[:,["Datetime","PJM_Load"]]
295/71: data.query("PJM_Load.notna()").loc[:,["Datetime","PJM_Load"]]
295/72: data.query("PJM_Load.notna()").loc[["Datetime","PJM_Load"]]
295/73: data.query("PJM_Load.notna()").loc[:,["Datetime","PJM_Load"]]
295/74: data.query("PJM_Load.notna()").loc[:,"Datetime":"PJM_Load"]
295/75: data.iloc[1:3,0:2]
295/76: PJM_Load = data.query("PJM_Load.notna()").loc[:,["Datetime","PJM_Load"]]
295/77: PJM_Load.mean()
295/78: PJM_Load.plot()
295/79: PJM_Load.std()
295/80: PJM_Load.loc["PJM_Load"].plot()
295/81: PJM_Load.loc["PJM_Load"]
295/82: PJM_Load
295/83: PJM_Load.loc["PJM_Load"]
295/84: PJM_Load
295/85: PJM_Load.loc[:,"PJM_Load"]
295/86: PJM_Load.loc[:,"PJM_Load"].plot()
295/87: PJM_Load.loc[:,"PJM_Load"].plot(kind=bar)
295/88: PJM_Load.loc[:,"PJM_Load"].plot(kind="bar")
295/89: PJM_Load.loc[:,"PJM_Load"].plot.bar()
295/90: PJM_Load.query("PJM_Load.mean()-PJM_Load.std()<PJM_Load<=PJM_Load.mean()+PJM_Load.std()")
295/91: PJM_Load.query("PJM_Load.mean()-PJM_Load.std()<PJM_Load<=PJM_Load.mean()+PJM_Load.std()").count(axis=0)
295/92: PJM_Load.query("PJM_Load.mean()-PJM_Load.std()<PJM_Load<=PJM_Load.mean()+PJM_Load.std()").count(axis=0)
295/93: buckets = PJM_Load.query("PJM_Load.mean()-PJM_Load.std()<PJM_Load<=PJM_Load.mean()+PJM_Load.std()").count(axis=0)
295/94:
buckets = PJM_Load.query("PJM_Load.mean()-PJM_Load.std()<PJM_Load<=PJM_Load.mean()+PJM_Load.std()").count(axis=0)
buckets[0]
295/95:
buckets = PJM_Load.query("PJM_Load.mean()-PJM_Load.std()<PJM_Load<=PJM_Load.mean()+PJM_Load.std()").count(axis=0)
buckets[1]
295/96:
buckets = PJM_Load.query("PJM_Load.mean()-PJM_Load.std()<PJM_Load<=PJM_Load.mean()+PJM_Load.std()").count(axis=0)
type(buckets)
295/97:
buckets = PJM_Load.query("PJM_Load.mean()-PJM_Load.std()<PJM_Load<=PJM_Load.mean()+PJM_Load.std()").count(axis=0)
buckets["PJM_Load"]
295/98: PJM_Load.loc[:,"PJM_Load"].plot.bar()
295/99: df = pd.DataFrame()
295/100: df["test"]=1
295/101:
df["test"]=1
df
295/102:
df["test"]=1
df["list"]=[1,2,3]
295/103:
df["test"]=1
df["list"]=[1,2,3]
df
295/104:
df["test"]=1
df["list"]=[1,2,3]
df[4] = [4]
df
295/105:
df["test"]=1
df["list"]=[1,2,3]
df[4] = [4,4,5]
df
295/106:
def std_buckets(df:pd.DataFrame, bucket_num=4)->pd.DataFrame:
    mean = df.mean()
    std = df.std()
    df_bucket = pd.DataFrame()
    for i in range(1, bucket_num+1):
        count_ds = df.query("mean-i*std <PJM_Load<= mean+i*std").count(axis=0)
        count = count_ds["PJM_Load"]
        df_bucket[i] = count
    return df_bucket
295/107:
def std_buckets(df:pd.DataFrame, bucket_num=4)->pd.DataFrame:
    """
    Counts the number of values in buckets of standard devations away 
    from the mean and returns them as a dataframe
    """
    mean = df.mean()
    std = df.std()
    df_bucket = pd.DataFrame()
    for i in range(1, bucket_num+1):
        count_ds = df.query("mean-i*std <PJM_Load<= mean+i*std").count(axis=0)
        count = count_ds["PJM_Load"]
        df_bucket[i] = count
    return df_bucket
295/108:
df["test"]=1
df["list"]=[1,2,3]
df[4] = [4,4,5]
df["test"]
295/109:
def std_buckets(df:pd.DataFrame, col_name, bucket_num=4)->pd.DataFrame:
    """
    Counts the number of values in buckets of standard devations away 
    from the mean and returns them as a dataframe
    """
    mean = df[col_name].mean()
    std = df[col_name].std()
    df_bucket = pd.DataFrame()
    for i in range(1, bucket_num+1):
        count_ds = df.query("mean-i*std <col_name<= mean+i*std").count(axis=0)
        count = count_ds["PJM_Load"]
        df_bucket[i] = count
    return df_bucket
295/110: buckets = std_buckets(PJM_Load, "PJM_Load", 4)
295/111: PJM_Loadd
295/112: PJM_Load
295/113: PJM_Load.mean()
295/114: PJM_Load.mean().value()
295/115: PJM_Load.mean().values()
295/116: PJM_Load.mean().values
295/117: PJM_Load.mean()
295/118: PJM_Load.mean()[0]
295/119: PJM_Load.mean().loc[0]
295/120: PJM_Load.mean().loc[:,:]
295/121: PJM_Load.mean().loc[0,:]
295/122: PJM_Load.mean().loc[0,0]
295/123: PJM_Load.mean()[0]
295/124: PJM_Load.std()[0]
295/125:
def std_buckets(df:pd.DataFrame, col_name, bucket_num=4)->pd.DataFrame:
    """
    Counts the number of values in buckets of standard devations away 
    from the mean and returns them as a dataframe
    """
    mean = df[col_name].mean()[0]
    std = df[col_name].std()[0]
    df_bucket = pd.DataFrame()
    for i in range(1, bucket_num+1):
        count_ds = df.query("mean-i*std <col_name<= mean+i*std").count(axis=0)
        count = count_ds["PJM_Load"]
        df_bucket[i] = count
    return df_bucket
295/126: buckets = std_buckets(PJM_Load, "PJM_Load", 4)
295/127: PJM_Load["PJM_Load"].std()[0]
295/128: PJM_Load["PJM_Load"].std()
295/129: PJM_Load.std()
295/130:
def std_buckets(df:pd.DataFrame, col_name, bucket_num=4)->pd.DataFrame:
    """
    Counts the number of values in buckets of standard devations away 
    from the mean and returns them as a dataframe
    """
    mean = df[col_name].mean()
    std = df[col_name].std()
    df_bucket = pd.DataFrame()
    for i in range(1, bucket_num+1):
        count_ds = df.query("mean-i*std <col_name<= mean+i*std").count(axis=0)
        count = count_ds["PJM_Load"]
        df_bucket[i] = count
    return df_bucket
295/131: buckets = std_buckets(PJM_Load, "PJM_Load", 4)
295/132:
def std_buckets(df:pd.DataFrame, col_name, bucket_num=4)->pd.DataFrame:
    """
    Counts the number of values in buckets of standard devations away 
    from the mean and returns them as a dataframe
    """
    mean = df[col_name].mean()
    std = df[col_name].std()
    print("here I am")
    df_bucket = pd.DataFrame()
    for i in range(1, bucket_num+1):
        count_ds = df.query("mean-i*std <col_name<= mean+i*std").count(axis=0)
        count = count_ds["PJM_Load"]
        df_bucket[i] = count
    return df_bucket
295/133: buckets = std_buckets(PJM_Load, "PJM_Load", 4)
295/134: std=PJM_Load["PJM_Load"].std()
295/135:
std=PJM_Load["PJM_Load"].std()
PJM_Load.query(PJM_Load>std)
295/136:
std=PJM_Load["PJM_Load"].std()
PJM_Load.query("PJM_Load>std")
295/137:
std=PJM_Load["PJM_Load"].std()
PJM_Load.query("PJM_Load>@std")
295/138:
def std_buckets(df:pd.DataFrame, col_name, bucket_num=4)->pd.DataFrame:
    """
    Counts the number of values in buckets of standard devations away 
    from the mean and returns them as a dataframe
    """
    mean = df[col_name].mean()
    std = df[col_name].std()
    df_bucket = pd.DataFrame()
    for i in range(1, bucket_num+1):
        count_ds = df.query("@mean-i*@std <col_name<= @mean+i*@std").count(axis=0)
        count = count_ds["PJM_Load"]
        df_bucket[i] = count
    return df_bucket
295/139: buckets = std_buckets(PJM_Load, "PJM_Load", 4)
295/140:
def std_buckets(df:pd.DataFrame, col_name, bucket_num=4)->pd.DataFrame:
    """
    Counts the number of values in buckets of standard devations away 
    from the mean and returns them as a dataframe
    """
    mean = df[col_name].mean()
    std = df[col_name].std()
    df_bucket = pd.DataFrame()
    for i in range(1, bucket_num+1):
        count_ds = df.query("@mean-@i*@std <col_name<= @mean+@i*@std").count(axis=0)
        count = count_ds["PJM_Load"]
        df_bucket[i] = count
    return df_bucket
295/141: buckets = std_buckets(PJM_Load, "PJM_Load", 4)
295/142:
def std_buckets(df:pd.DataFrame, col_name, bucket_num=4)->pd.DataFrame:
    """
    Counts the number of values in buckets of standard devations away 
    from the mean and returns them as a dataframe
    """
    mean = df[col_name].mean()
    std = df[col_name].std()
    df_bucket = pd.DataFrame()
    for i in range(1, bucket_num+1):
        count_ds = df.query("@mean-@i*@std <@col_name<= @mean+@i*@std").count(axis=0)
        count = count_ds["PJM_Load"]
        df_bucket[i] = count
    return df_bucket
295/143: buckets = std_buckets(PJM_Load, "PJM_Load", 4)
291/68: import os
291/69: timit_path="/Users/dustin/CS/consulting/firstlayerai/data/timit_datasets/ffmpeg"
291/70: listdir_timit = os.listdir(timit_path)
291/71: type(listdir_timit)
291/72: len(listdir_timit)
291/73: listdir_timit
291/74: os.scandir(timit_path)
291/75: scandir = os.scandir(timit_path)
291/76: os.getsize(scandir)
291/77: import sys
291/78: sys.getsizeof(scandir)
291/79: tim_glob = glob.iglob(timit_path+"/*/*/*/*/*.wav")
291/80: import glob
291/81: tim_glob = glob.iglob(timit_path+"/*/*/*/*/*.wav")
291/82: sys.getsizeof(tim_glob)
291/83: tim_list = glob.glob(timit_path+"/*/*/*/*/*.wav")
291/84: tim_iter = glob.iglob(timit_path+"/*/*/*/*/*.wav")
291/85: sys.getsizeof(tim_list)
291/86: sys.getsizeof(tim_iter)
291/87: tim_list
291/88: [tim_iter]
291/89: [*tim_iter]
291/90: tim_iter
291/91: tiimt_path="/Users/dustin/CS/consulting/firstlayerai/data/timit_datasets/ffmpeg/timit/TRAIN"
291/92: tim_iter = glob.iglob(timit_path+"/*/*/*/*/*.WAV")
291/93: tim_list = glob.glob(timit_path+"/*/*/*.WAV")
291/94: tim_iter = glob.iglob(timit_path+"/*/*/*.WAV")
291/95: sys.getsizeof(tim_iter)
291/96: sys.getsizeof(tim_list)
291/97: tim_list
291/98: timit_path
291/99: timit_path="/Users/dustin/CS/consulting/firstlayerai/data/timit_datasets/ffmpeg/timit/TRAIN"
291/100: tim_list = glob.glob(timit_path+"/*/*/*.WAV")
291/101: tim_iter = glob.iglob(timit_path+"/*/*/*.WAV")
291/102: sys.getsizeof(tim_list)
291/103: sys.getsizeof(tim_iter)
291/104: import timeit
291/105: import random
291/106: import numpy as np
291/107: np.random.choice(time_list)
291/108: np.random.choice(tim_list)
291/109: np.random.choice(tim_iter)
291/110: tim_iter = glob.iglob(timit_path+"/*/*/*.WAV")
291/111: len([*tim_iter])
291/112: len([*tim_iter])
291/113: tim_iter = glob.iglob(timit_path+"/*/*/*.WAV")
291/114: random.choice(tim_list)
291/115: random.choice(tim_iter)
291/116: inte=7
291/117: sys.getsizeof(inte)
291/118: sys.getsizeof(7)
291/119: sys.getsizeof("hello")
291/120: sys.getsizeof("hell")
291/121: sys.getsizeof("hel")
291/122: sys.getsizeof("he")
291/123: sys.getsizeof("h")
291/124: sys.getsizeof('a')
291/125: sys.getsizeof(7)
291/126: sys.getsizeof(7.0)
291/127: sys.getsizeof(timit_path)
291/128: 128*4260
291/129: len([*tim_list])
291/130: tim_list[0]
291/131: sys.getsizeof('/Users/dustin/CS/consulting/firstlayerai/data/timit_datasets/ffmpeg/timit/TRAIN/DR4/MMDM0/SI681.WAV')
291/132: 4620*150
291/133: sys.getsizeof(tim_list)
291/134: sys.getsizeof(7)
291/135: sys.getsizeof([7,7,7,7])
291/136: 4*28
291/137: ls
291/138: cd ..
291/139: from speech import dataset_info
291/140: libsp = dataset_info.LibrispeechDataset()
291/141: libsp = dataset_info.Librispeech100Dataset()
291/142: libsp = dataset_info.Dataset()
291/143: libsp = dataset_info.Librispeech100Dataset()
291/144: libsp.get_audio_files()
291/145: import importlib
291/146: importlib.reload(dataset_info)
291/147: importlib.reload(dataset_info)
291/148: libsp = dataset_info.Librispeech100Dataset()
291/149: libsp.get_audio_files()
291/150: importlib.reload(dataset_info)
291/151: libsp = dataset_info.Librispeech100Dataset()
291/152: libsp.get_audio_files()
291/153: importlib.reload(dataset_info)
291/154: libsp = dataset_info.Librispeech100Dataset()
291/155: len(libsp.get_audio_files())
291/156: importlib.reload(data_helpers)
291/157: from speech.utils import data_helpers
291/158: importlib.reload(dataset_info)
291/159: libsp = dataset_info.Librispeech100Dataset()
291/160: len(libsp.get_audio_files())
296/1: ls
296/2: from speech import dataset_info
296/3: libsp = Librispeech100Dataset()
296/4: libsp = dataset_info.Librispeech100Dataset()
296/5: libsp.get_audio_files()
296/6: string = "com.example.appname.library.widgetname"
296/7: string.find('.')
296/8: string.find('.')
296/9: string.find('.')
296/10: ls
296/11: cd examples/noise/
296/12: FS_CREDITS = "./FREESOUNDCREDITS.txt"
296/13:
with open(FS_CREDITS, 'r') as fid:
    fid.seek(7)
    print(fid.readline())
296/14:
with open(FS_CREDITS, 'r') as fid:
    for i in range(8):
        tmp=fid.readline()
    print(fid.readline())
296/15:
with open(FS_CREDITS, 'r') as fid:
    for i in range(7):
        tmp=fid.readline()
    print(fid.readline())
296/16:
with open(FS_CREDITS, 'r') as fid:
    for i in range(6):
        tmp=fid.readline()
    print(fid.readline())
296/17: len(range(7))
296/18:
with open(FS_CREDITS, 'r') as fid:
    for i in range(6):
        tmp=fid.readline()
    print(fid.readline().split())
296/19:
with open(FS_CREDITS, 'r') as fid:
    for i in range(7):
        tmp=fid.readline()
    print(fid.readline().split())
296/20:
with open(FS_CREDITS, 'r') as fid:
    for i in range(7):
        tmp=fid.readline()
    soundid, by, username = fid.readline().split()
296/21:
with open(FS_CREDITS, 'r') as fid:
    for i in range(7):
        tmp=fid.readline()
    soundid, by, username = fid.readline().split()
    print(soundid, username)
296/22:
with open(FS_CREDITS, 'r') as fid:
    for i in range(7):
        tmp=fid.readline()
    soundid, by, username = fid.readline().strip.split()
    print(soundid, username)
296/23:
with open(FS_CREDITS, 'r') as fid:
    for i in range(7):
        tmp=fid.readline()
    soundid, by, username = fid.readline().strip().split()
    print(soundid, username)
296/24: skip_lines = 7  # skip the first lines
296/25:
with open(FS_CREDITS, 'r') as fid:
    for index, line in enumerate(fid):
        if index <= skip_lines:
            continue
        parsed_line = line.strip().split()
        assert len(parsed_line)==3, "more than 3 elements in parsed_line"
        soundid, by, username = parsed_line
296/26:
with open(FS_CREDITS, 'r') as fid:
    for index, line in enumerate(fid):
        if index <= skip_lines:
            continue
        parsed_line = line.strip().split()
        assert len(parsed_line)==3, f"more than 3 elements in parsed_line: {parsed_line}"
        soundid, by, username = parsed_line
296/27: l = ["1234", "by", "frank", "drebin"]
296/28: "%20
296/29: "%20".join(l[2:])
296/30: l[2:] = "%20".join(l[2:])
296/31: l
296/32: l[:2]
296/33: l = l[:2]
296/34: l = l[:2]
296/35: int("1234")
296/36: int("frank")
296/37:
with open(FS_CREDITS, 'r') as fid:
    for index, line in enumerate(fid):
        if index <= skip_lines:
            continue
        print(line)
296/38:
with open(FS_CREDITS, 'r') as fid:
    for index, line in enumerate(fid):
        if index <= skip_lines:
            continue
        if index == skip_lines+1:
            print(line)
296/39: skip_lines=6
296/40:
with open(FS_CREDITS, 'r') as fid:
    for index, line in enumerate(fid):
        if index <= skip_lines:
            continue
        if index == skip_lines+1:
            print(line)
296/41:
with open(FS_CREDITS, 'r') as fid:
    for index, line in enumerate(fid):
        if index <= skip_lines:
            continue
        # format of parsed_line is [soundid, "by", username]
        parsed_line = line.strip().split()
        soundid = parsed_line[0]
        by = parsed_line[1]
        username = parsed_line[2:]

        #check the values are what we expect
        assert int(soundid), "sounid is not string of integer"
        assert by == "by", "middle value is not expected"
        assert len(username) < 3, "username longer than 3 elements"

        # some usernames have space in them
        if len(username)==2:
            # adding in URL space character %20
            username = "%20".join(username)
296/42:
with open(FS_CREDITS, 'r') as fid:
    for index, line in enumerate(fid):
        if index <= skip_lines:
            continue
        # format of parsed_line is [soundid, "by", username]
        parsed_line = line.strip().split()
        soundid = parsed_line[0]
        by = parsed_line[1]
        username = parsed_line[2:]

        #check the values are what we expect
        assert int(soundid), "sounid is not string of integer"
        assert by == "by", "middle value is not expected"
        assert len(username) < 3, f"username longer than 3 elements: {username}"

        # some usernames have space in them
        if len(username)==2:
            # adding in URL space character %20
            username = "%20".join(username)
296/43:
with open(FS_CREDITS, 'r') as fid:
    for index, line in enumerate(fid):
        if index <= skip_lines:
            continue
        # format of parsed_line is [soundid, "by", username]
        parsed_line = line.strip().split()
        soundid = parsed_line[0]
        by = parsed_line[1]
        username = parsed_line[2:]

        if len(username) > 1:
            if " ".join(username)=="Pullover aus Milch":
                continue

        #check the values are what we expect
        assert int(soundid), "sounid is not string of integer"
        assert by == "by", "middle value is not expected"
        assert len(username) < 3, f"username longer than 3 elements: {username}"

        # some usernames have space in them
        if len(username)==2:
            # adding in URL space character %20
            username = "%20".join(username)
296/44:
with open(FS_CREDITS, 'r') as fid:
    for index, line in enumerate(fid):
        if index <= skip_lines:
            continue
        # format of parsed_line is [soundid, "by", username]
        parsed_line = line.strip().split()
        soundid = parsed_line[0]
        by = parsed_line[1]
        username = parsed_line[2:]

        if len(username) > 1:
            if " ".join(username)=="Pullover aus Milch":
                continue

        #check the values are what we expect
        assert int(soundid), "sounid is not string of integer"
        assert by == "by", "middle value is not expected"
        assert len(username) < 3, f"username longer than 3 elements: {username}"

        # some usernames have space in them
        if len(username)==2:
            # adding in URL space character %20
            username = "%20".join(username)
296/45:
missing_usernames = set("Oscar de Ávila", "Pullover aus Milch")
with open(FS_CREDITS, 'r') as fid:
    for index, line in enumerate(fid):
        if index <= skip_lines:
            continue
        # format of parsed_line is [soundid, "by", username]
        parsed_line = line.strip().split()
        soundid = parsed_line[0]
        by = parsed_line[1]
        username = parsed_line[2:]

        if len(username) > 1:
            if " ".join(username) in missing_usernames:
                continue

        #check the values are what we expect
        assert int(soundid), "sounid is not string of integer"
        assert by == "by", "middle value is not expected"
        assert len(username) < 3, f"username longer than 3 elements: {username}"

        # some usernames have space in them
        if len(username)==2:
            # adding in URL space character %20
            username = "%20".join(username)
296/46:
missing_usernames = {"Oscar de Ávila", "Pullover aus Milch"}
with open(FS_CREDITS, 'r') as fid:
    for index, line in enumerate(fid):
        if index <= skip_lines:
            continue
        # format of parsed_line is [soundid, "by", username]
        parsed_line = line.strip().split()
        soundid = parsed_line[0]
        by = parsed_line[1]
        username = parsed_line[2:]

        if len(username) > 1:
            if " ".join(username) in missing_usernames:
                continue

        #check the values are what we expect
        assert int(soundid), "sounid is not string of integer"
        assert by == "by", "middle value is not expected"
        assert len(username) < 3, f"username longer than 3 elements: {username}"

        # some usernames have space in them
        if len(username)==2:
            # adding in URL space character %20
            username = "%20".join(username)
296/47:
skip_lines = 6  # skip the first lines
username_sounid = list()
missing_usernames = {"Oscar de Ávila", "Pullover aus Milch"}
with open(FS_CREDITS, 'r') as fid:
    for index, line in enumerate(fid):
        if index <= skip_lines:
            continue
        # format of parsed_line is [soundid, "by", username]
        parsed_line = line.strip().split()
        soundid = parsed_line[0]
        by = parsed_line[1]
        username = parsed_line[2:]

        if len(username) > 1:
            if " ".join(username) in missing_usernames:
                continue
            
            # some usernames have a space in them
            if len(username)==2:
                # adding in URL space character %20
                username = "%20".join(username)

        #check the values are what we expect
        assert int(soundid), "sounid is not string of integer"
        assert by == "by", "middle value is not expected"
        assert len(username) < 3, f"username longer than 3 elements: {username}"

        username_sounid.append( (username, soundid) )
296/48:
skip_lines = 6  # skip the first lines
username_sounid = list()
missing_usernames = {"Oscar de Ávila", "Pullover aus Milch"}
with open(FS_CREDITS, 'r') as fid:
    for index, line in enumerate(fid):
        if index <= skip_lines:
            continue
        # format of parsed_line is [soundid, "by", username]
        parsed_line = line.strip().split()
        soundid = parsed_line[0]
        by = parsed_line[1]
        username = parsed_line[2:]

        if len(username) > 1:
            if " ".join(username) in missing_usernames:
                continue
            # some usernames have a space in them
            elif len(username)==2:
                # adding in URL space character %20
                username = ["%20".join(username)]

        #check the values are what we expect
        assert int(soundid), "sounid is not string of integer"
        assert by == "by", "middle value is not expected"
        assert len(username)==1, f"username longer than 1 element: {username}"

        # unpack username list
        username = username[0]

        username_sounid.append( (username, soundid) )
296/49:
FS_CREDITS = "./FREESOUNDCREDITS.txt"

skip_lines = 6  # skip the first lines
username_sounid = list()
missing_usernames = {"Oscar de Ávila", "Pullover aus Milch"}
with open(FS_CREDITS, 'r') as fid:
    for index, line in enumerate(fid):
        if index <= skip_lines:
            continue
        # format of parsed_line is [soundid, "by", username]
        parsed_line = line.strip().split()
        soundid = parsed_line[0]
        by = parsed_line[1]
        username = parsed_line[2:]

        if len(username) > 1:
            if " ".join(username) in missing_usernames:
                continue
            # some usernames have a space in them
            elif len(username)==2:
                # adding in URL space character %20
                username = ["%20".join(username)]

        #check the values are what we expect
        assert int(soundid), "sounid is not string of integer"
        assert by == "by", "middle value is not expected"
        assert len(username)==1, f"username longer than 1 element: {username}"

        # unpack username list
        username = username[0]

        username_sounid.append( (username, soundid) )

URL_TEMPLATE = "http://www.freesound.org/people/{username}/sounds/{soundid}"
test_u, test_s = username_soundid[0]
print(URL_TEMPLATE.format(username=test_u, soundid=test_s))
296/50:
skip_lines = 6  # skip the first lines
username_soundid = list()
missing_usernames = {"Oscar de Ávila", "Pullover aus Milch"}
with open(FS_CREDITS, 'r') as fid:
    for index, line in enumerate(fid):
        if index <= skip_lines:
            continue
        # format of parsed_line is [soundid, "by", username]
        parsed_line = line.strip().split()
        soundid = parsed_line[0]
        by = parsed_line[1]
        username = parsed_line[2:]

        if len(username) > 1:
            if " ".join(username) in missing_usernames:
                continue
            # some usernames have a space in them
            elif len(username)==2:
                # adding in URL space character %20
                username = ["%20".join(username)]

        #check the values are what we expect
        assert int(soundid), "sounid is not string of integer"
        assert by == "by", "middle value is not expected"
        assert len(username)==1, f"username longer than 1 element: {username}"

        # unpack username list
        username = username[0]

        username_soundid.append( (username, soundid) )

URL_TEMPLATE = "http://www.freesound.org/people/{username}/sounds/{soundid}"
test_u, test_s = username_soundid[0]
print(URL_TEMPLATE.format(username=test_u, soundid=test_s))
296/51:
FS_CREDITS = "./FREESOUNDCREDITS.txt"

skip_lines = 6  # skip the first lines
username_soundid = list()
missing_usernames = {"Oscar de Ávila", "Pullover aus Milch"}
with open(FS_CREDITS, 'r') as fid:
    for index, line in enumerate(fid):
        if index <= skip_lines:
            continue
        # format of parsed_line is [soundid, "by", username]
        parsed_line = line.strip().split()
        soundid = parsed_line[0]
        by = parsed_line[1]
        username = parsed_line[2:]

        if len(username) > 1:
            if " ".join(username) in missing_usernames:
                continue
            # some usernames have a space in them
            elif len(username)==2:
                # adding in URL space character %20
                username = ["%20".join(username)]

        #check the values are what we expect
        assert int(soundid), "sounid is not string of integer"
        assert by == "by", "middle value is not expected"
        assert len(username)==1, f"username longer than 1 element: {username}"

        # unpack username list
        username = username[0]

        username_soundid.append( (username, soundid) )

URL_TEMPLATE = "http://www.freesound.org/people/{username}/sounds/{soundid}/download"
test_u, test_s = username_soundid[0]
print(URL_TEMPLATE.format(username=test_u, soundid=test_s))
296/52: import requests
296/53: conda install requests
296/54: import requests
296/55: FS_base = "https://freesound.org/"
296/56: endpont = "https://freesound.org/apiv2/sounds/100852/"
296/57: endpoint = "https://freesound.org/apiv2/sounds/100852/"
296/58: API_KEY
296/59: API_KEY="61ByMhJd6piSFb6eZrypVOqizsyKv8abidzpxnLE   "
296/60: API_KEY
296/61: API_KEY=r'61ByMhJd6piSFb6eZrypVOqizsyKv8abidzpxnLE  '
296/62: API_KEY
296/63: API_KEY=r'61ByMhJd6piSFb6eZrypVOqizsyKv8abidzpxnLE'
296/64: API_KEY
296/65: data = {'api_dev_key':API_KEY}
296/66: PARAMS = {'address':location}
296/67: test_id="100852"
296/68: endpoint = "https://freesound.org/apiv2/sounds/"
296/69: PARAMS = {'sound_id':test_id}
296/70: r = requests.get(url = URL, params = PARAMS)
296/71: r = requests.get(url =endpoint, params = PARAMS)
296/72: r
296/73: data
296/74: r = requests.get(url =endpoint, params = PARAMS, data=data)
296/75: r
296/76: dir(requests.get)
296/77: curl -H "Authorization: Bearer {{61ByMhJd6piSFb6eZrypVOqizsyKv8abidzpxnLE}}" 'httpss://freesound.org/apiv2/sounds/14854/download/'
296/78: curl -H "Authorization: Bearer {{61ByMhJd6piSFb6eZrypVOqizsyKv8abidzpxnLE}}" 'https://freesound.org/apiv2/sounds/14854/download/'
296/79: r = requests.get(url='https://freesound.org/apiv2/sounds/1234/')
296/80: r
296/81: APIKEY_FILE = "/Users/dustin/CS/consulting/firstlayerai/phoneme_classification/src/awni_speech/freesound_apikey.txt"
296/82: API_KEY = with open(APIKEY_FILE, 'r') as fid: fid.readline
296/83: with open(APIKEY_FILE, 'r') as fid: fid.readline
296/84: with open(APIKEY_FILE, 'r') as fid: fid.readline
296/85:
with open(APIKEY_FILE, 'r') as fid:
    API_KEY = fid.readline()
296/86: API_KEY
296/87: API_KEY = with open(APIKEY_FILE, 'r') as fid: fid.readline()
296/88: r = requests.get(url="https://freesound.org/apiv2/sounds/100852/?token=61ByMhJd6piSFb6eZrypVOqizsyKv8abidzpxnLE")
296/89: r
296/90: r.text
296/91: type(r.text)
296/92: dict(r.text)
296/93: import json
296/94: json.load(r.text)
296/95: json.loads(r.text)
296/96: properties = json.loads(r.text)
296/97: properties.get("duration")
296/98: properties.get("previews")
296/99: properties.get("previews").get('preview-lq-mp3')
296/100: requests.get(properties.get("previews").get('preview-lq-mp3'))
296/101: ls
296/102: requests.get(properties.get("previews").get('preview-lq-mp3'), allow_redirects=True)
296/103: ls
296/104: download_req = requests.get(properties.get("previews").get('preview-lq-mp3'), allow_redirects=True)
296/105: download_req
296/106: download_req.text
296/107:
with open("./test_download.mp3", 'w') as fid:
    fid.write(download_req.content)
296/108:
with open("./test_download.mp3", 'wb') as fid:
    fid.write(download_req.content)
296/109: download_url = properties.get("previews").get('preview-lq-mp3')
296/110: download_url
296/111: import os
296/112: filename = os.path.basename(download_url)
296/113: filename
296/114: soundid
296/115: GET_TEMPLATE = "https://freesound.org/apiv2/sounds/{sound_id}/?token={API_KEY}"
296/116: response = requests.get(GET_TEMPLATE.format(sound_id=sound_soid, API_KEY=API_KEY))
296/117:         response = requests.get(GET_TEMPLATE.format(sound_id=sound_id, API_KEY=API_KEY))
296/118: sound_id = soundid
296/119:         response = requests.get(GET_TEMPLATE.format(sound_id=sound_id, API_KEY=API_KEY))
296/120:         properties = json.loads(response.text)
296/121: properies
296/122: properties
296/123: properties.get("duration")
296/124: download_url =  properties.get("previews").get('preview-lq-mp3')
296/125: download_response = requests.get(download_url)
296/126: download_dir="/Users/dustin/CS/consulting/firstlayerai/phoneme_classification/src/awni_speech/speech/examples/noise"
296/127: download_file = os.path.basename(download_url)
296/128: download_file
296/129: download_file = os.path.join(download_dir, download_file)
296/130: download_file
296/131:
with open(download_file, 'wb') as fid:
                fid.write(download_response.content)
296/132: sound_id = 103199
296/133:
        response = requests.get(GET_TEMPLATE.format(sound_id=sound_id, API_KEY=API_KEY))
        properties = json.loads(response.text)
        if properties.get("duration") > min_duration:
            # retrieve the url for the low-quality mp3 version
            download_url =  properties.get("previews").get('preview-lq-mp3')
            # get the download_url file and write to download_dir
            download_response = requests.get(download_url)
            download_file = os.path.basename(download_url)
            download_file = os.path.join(download_dir, download_file)
            with open(download_file, 'wb') as fid:
                fid.write(download_response.content)
296/134:     min_duration = 20   # min file duration to filter upon
296/135:
        response = requests.get(GET_TEMPLATE.format(sound_id=sound_id, API_KEY=API_KEY))
        properties = json.loads(response.text)
        if properties.get("duration") > min_duration:
            # retrieve the url for the low-quality mp3 version
            download_url =  properties.get("previews").get('preview-lq-mp3')
            # get the download_url file and write to download_dir
            download_response = requests.get(download_url)
            download_file = os.path.basename(download_url)
            download_file = os.path.join(download_dir, download_file)
            with open(download_file, 'wb') as fid:
                fid.write(download_response.content)
296/136: sound_id=103074
296/137:
        response = requests.get(GET_TEMPLATE.format(sound_id=sound_id, API_KEY=API_KEY))
        properties = json.loads(response.text)
        if properties.get("duration") > min_duration:
            # retrieve the url for the low-quality mp3 version
            download_url =  properties.get("previews").get('preview-lq-mp3')
            # get the download_url file and write to download_dir
            download_response = requests.get(download_url)
            download_file = os.path.basename(download_url)
            download_file = os.path.join(download_dir, download_file)
            with open(download_file, 'wb') as fid:
                fid.write(download_response.content)
296/138: sound_id=102305
296/139:
        response = requests.get(GET_TEMPLATE.format(sound_id=sound_id, API_KEY=API_KEY))
        properties = json.loads(response.text)
        if properties.get("duration") > min_duration:
            # retrieve the url for the low-quality mp3 version
            download_url =  properties.get("previews").get('preview-lq-mp3')
            # get the download_url file and write to download_dir
            download_response = requests.get(download_url)
            download_file = os.path.basename(download_url)
            download_file = os.path.join(download_dir, download_file)
            with open(download_file, 'wb') as fid:
                fid.write(download_response.content)
296/140: sound_id=102305
296/141:
        response = requests.get(GET_TEMPLATE.format(sound_id=sound_id, API_KEY=API_KEY))
        properties = json.loads(response.text)
        if properties.get("duration") > min_duration:
            # retrieve the url for the low-quality mp3 version
            download_url =  properties.get("previews").get('preview-lq-mp3')
            # get the download_url file and write to download_dir
            download_response = requests.get(download_url)
            download_file = os.path.basename(download_url)
            download_file = os.path.join(download_dir, download_file)
            with open(download_file, 'wb') as fid:
                fid.write(download_response.content)
296/142: sound_id=106955
296/143:
        response = requests.get(GET_TEMPLATE.format(sound_id=sound_id, API_KEY=API_KEY))
        properties = json.loads(response.text)
        if properties.get("duration") > min_duration:
            # retrieve the url for the low-quality mp3 version
            download_url =  properties.get("previews").get('preview-lq-mp3')
            # get the download_url file and write to download_dir
            download_response = requests.get(download_url)
            download_file = os.path.basename(download_url)
            download_file = os.path.join(download_dir, download_file)
            with open(download_file, 'wb') as fid:
                fid.write(download_response.content)
296/144: download_dir
296/145: response = requests.get(GET_TEMPLATE.format(sound_id=sound_id, API_KEY=API_KEY))
296/146: repsonse
296/147: response
296/148: response.text
296/149: properties.get("duration")
296/150: sound_id =110622
296/151:
        response = requests.get(GET_TEMPLATE.format(sound_id=sound_id, API_KEY=API_KEY))
        properties = json.loads(response.text)
        if properties.get("duration") > min_duration:
            # retrieve the url for the low-quality mp3 version
            download_url =  properties.get("previews").get('preview-lq-mp3')
            # get the download_url file and write to download_dir
            download_response = requests.get(download_url)
            download_file = os.path.basename(download_url)
            download_file = os.path.join(download_dir, download_file)
            with open(download_file, 'wb') as fid:
                fid.write(download_response.content)
296/152: sound_id =110622123
296/153:
        response = requests.get(GET_TEMPLATE.format(sound_id=sound_id, API_KEY=API_KEY))
        properties = json.loads(response.text)
        if properties.get("duration") > min_duration:
            # retrieve the url for the low-quality mp3 version
            download_url =  properties.get("previews").get('preview-lq-mp3')
            # get the download_url file and write to download_dir
            download_response = requests.get(download_url)
            download_file = os.path.basename(download_url)
            download_file = os.path.join(download_dir, download_file)
            with open(download_file, 'wb') as fid:
                fid.write(download_response.content)
296/154: properties
296/155: len(properties)
296/156: sound_id =110622
296/157:
        response = requests.get(GET_TEMPLATE.format(sound_id=sound_id, API_KEY=API_KEY))
        properties = json.loads(response.text)
        if properties.get("duration") > min_duration:
            # retrieve the url for the low-quality mp3 version
            download_url =  properties.get("previews").get('preview-lq-mp3')
            # get the download_url file and write to download_dir
            download_response = requests.get(download_url)
            download_file = os.path.basename(download_url)
            download_file = os.path.join(download_dir, download_file)
            with open(download_file, 'wb') as fid:
                fid.write(download_response.content)
296/158: len(properties)
296/159: import sleep
296/160: import time
296/161: time.sleep(3)
296/162: time.sleep(0.3)
296/163: time.sleep(0.5)
296/164: sound_id
296/165:
        response = requests.get(GET_TEMPLATE.format(sound_id=sound_id, API_KEY=API_KEY))
        properties = json.loads(response.text)
        if len(properties)==1: continue     # sound_id was not found, skip to next example
        if properties.get("duration") > min_duration:
            # retrieve the url for the low-quality mp3 version
            download_url =  properties.get("previews").get('preview-lq-mp3')
            # get the download_url file and write to download_dir
            download_response = requests.get(download_url)
            download_file = os.path.basename(download_url)
            download_file = os.path.join(download_dir, download_file)
            with open(download_file, 'wb') as fid:
                fid.write(download_response.content)
            time.sleep(0.200)    # sleep for 200 ms to not overload API
296/166:
        response = requests.get(GET_TEMPLATE.format(sound_id=sound_id, API_KEY=API_KEY))
        properties = json.loads(response.text)
        # skip to next example
        if properties.get("duration") > min_duration:
            # retrieve the url for the low-quality mp3 version
            download_url =  properties.get("previews").get('preview-lq-mp3')
            # get the download_url file and write to download_dir
            download_response = requests.get(download_url)
            download_file = os.path.basename(download_url)
            download_file = os.path.join(download_dir, download_file)
            with open(download_file, 'wb') as fid:
                fid.write(download_response.content)
            time.sleep(0.200)    # sleep for 200 ms to not overload API
296/167: ls
296/168: import download
296/169: list = download.collect_soundid()
296/170: import importlib
296/171: importlib.reload(download)
296/172: list = download.collect_soundid()
296/173: id_list = download.collect_soundid()
296/174: len(id_list)
296/175: importlib.reload(download)
296/176: id_list = download.collect_soundid()
296/177: len(id_list)
296/178: download_dir
296/179: download.get_write(id_list, download_dir)
296/180: download.get_and_write(id_list, download_dir)
296/181: API_KEY
296/182:
val_df[val_df.sentence.eq(sentence)]\
            .sort_values("vote_diff", ascending=False)\
            .iloc[filter_value:,:].index()
297/1: validated_path ="/Users/dustin/CS/consulting/firstlayerai/data/common_voice/en/validated.tsv"
297/2: import pandas as pd
297/3: val_df = pd.read_csv(validated_path, delimiter='\t',encoding='utf-8')
297/4:
accents=["us", "canada"]    
    # 231011 rows with accents "us" and "canada", 206653 with us and 24358 with canada 
    val_df = val_df[val_df.accent.isin(accents)]
297/5:
accents=["us", "canada"]    
# 231011 rows with accents "us" and "canada", 206653 with us and 24358 with canada 
val_df = val_df[val_df.accent.isin(accents)]
297/6: val_df.count()
297/7: val_df["vote_diff"] = val_df.up_votes - val_df.down_votes
297/8:     val_df['sentence']=val_df['sentence'].str.replace('[^\w\s]','').str.lower()
297/9:     val_df.sentence.value_counts(sort=True, ascending=False)
297/10:     count_dict=val_df.sentence.value_counts(sort=True, ascending=False).to_dict()
297/11: max_sentence="he did find it soon after dawn and not far from the sand pits"
297/12: min_sentence="a man in all black runs down the road"
297/13:
drop_index = val_df[val_df.sentence.eq(max_sentence)]\
            .sort_values("vote_diff", ascending=False)\
            .iloc[filter_value:,:].index
297/14: filter_value=25
297/15:
drop_index = val_df[val_df.sentence.eq(max_sentence)]\
            .sort_values("vote_diff", ascending=False)\
            .iloc[filter_value:,:].index
297/16: drop_index.count()
297/17: len(drop_index)
297/18: cd examples/
297/19: ls
297/20: import assess
297/21: val_df, drop_row_count = assess.filter_by_count(val_df, count_dict, max_occurance)
297/22: val_df, drop_row_count = assess.filter_by_count(val_df, count_dict, filter_value)
297/23:     print(f"number of rows dropped: {drop_row_count}")
297/24: for values in count_dict.values()
297/25: drop_row_count_2 = 0
297/26:
for repeat in count_dict.values():
    if repeat > filter_value:
        drop_row_count_2+= (repeat - filter_value)
297/27: print(drop_row_count_2)
297/28:  accents = ['us', 'canada']
297/29: validated_path
297/30: row_count = 0
297/31: import csv
297/32:
with open(validated_path) as fid:  
    reader = list(csv.reader(fid, delimiter='\t'))
    header = reader.readline()
    for line in reader:
                # filter by accent
                if line[7] in accents:
                    row_count +=1
297/33:
with open(validated_path) as fid:  
    reader = csv.reader(fid, delimiter='\t')
    header = reader.readline()
    for line in reader:
                # filter by accent
                if line[7] in accents:
                    row_count +=1
297/34:
with open(validated_path) as fid:  
    reader = csv.reader(fid, delimiter='\t')
    header = next(reader)
    for line in reader:
                # filter by accent
                if line[7] in accents:
                    row_count +=1
297/35: row_count
297/36: row_count=0
297/37:
with open(validated_path) as fid:  
    reader = csv.reader(fid, delimiter='\t')
    header = next(reader)
    for line in reader:
                # filter by accent
                if line[7] in accents:
                    row_count +=1
297/38: row_count
297/39: audio_trans=list()
297/40: dir_path=""
297/41:
with open(validated_path) as fid:  
    reader = csv.reader(fid, delimiter='\t')
    header = next(reader)
    for line in reader:
                # filter by accent
                if line[7] in accents:
                    audio_path = os.path.join(dir_path, "clips", line[1])
                    transcript = line[2]
                    audio_trans.append((audio_path, transcript))
297/42: import os
297/43:
with open(validated_path) as fid:  
    reader = csv.reader(fid, delimiter='\t')
    header = next(reader)
    for line in reader:
                # filter by accent
                if line[7] in accents:
                    audio_path = os.path.join(dir_path, "clips", line[1])
                    transcript = line[2]
                    audio_trans.append((audio_path, transcript))
297/44: len(audio_trans)
297/45: train_path = "/Users/dustin/CS/consulting/firstlayerai/data/common_voice/en/train.tsv"
297/46: row_count
297/47: row_count=0
297/48:
with open(train_path) as fid:  
    reader = csv.reader(fid, delimiter='\t')
    header = next(reader)
    for line in reader:
                # filter by accent
                if line[7] in accents:
                    row_count +=1
297/49: row_count
298/1: tat_path="/Users/dustin/CS/consulting/firstlayerai/data/tatoeba/tatoeba_audio_eng/audio"
298/2: import glo b
298/3: from speech import dataset_info
298/4: conda info -e
298/5: conda activate awni_env36
299/1: tat_path="/Users/dustin/CS/consulting/firstlayerai/data/tatoeba/tatoeba_audio_eng/audio"
299/2: from speech import dataset_info
299/3: dataset=dataset_info.TatoebaDataset()
299/4: audio_files = dataset.get_audio_files()
299/5: audio_files[:10]
299/6: dataset.pattern
299/7: dataset.audio_dir
299/8: dataset.get_audio_files()
299/9: import importlib
299/10: importlib.reload(dataset_info)
299/11: audio_files = dataset.get_audio_files()
299/12: audio_files[:10]
299/13: from speech.utils import data_helpers
299/14: audio_files = data_helpers.get_files(dataset.audio_dir, dataset.pattern)
299/15: len(audio_files)
299/16: dataset.audio_dir
299/17: dataset.pattern
299/18: list(range(0,1.2, 0.05))
299/19: list(range(0,120, 5))
299/20: m = list(range(0,120, 5))/100
299/21: m = [x/100 for x in range(0,120, 5)]
299/22: m
299/23: m = [x/100 for x in range(0,120, 5)]
299/24: [range(0,25,10)]
299/25: [*range(0,25,10)]
299/26: m = iter('foobar')
299/27: m.next()
299/28: next(m)
299/29: next(m)
299/30: next(m)
299/31: next(m)
299/32: len(m)
299/33: next(m)
299/34: next(m)
299/35: next(m)
299/36: next(m)
299/37: next(m)
299/38:
for i, j, k in [(1, 2, 3), (3, 4, 5), (5, 6, 7)]:
    print(i, j, k)
299/39: m = iter('foobar')
299/40: m.__next__()
299/41:
def infinite_sequence():
    num = 0
    while True:
        yield num
        num += 1
299/42: gen = infinite_sequence()
299/43: next(gen)
299/44: next(gen)
299/45: next(gen)
299/46: next(gen)
299/47: next(gen)
299/48: next(gen)
299/49: import numpy as np
299/50: tempo_range=(0.85, 0.85)
299/51: low_tempo, high_tempo = tempo_range
299/52: tempo_value = np.random.uniform(low=low_tempo, high=high_tempo)
299/53: tempo_value
299/54: np.random.uniform(low=low_tempo, high=high_tempo)
299/55: np.random.uniform(low=low_tempo, high=high_tempo)
299/56: np.random.uniform(low=low_tempo, high=high_tempo)
299/57: np.random.uniform(low=low_tempo, high=high_tempo)
299/58: np.random.uniform(low=low_tempo, high=high_tempo)
299/59: np.random.uniform(low=low_tempo, high=high_tempo)
299/60: np.random.uniform(low=low_tempo, high=high_tempo)
299/61: np.random.uniform(low=low_tempo, high=high_tempo)
299/62: np.random.uniform(low=low_tempo, high=high_tempo)
299/63: np.random.uniform(low=low_tempo, high=high_tempo)
299/64: np.random.uniform(low=low_tempo, high=high_tempo)
299/65: np.random.uniform(low=low_tempo, high=high_tempo)
299/66: from speech.utils.speed_vol_perturb import speed_vol_perturb
299/67: audio_path = "/home/dzubke/awni_speech/data/LibriSpeech/train-clean-100/19/198/19-198-0000.wav
299/68: audio_path = "/home/dzubke/awni_speech/data/LibriSpeech/train-clean-100/19/198/19-198-0000.wav"
299/69: from speech.utils.wave import array_from_wave
299/70: audio, sr = array_from_wave(audio_path)
299/71: audio_path = "/Users/dustin/CS/consulting/firstlayerai/data/voxforge/na-accent/_caustic_-20170306-smy/wav/en-0186.wav"
299/72: audio, sr = array_from_wave(audio_path)
299/73: aug_audio = speed_vol_pertrub(audio_path)
299/74: aug_audio = speed_vol_perturb(audio_path)
299/75: len(audio)
299/76: aug_audio = speed_vol_perturb(audio_path, tempo=(1,1))
299/77: aug_audio = speed_vol_perturb(audio_path, tempo_range=(1,1))
299/78: len(aug_audio)
299/79: tempo = 1
299/80: aug_audio = speed_vol_perturb(audio_path, tempo_range=(tempo,tempo))
299/81: assert len(audio)==tempo*len(aug_audio)
299/82: tempo = 0.85
299/83: aug_audio = speed_vol_perturb(audio_path, tempo_range=(tempo,tempo))
299/84: assert len(audio)==tempo*len(aug_audio)
299/85: len(aug_audio)
299/86: assert len(audio)==len(aug_audio)/tempo
299/87: len(aug_audio)/tempo
299/88: len(audio)
299/89: tempo
299/90: len(aug_audio)
299/91: len(aug_audio)*tempo
299/92: assert len(audio)==len(aug_audio)*tempo
299/93: import pytest
299/94: conda info -e
299/95: import pytest
299/96: type(audio)
299/97: audio.size
299/98: np.testing.assert_allclose(aug_audio.size*tempo, audio.size, decimal=7)
299/99: np.testing.assert_allclose(aug_audio.size*tempo, audio.size, decimal=3)
299/100: np.random(1)
300/1: import pytest
301/1: import pytest
302/1: import pytest
303/1:
def test_tupe(a):
    one, two = a
    return one + two
303/2: test_tupe((1,4))
303/3: test_tupe([1,4])
303/4:
def test_tupe(a=(1,4)):
    one, two = a
    return one + two
303/5: test_tupe()
303/6: test_tupe((5,4))
303/7: np
303/8: import numpy as np
303/9: l = [1,2,3,4,5]
303/10: l.append("str')
303/11: l.append("str")
303/12: np.random.choice(l)
303/13: np.random.choice(l)
303/14: np.random.choice(l)
303/15: np.random.choice(l)
303/16: np.random.choice(l)
303/17: np.random.choice(l)
303/18: np.random.choice(l)
303/19: np.random.choice(l)
303/20: np.random.choice(l)
303/21: np.random.choice(l, size=2)
303/22: np.random.choice(l, size=2)
303/23: np.random.choice(l, size=2)
303/24: np.random.choice(l, size=2)
303/25: np.random.choice(l, size=2)
303/26: np.random.choice(l, size=2)
303/27: np.random.choice(l, size=2)
303/28: np.random.choice(l, size=2)
303/29: np.random.choice(l, size=2)
303/30: np.random.choice(l, size=2)
303/31: np.random.choice(l, size=2, replace=False)
303/32: np.random.choice(l, size=2, replace=False)
303/33: np.random.choice(l, size=2, replace=False)
303/34: np.random.choice(l, size=2, replace=False)
303/35: np.random.choice(l, size=2, replace=False)
303/36: np.random.choice(l, size=2, replace=False)
303/37: np.random.choice(l, size=2, replace=False)
303/38: np.random.choice(l, size=2, replace=False)
303/39: np.random.choice(l, size=2, replace=False)
303/40: np.random.choice(l, size=2, replace=False)
303/41: np.random.choice(l, size=2, replace=False)
303/42: np.random.choice(l, size=2, replace=False)
303/43: np.random.choice(l, size=2, replace=False)
303/44: np.random.choice(l, size=2, replace=False)
303/45: np.random.choice(l, size=2, replace=False)
303/46: np.random.choice(l, size=2, replace=False)
303/47: np.random.choice(l, size=2, replace=False)
303/48: np.random.choice(l, size=2, replace=False)
303/49: np.random.choice(l, size=2, replace=False)
303/50: np.random.choice(l, size=2, replace=False)
303/51: np.random.choice(l, size=2, replace=False)
303/52: np.random.choice(l, size=2, replace=False)
303/53: np.random.choice(l, size=2, replace=False)
303/54: np.random.choice(l, size=2, replace=False)
303/55: np.random.choice(l, size=2, replace=False)
303/56: np.random.choice(l, size=2, replace=False)
303/57: np.random.choice(l, size=2, replace=False)
303/58: cmd_str = "gcloud compute scp "+GCP_USER_PROJECT+"{audio} {dst_dir}"
303/59: GCP_USER_PROJECT = "dzubke@phoneme-2:"
303/60: cmd_str = "gcloud compute scp "+GCP_USER_PROJECT+"{audio} {dst_dir}"
303/61: cmd_str.format(audio="hello", dst_dir="frank")
303/62: LOCAL_DIR = "/Users/dustin/CS/consulting/firstlayerai/data/dataset_samples"
303/63: os.exists(LOCAL_DIR)
303/64: import os
303/65: os.exists(LOCAL_DIR)
303/66: os.path.exists(LOCAL_DIR)
303/67: __dir__(os)
303/68: os.__dir__()
303/69: "mkdir" in os.__dir__()
303/70: "path" in os.__dir__()
303/71: "exists" in os.path.__dir__()
303/72: "join" in os.path.__dir__()
303/73: "mkdir" in os.path.__dir__()
303/74:
audio_choices = ['/home/dzubke/awni_speech/data/common-voice/clips/common_voice_en_18861611.wv',
       '/home/dzubke/awni_speech/data/common-voice/clips/common_voice_en_18934993.wv',
       '/home/dzubke/awni_speech/data/common-voice/clips/common_voice_en_17843777.wv',
       '/home/dzubke/awni_speech/data/common-voice/clips/common_voice_en_153126.wv',
       '/home/dzubke/awni_speech/data/common-voice/clips/common_voice_en_9567.wv',
       '/home/dzubke/awni_speech/data/common-voice/clips/common_voice_en_18355538.wv',
       '/home/dzubke/awni_speech/data/common-voice/clips/common_voice_en_501550.wv',
       '/home/dzubke/awni_speech/data/common-voice/clips/common_voice_en_17391353.wv',
       '/home/dzubke/awni_speech/data/common-voice/clips/common_voice_en_591691.wv',
       '/home/dzubke/awni_speech/data/common-voice/clips/common_voice_en_621414.wv',
       '/home/dzubke/awni_speech/data/common-voice/clips/common_voice_en_18717777.wv',
       '/home/dzubke/awni_speech/data/common-voice/clips/common_voice_en_96037.wv',
       '/home/dzubke/awni_speech/data/common-voice/clips/common_voice_en_17492851.wv',
       '/home/dzubke/awni_speech/data/common-voice/clips/common_voice_en_18901850.wv',
       '/home/dzubke/awni_speech/data/common-voice/clips/common_voice_en_164410.wv',
       '/home/dzubke/awni_speech/data/common-voice/clips/common_voice_en_535048.wv',
       '/home/dzubke/awni_speech/data/common-voice/clips/common_voice_en_18339984.wv',
       '/home/dzubke/awni_speech/data/common-voice/clips/common_voice_en_17450784.wv',
       '/home/dzubke/awni_speech/data/common-voice/clips/common_voice_en_163971.wv',
       '/home/dzubke/awni_speech/data/common-voice/clips/common_voice_en_19511860.wv',
       '/home/dzubke/awni_speech/data/common-voice/clips/common_voice_en_20118586.wv',
       '/home/dzubke/awni_speech/data/common-voice/clips/common_voice_en_670973.wv']
303/75: len(audio_choices)
303/76:
cv_audio_choices=['/home/dzubke/awni_speech/data/common-voice/clips/common_voice_en_121241.wv',
       '/home/dzubke/awni_speech/data/common-voice/clips/common_voice_en_160188.wv',
       '/home/dzubke/awni_speech/data/common-voice/clips/common_voice_en_20162063.wv',
       '/home/dzubke/awni_speech/data/common-voice/clips/common_voice_en_20161710.wv',
       '/home/dzubke/awni_speech/data/common-voice/clips/common_voice_en_673610.wv',
       '/home/dzubke/awni_speech/data/common-voice/clips/common_voice_en_20045103.wv',
       '/home/dzubke/awni_speech/data/common-voice/clips/common_voice_en_18579395.wv',
       '/home/dzubke/awni_speech/data/common-voice/clips/common_voice_en_672717.wv',
       '/home/dzubke/awni_speech/data/common-voice/clips/common_voice_en_19156933.wv',
       '/home/dzubke/awni_speech/data/common-voice/clips/common_voice_en_18522557.wv',
       '/home/dzubke/awni_speech/data/common-voice/clips/common_voice_en_177977.wv',
       '/home/dzubke/awni_speech/data/common-voice/clips/common_voice_en_617079.wv',
       '/home/dzubke/awni_speech/data/common-voice/clips/common_voice_en_18548617.wv',
       '/home/dzubke/awni_speech/data/common-voice/clips/common_voice_en_675777.wv',
       '/home/dzubke/awni_speech/data/common-voice/clips/common_voice_en_39895.wv',
       '/home/dzubke/awni_speech/data/common-voice/clips/common_voice_en_18281671.wv',
       '/home/dzubke/awni_speech/data/common-voice/clips/common_voice_en_19344607.wv',
       '/home/dzubke/awni_speech/data/common-voice/clips/common_voice_en_17876975.wv',
       '/home/dzubke/awni_speech/data/common-voice/clips/common_voice_en_1427692.wv',
       '/home/dzubke/awni_speech/data/common-voice/clips/common_voice_en_672111.wv',
       '/home/dzubke/awni_speech/data/common-voice/clips/common_voice_en_20090963.wv',
       '/home/dzubke/awni_speech/data/common-voice/clips/common_voice_en_19690205.wv',
       '/home/dzubke/awni_speech/data/common-voice/clips/common_voice_en_6932927.wv',
       '/home/dzubke/awni_speech/data/common-voice/clips/common_voice_en_481678.wv',
       '/home/dzubke/awni_speech/data/common-voice/clips/common_voice_en_17262193.wv',
       '/home/dzubke/awni_speech/data/common-voice/clips/common_voice_en_700658.wv',
       '/home/dzubke/awni_speech/data/common-voice/clips/common_voice_en_17263342.wv',
       '/home/dzubke/awni_speech/data/common-voice/clips/common_voice_en_33730.wv',
       '/home/dzubke/awni_speech/data/common-voice/clips/common_voice_en_18052336.wv',
       '/home/dzubke/awni_speech/data/common-voice/clips/common_voice_en_18670750.wv',
       '/home/dzubke/awni_speech/data/common-voice/clips/common_voice_en_17250297.wv',
       '/home/dzubke/awni_speech/data/common-voice/clips/common_voice_en_19244436.wv',
       '/home/dzubke/awni_speech/data/common-voice/clips/common_voice_en_19187496.wv',
       '/home/dzubke/awni_speech/data/common-voice/clips/common_voice_en_660162.wv',
       '/home/dzubke/awni_speech/data/common-voice/clips/common_voice_en_18974241.wv',
       '/home/dzubke/awni_speech/data/common-voice/clips/common_voice_en_18851796.wv',
       '/home/dzubke/awni_speech/data/common-voice/clips/common_voice_en_19010887.wv',
       '/home/dzubke/awni_speech/data/common-voice/clips/common_voice_en_19701393.wv',
       '/home/dzubke/awni_speech/data/common-voice/clips/common_voice_en_617568.wv',
       '/home/dzubke/awni_speech/data/common-voice/clips/common_voice_en_19514973.wv',
       '/home/dzubke/awni_speech/data/common-voice/clips/common_voice_en_19747782.wv',
       '/home/dzubke/awni_speech/data/common-voice/clips/common_voice_en_630277.wv',
       '/home/dzubke/awni_speech/data/common-voice/clips/common_voice_en_19148413.wv',
       '/home/dzubke/awni_speech/data/common-voice/clips/common_voice_en_19965736.wv',
       '/home/dzubke/awni_speech/data/common-voice/clips/common_voice_en_561207.wv',
       '/home/dzubke/awni_speech/data/common-voice/clips/common_voice_en_211676.wv',
       '/home/dzubke/awni_speech/data/common-voice/clips/common_voice_en_17246164.wv',
       '/home/dzubke/awni_speech/data/common-voice/clips/common_voice_en_18539504.wv',
       '/home/dzubke/awni_speech/data/common-voice/clips/common_voice_en_19747922.wv',
       '/home/dzubke/awni_speech/data/common-voice/clips/common_voice_en_519824.wv']
303/77: len(cv_audio_choices)
303/78: LOCAL_DIR = "/Users/dustin/CS/consulting/firstlayerai/data/dataset_samples"
303/79: import os
303/80: dst_dir = os.path.join(LOCAL_DIR,"common-voice")
303/81: cmd_str = "gcloud compute scp "+GCP_USER_PROJECT+"{audio_file} "+dst_dir
303/82:
        for audio_file in cv_audio_choices:
            cmd_str.format(audio_file=audio_file)
            os.system(cmd_str)
303/83: cv_audio_files[0]
303/84: cv_audio_file[0]
303/85: cv_audio_choices[0]
303/86: cmd_str
303/87: os.mkdir(dst_dir)
303/88: cmd_str.format(audio_file=cv_audio_choices[0])
303/89: os.system(cmd_str.format(audio_file=cv_audio_choices[0]))
303/90:
        for audio_file in cv_audio_choices:
            cmd_str.format(audio_file=audio_file)
            os.system(cmd_str)
303/91: os.system(cmd_str.format(audio_file=cv_audio_choices[0]))
303/92:
for audio_file in cv_audio_choices:
            os.system(cmd_str.format(audio_file=audio_file))
303/93:
ted_audio_choices= ['/home/dzubke/awni_speech/data/tedlium/TEDLIUM_release-3/data/converted/wav/AndreaGhez_2009G_74.wav.wav',
       '/home/dzubke/awni_speech/data/tedlium/TEDLIUM_release-3/data/converted/wav/AnneCurzan_2014X_141.wav.wav',
       '/home/dzubke/awni_speech/data/tedlium/TEDLIUM_release-3/data/converted/wav/AdamGrant_2016_55.wav.wav',
       '/home/dzubke/awni_speech/data/tedlium/TEDLIUM_release-3/data/converted/wav/RishiManchanda_2014S_37.wav.wav',
       '/home/dzubke/awni_speech/data/tedlium/TEDLIUM_release-3/data/converted/wav/PaulCollier_2009S_136.wav.wav',
       '/home/dzubke/awni_speech/data/tedlium/TEDLIUM_release-3/data/converted/wav/TimFerriss_2008P_133.wav.wav',
       '/home/dzubke/awni_speech/data/tedlium/TEDLIUM_release-3/data/converted/wav/DeborahLipstadt_2017X_116.wav.wav',
       '/home/dzubke/awni_speech/data/tedlium/TEDLIUM_release-3/data/converted/wav/AnthonyAtala_2009P_7.wav.wav',
       '/home/dzubke/awni_speech/data/tedlium/TEDLIUM_release-3/data/converted/wav/EdwardSnowden_2014_24.wav.wav',
       '/home/dzubke/awni_speech/data/tedlium/TEDLIUM_release-3/data/converted/wav/AspenBaker_2015W_3.wav.wav',
       '/home/dzubke/awni_speech/data/tedlium/TEDLIUM_release-3/data/converted/wav/ThomasInsel_2013X_69.wav.wav',
       '/home/dzubke/awni_speech/data/tedlium/TEDLIUM_release-3/data/converted/wav/IlonaSzabodeCarvalho_2014G_45.wav.wav',
       '/home/dzubke/awni_speech/data/tedlium/TEDLIUM_release-3/data/converted/wav/OmarAhmad_2010U_30.wav.wav',
       '/home/dzubke/awni_speech/data/tedlium/TEDLIUM_release-3/data/converted/wav/MichaelMilken_2001_59.wav.wav',
       '/home/dzubke/awni_speech/data/tedlium/TEDLIUM_release-3/data/converted/wav/MajoraCarter_2010X_79.wav.wav',
       '/home/dzubke/awni_speech/data/tedlium/TEDLIUM_release-3/data/converted/wav/OrenYakobovich_2014G_67.wav.wav',
       '/home/dzubke/awni_speech/data/tedlium/TEDLIUM_release-3/data/converted/wav/MargaretBourdeaux_2015X_22.wav.wav',
       '/home/dzubke/awni_speech/data/tedlium/TEDLIUM_release-3/data/converted/wav/JamaisCascio_2006_105.wav.wav',
       '/home/dzubke/awni_speech/data/tedlium/TEDLIUM_release-3/data/converted/wav/ThomasBarnett_2005_106.wav.wav',
       '/home/dzubke/awni_speech/data/tedlium/TEDLIUM_release-3/data/converted/wav/AubreydeGrey_2005G_138.wav.wav',
       '/home/dzubke/awni_speech/data/tedlium/TEDLIUM_release-3/data/converted/wav/EduardoBriceno_2016X_85.wav.wav',
       '/home/dzubke/awni_speech/data/tedlium/TEDLIUM_release-3/data/converted/wav/AngelaPatton_2012X_45.wav.wav',
       '/home/dzubke/awni_speech/data/tedlium/TEDLIUM_release-3/data/converted/wav/DanielPauly_2010Z_1.wav.wav',
       '/home/dzubke/awni_speech/data/tedlium/TEDLIUM_release-3/data/converted/wav/SugataMitra_2010G_114.wav.wav',
       '/home/dzubke/awni_speech/data/tedlium/TEDLIUM_release-3/data/converted/wav/EricSanderson_2009G_3.wav.wav',
       '/home/dzubke/awni_speech/data/tedlium/TEDLIUM_release-3/data/converted/wav/JonathanHaidt_2012S_14.wav.wav',
       '/home/dzubke/awni_speech/data/tedlium/TEDLIUM_release-3/data/converted/wav/TyroneHayes_2010W_44.wav.wav',
       '/home/dzubke/awni_speech/data/tedlium/TEDLIUM_release-3/data/converted/wav/AnandGiridharadas_2015_164.wav.wav',
       '/home/dzubke/awni_speech/data/tedlium/TEDLIUM_release-3/data/converted/wav/SalmanKhan_2015P_40.wav.wav',
       '/home/dzubke/awni_speech/data/tedlium/TEDLIUM_release-3/data/converted/wav/TimBrown_2009G_159.wav.wav',
       '/home/dzubke/awni_speech/data/tedlium/TEDLIUM_release-3/data/converted/wav/NielsDiffrient_2002_112.wav.wav',
       '/home/dzubke/awni_speech/data/tedlium/TEDLIUM_release-3/data/converted/wav/KarenArmstrong_2009G_71.wav.wav',
       '/home/dzubke/awni_speech/data/tedlium/TEDLIUM_release-3/data/converted/wav/FrancisdelosReyes_2013U_66.wav.wav',
       '/home/dzubke/awni_speech/data/tedlium/TEDLIUM_release-3/data/converted/wav/ArthurBenjamin_2013G_43.wav.wav',
       '/home/dzubke/awni_speech/data/tedlium/TEDLIUM_release-3/data/converted/wav/CourtneyMartin_2016_95.wav.wav',
       '/home/dzubke/awni_speech/data/tedlium/TEDLIUM_release-3/data/converted/wav/SteveRamirez_2013X_41.wav.wav',
       '/home/dzubke/awni_speech/data/tedlium/TEDLIUM_release-3/data/converted/wav/RaghavaKK_2011G_11.wav.wav',
       '/home/dzubke/awni_speech/data/tedlium/TEDLIUM_release-3/data/converted/wav/CharlesAnderson_2009I_121.wav.wav',
       '/home/dzubke/awni_speech/data/tedlium/TEDLIUM_release-3/data/converted/wav/SasaVucinic_2005G_43.wav.wav',
       '/home/dzubke/awni_speech/data/tedlium/TEDLIUM_release-3/data/converted/wav/JamesWatson_2005_96.wav.wav',
       '/home/dzubke/awni_speech/data/tedlium/TEDLIUM_release-3/data/converted/wav/ScottKim_2008P_17.wav.wav',
       '/home/dzubke/awni_speech/data/tedlium/TEDLIUM_release-3/data/converted/wav/WillPotter_2015F_34.wav.wav',
       '/home/dzubke/awni_speech/data/tedlium/TEDLIUM_release-3/data/converted/wav/PaulWolpe_2010X_129.wav.wav',
       '/home/dzubke/awni_speech/data/tedlium/TEDLIUM_release-3/data/converted/wav/AmoryLovins_2012S_145.wav.wav',
       '/home/dzubke/awni_speech/data/tedlium/TEDLIUM_release-3/data/converted/wav/HallaTomasdottir_2016W_83.wav.wav',
       '/home/dzubke/awni_speech/data/tedlium/TEDLIUM_release-3/data/converted/wav/NataliePanek_2015X_40.wav.wav',
       '/home/dzubke/awni_speech/data/tedlium/TEDLIUM_release-3/data/converted/wav/JosephPine_2004_104.wav.wav',
       '/home/dzubke/awni_speech/data/tedlium/TEDLIUM_release-3/data/converted/wav/RobertFull_2014U_21.wav.wav',
       '/home/dzubke/awni_speech/data/tedlium/TEDLIUM_release-3/data/converted/wav/NormanFoster_2007P_52.wav.wav',
       '/home/dzubke/awni_speech/data/tedlium/TEDLIUM_release-3/data/converted/wav/RutgerBregman_2017_42.wav.wav']
303/94: len(ted_audio_choices)
303/95: dst_dir = os.path.join(LOCAL_DIR, "tedlium")
303/96: os.mkdir(dst_dir)
303/97: cmd_str = "gcloud compute scp "+GCP_USER_PROJECT+"{audio_file} "+dst_dir
303/98:
for audio_file in ted_audio_choices:
            os.system(cmd_str.format(audio_file=audio_file))
303/99:
lb_audio_choices =['/home/dzubke/awni_speech/data/LibriSpeech/train-other-500/5355/35477/5355-35477-0003.wav',
       '/home/dzubke/awni_speech/data/LibriSpeech/train-other-500/6087/50469/6087-50469-0029.wav',
       '/home/dzubke/awni_speech/data/LibriSpeech/train-other-500/8367/279367/8367-279367-0013.wav',
       '/home/dzubke/awni_speech/data/LibriSpeech/train-other-500/4576/66275/4576-66275-0039.wav',
       '/home/dzubke/awni_speech/data/LibriSpeech/train-other-500/5043/28396/5043-28396-0021.wav',
       '/home/dzubke/awni_speech/data/LibriSpeech/train-other-500/448/122800/448-122800-0053.wav',
       '/home/dzubke/awni_speech/data/LibriSpeech/train-other-500/4791/25379/4791-25379-0012.wav',
       '/home/dzubke/awni_speech/data/LibriSpeech/train-other-500/5628/47240/5628-47240-0007.wav',
       '/home/dzubke/awni_speech/data/LibriSpeech/train-other-500/622/128666/622-128666-0018.wav',
       '/home/dzubke/awni_speech/data/LibriSpeech/train-other-500/2262/141716/2262-141716-0017.wav',
       '/home/dzubke/awni_speech/data/LibriSpeech/train-other-500/4771/6409/4771-6409-0008.wav',
       '/home/dzubke/awni_speech/data/LibriSpeech/train-other-500/6724/73991/6724-73991-0037.wav',
       '/home/dzubke/awni_speech/data/LibriSpeech/train-other-500/1767/142932/1767-142932-0016.wav',
       '/home/dzubke/awni_speech/data/LibriSpeech/train-other-500/8445/281202/8445-281202-0052.wav',
       '/home/dzubke/awni_speech/data/LibriSpeech/train-other-500/4313/9937/4313-9937-0002.wav',
       '/home/dzubke/awni_speech/data/LibriSpeech/train-other-500/5933/73033/5933-73033-0011.wav',
       '/home/dzubke/awni_speech/data/LibriSpeech/train-other-500/104/132091/104-132091-0004.wav',
       '/home/dzubke/awni_speech/data/LibriSpeech/train-other-500/8543/280376/8543-280376-0016.wav',
       '/home/dzubke/awni_speech/data/LibriSpeech/train-other-500/3394/7597/3394-7597-0061.wav',
       '/home/dzubke/awni_speech/data/LibriSpeech/train-other-500/1444/133287/1444-133287-0019.wav',
       '/home/dzubke/awni_speech/data/LibriSpeech/train-other-500/3394/184333/3394-184333-0006.wav',
       '/home/dzubke/awni_speech/data/LibriSpeech/train-other-500/2346/152201/2346-152201-0050.wav',
       '/home/dzubke/awni_speech/data/LibriSpeech/train-other-500/8666/293943/8666-293943-0029.wav',
       '/home/dzubke/awni_speech/data/LibriSpeech/train-other-500/1274/121771/1274-121771-0007.wav',
       '/home/dzubke/awni_speech/data/LibriSpeech/train-other-500/8316/279798/8316-279798-0047.wav',
       '/home/dzubke/awni_speech/data/LibriSpeech/train-other-500/1051/133883/1051-133883-0035.wav',
       '/home/dzubke/awni_speech/data/LibriSpeech/train-other-500/7585/96245/7585-96245-0106.wav',
       '/home/dzubke/awni_speech/data/LibriSpeech/train-other-500/348/132611/348-132611-0037.wav',
       '/home/dzubke/awni_speech/data/LibriSpeech/train-other-500/1828/141203/1828-141203-0060.wav',
       '/home/dzubke/awni_speech/data/LibriSpeech/train-other-500/5671/24628/5671-24628-0029.wav',
       '/home/dzubke/awni_speech/data/LibriSpeech/train-other-500/3381/9921/3381-9921-0006.wav',
       '/home/dzubke/awni_speech/data/LibriSpeech/train-other-500/1708/142417/1708-142417-0017.wav',
       '/home/dzubke/awni_speech/data/LibriSpeech/train-other-500/3606/6850/3606-6850-0085.wav',
       '/home/dzubke/awni_speech/data/LibriSpeech/train-other-500/6009/57646/6009-57646-0010.wav',
       '/home/dzubke/awni_speech/data/LibriSpeech/train-other-500/791/127519/791-127519-0016.wav',
       '/home/dzubke/awni_speech/data/LibriSpeech/train-other-500/4712/93896/4712-93896-0021.wav',
       '/home/dzubke/awni_speech/data/LibriSpeech/train-other-500/1985/144468/1985-144468-0022.wav',
       '/home/dzubke/awni_speech/data/LibriSpeech/train-other-500/8169/118200/8169-118200-0031.wav',
       '/home/dzubke/awni_speech/data/LibriSpeech/train-other-500/4411/14501/4411-14501-0055.wav',
       '/home/dzubke/awni_speech/data/LibriSpeech/train-other-500/5979/42002/5979-42002-0013.wav',
       '/home/dzubke/awni_speech/data/LibriSpeech/train-other-500/6749/74575/6749-74575-0006.wav',
       '/home/dzubke/awni_speech/data/LibriSpeech/train-other-500/3885/183973/3885-183973-0030.wav',
       '/home/dzubke/awni_speech/data/LibriSpeech/train-other-500/8322/276748/8322-276748-0130.wav',
       '/home/dzubke/awni_speech/data/LibriSpeech/train-other-500/7131/92815/7131-92815-0034.wav',
       '/home/dzubke/awni_speech/data/LibriSpeech/train-other-500/7746/104993/7746-104993-0022.wav',
       '/home/dzubke/awni_speech/data/LibriSpeech/train-other-500/8587/295524/8587-295524-0023.wav',
       '/home/dzubke/awni_speech/data/LibriSpeech/train-other-500/6625/39674/6625-39674-0001.wav',
       '/home/dzubke/awni_speech/data/LibriSpeech/train-other-500/7265/74918/7265-74918-0054.wav',
       '/home/dzubke/awni_speech/data/LibriSpeech/train-other-500/4701/15046/4701-15046-0008.wav',
       '/home/dzubke/awni_speech/data/LibriSpeech/train-other-500/228/121761/228-121761-0038.wav']
303/100:
for audio_file in lb_audio_choices:
            os.system(cmd_str.format(audio_file=audio_file))
303/101: dst_dir = os.path.join(LOCAL_DIR, "librispeech")
303/102: os.mkdir(dst_dir)
303/103: cmd_str = "gcloud compute scp "+GCP_USER_PROJECT+"{audio_file} "+dst_dir
303/104:
for audio_file in lb_audio_choices:
            os.system(cmd_str.format(audio_file=audio_file))
303/105: from speech.dataset_info import VoxforgeDataset()
303/106:
vxf_audio_choices=['/home/dzubke/awni_speech/data/voxforge/archive/surgemcgee-20120722-jzb/wav/rb-36.wv',
       '/home/dzubke/awni_speech/data/voxforge/archive/TomM-20120224-xnp/wav/b0502.wv',
       '/home/dzubke/awni_speech/data/voxforge/archive/anonymous-20140819-qvh/wav/b0109.wv',
       '/home/dzubke/awni_speech/data/voxforge/archive/Primus-20150102-cfo/wav/b0484.wv',
       '/home/dzubke/awni_speech/data/voxforge/archive/grigi-20150812-sts/wav/b0389.wv',
       '/home/dzubke/awni_speech/data/voxforge/archive/anonymous-20100907-yla/wav/rb-16.wv',
       '/home/dzubke/awni_speech/data/voxforge/archive/anonymous-20120719-uja/wav/a0120.wv',
       '/home/dzubke/awni_speech/data/voxforge/archive/GaylandGGump-20141207-kew/wav/b0471.wv',
       '/home/dzubke/awni_speech/data/voxforge/archive/mcmurray02-20090312-oxw/wav/b0481.wv',
       '/home/dzubke/awni_speech/data/voxforge/archive/wicke-20090314-tdm/wav/a0307.wv',
       '/home/dzubke/awni_speech/data/voxforge/archive/junkong-20120118-dlm/wav/a0384.wv',
       '/home/dzubke/awni_speech/data/voxforge/archive/k-20090202-bzc/wav/b0235.wv',
       '/home/dzubke/awni_speech/data/voxforge/archive/cmu_com_kal_ldom/wav/com_4152.wv',
       '/home/dzubke/awni_speech/data/voxforge/archive/mitch-20130404-nxw/wav/a0068.wv',
       '/home/dzubke/awni_speech/data/voxforge/archive/robertburrelldonkin-20071007-vf23/wav/vf23-23.wv',
       '/home/dzubke/awni_speech/data/voxforge/archive/ralfherzog-20070910-en3/flac/en3-70.wv',
       '/home/dzubke/awni_speech/data/voxforge/archive/Rovanion-20100821-xsw/wav/b0468.wv',
       '/home/dzubke/awni_speech/data/voxforge/archive/rortiz-20160226-pux/wav/e0079.wv',
       '/home/dzubke/awni_speech/data/voxforge/archive/apdsqueaky-20151112-xve/wav/b0108.wv',
       '/home/dzubke/awni_speech/data/voxforge/archive/cmu_us_bdl_arctic/wav/arctic_a0592.wv',
       '/home/dzubke/awni_speech/data/voxforge/archive/cmu_us_jmk_arctic/wav/arctic_a0379.wv',
       '/home/dzubke/awni_speech/data/voxforge/archive/nikita-20110718-nww/wav/a0450.wv',
       '/home/dzubke/awni_speech/data/voxforge/archive/cmu_us_clb_arctic/wav/arctic_a0295.wv',
       '/home/dzubke/awni_speech/data/voxforge/archive/LunaTick-20080326-cc/wav/cc-9.wv',
       '/home/dzubke/awni_speech/data/voxforge/archive/_r2h-20100822-kmh/wav/b0286.wv',
       '/home/dzubke/awni_speech/data/voxforge/archive/nico-20110922-rso/wav/a0482.wv',
       '/home/dzubke/awni_speech/data/voxforge/archive/cmu_us_bdl_arctic/wav/arctic_b0233.wv',
       '/home/dzubke/awni_speech/data/voxforge/archive/cmu_us_bdl_arctic/wav/arctic_b0040.wv',
       '/home/dzubke/awni_speech/data/voxforge/archive/jaiger-vf20-20070220/wav/vf20-05.wv',
       '/home/dzubke/awni_speech/data/voxforge/archive/GaylandGGump-20141207-zey/wav/b0205.wv',
       '/home/dzubke/awni_speech/data/voxforge/archive/ralfherzog-20071126-en25/flac/en25-64.wv',
       '/home/dzubke/awni_speech/data/voxforge/archive/glenton-20090924-lpy/wav/a0231.wv',
       '/home/dzubke/awni_speech/data/voxforge/archive/ralfherzog-20070924-en9/flac/en9-151.wv',
       '/home/dzubke/awni_speech/data/voxforge/archive/tsmock-20120722-gsn/wav/b0164.wv',
       '/home/dzubke/awni_speech/data/voxforge/archive/anonymous-20131029-wcc/wav/b0359.wv',
       '/home/dzubke/awni_speech/data/voxforge/archive/ductapeguy-20070308b/wav/bab.0069.wv',
       '/home/dzubke/awni_speech/data/voxforge/archive/cmu_us_slt_arctic/wav/arctic_b0384.wv',
       '/home/dzubke/awni_speech/data/voxforge/archive/anonymous-20120116-qkw/wav/b0360.wv',
       '/home/dzubke/awni_speech/data/voxforge/archive/cmu_us_rms_arctic/wav/arctic_a0418.wv',
       '/home/dzubke/awni_speech/data/voxforge/archive/anonymous-20111029-rrz/wav/b0196.wv',
       '/home/dzubke/awni_speech/data/voxforge/archive/anonymous-20101224-lxx/wav/a0063.wv',
       '/home/dzubke/awni_speech/data/voxforge/archive/anonymous-20110208-iki/wav/b0119.wv',
       '/home/dzubke/awni_speech/data/voxforge/archive/klaus-20120711-crf/wav/b0414.wv',
       '/home/dzubke/awni_speech/data/voxforge/archive/DanielHeath-20090117-rmo/wav/rb-35.wv',
       '/home/dzubke/awni_speech/data/voxforge/archive/soulphox-20100203-mmg/wav/a0328.wv',
       '/home/dzubke/awni_speech/data/voxforge/archive/peterwhy-20080503-win/wav/win0037.wv',
       '/home/dzubke/awni_speech/data/voxforge/archive/ralfherzog-20070914-en5/flac/en5-15.wv',
       '/home/dzubke/awni_speech/data/voxforge/archive/rortiz-20090504/wav/cc-3.wv',
       '/home/dzubke/awni_speech/data/voxforge/archive/anonymous-20120603-xzu/wav/a0076.wv',
       '/home/dzubke/awni_speech/data/voxforge/archive/dave-20090801-bmc/wav/a0268.wv']
303/107: dst_dir = os.path.join(LOCAL_DIR, "voxforge")
303/108: os.mkdir(dst_dir)
303/109: cmd_str = "gcloud compute scp "+GCP_USER_PROJECT+"{audio_file} "+dst_dir
303/110:
for audio_file in vxf_audio_choices:
            os.system(cmd_str.format(audio_file=audio_file))
303/111:
tat_audio_choices=['/home/dzubke/awni_speech/data/voxforge/archive/cmu_us_ksp_arctic/wav/arctic_a0445.wv',
       '/home/dzubke/awni_speech/data/voxforge/archive/chocoholic-20080420-pos/wav/eti0178.wv',
       '/home/dzubke/awni_speech/data/voxforge/archive/jaiger-20070103-vf9/wav/vf9-37.wv',
       '/home/dzubke/awni_speech/data/voxforge/archive/anonymous-20090406-evj/wav/a0209.wv',
       '/home/dzubke/awni_speech/data/voxforge/archive/starlite-20070613-fur6/wav/fur0214.wv',
       '/home/dzubke/awni_speech/data/voxforge/archive/farmerjack-20080917-agt/wav/a0116.wv',
       '/home/dzubke/awni_speech/data/voxforge/archive/anonymous-20080623-suj/wav/b0094.wv',
       '/home/dzubke/awni_speech/data/voxforge/archive/kayray-20070611-leo/wav/leo0012.wv',
       '/home/dzubke/awni_speech/data/voxforge/archive/osman-20131010-lwh/wav/a0406.wv',
       '/home/dzubke/awni_speech/data/voxforge/archive/ColinBeckingham-20091121-grq/wav/ar-18.wv',
       '/home/dzubke/awni_speech/data/voxforge/archive/bonzer-20090919-ccj/wav/a0025.wv',
       '/home/dzubke/awni_speech/data/voxforge/archive/riban-20100201-ebw/wav/a0519.wv',
       '/home/dzubke/awni_speech/data/voxforge/archive/ShingoTamai-20080918-wkt/wav/b0400.wv',
       '/home/dzubke/awni_speech/data/voxforge/archive/anonymous-20100130-nym/wav/b0411.wv',
       '/home/dzubke/awni_speech/data/voxforge/archive/cmu_com_kal_ldom/wav/com_4053.wv',
       '/home/dzubke/awni_speech/data/voxforge/archive/ndkoskey-20130605-dlh/wav/a0454.wv',
       '/home/dzubke/awni_speech/data/voxforge/archive/pcsnpny-20150321-vkj/wav/b0066.wv',
       '/home/dzubke/awni_speech/data/voxforge/archive/anonymous-20131221-urh/wav/a0038.wv',
       '/home/dzubke/awni_speech/data/voxforge/archive/gravity-20080309-mms/wav/a0147.wv',
       '/home/dzubke/awni_speech/data/voxforge/archive/anonymous-20100719-vdp/wav/b0444.wv',
       '/home/dzubke/awni_speech/data/voxforge/archive/anonymous-20120108-pso/wav/b0147.wv',
       '/home/dzubke/awni_speech/data/voxforge/archive/starlite-20070613-fur6/wav/fur0048b.wv',
       '/home/dzubke/awni_speech/data/voxforge/archive/sfauzia-20100731-cqd/wav/a0314.wv',
       '/home/dzubke/awni_speech/data/voxforge/archive/anonymous-20100928-twd/wav/a0175.wv',
       '/home/dzubke/awni_speech/data/voxforge/archive/anonymous-20090731-tfj/wav/b0384.wv',
       '/home/dzubke/awni_speech/data/voxforge/archive/anonymous-20140729-ipn/wav/b0072.wv',
       '/home/dzubke/awni_speech/data/voxforge/archive/maheshchandra-20160719-com/wav/sentence69.wv',
       '/home/dzubke/awni_speech/data/voxforge/archive/robin-20070401-vf19/wav/vf19-01.wv',
       '/home/dzubke/awni_speech/data/voxforge/archive/cmu_us_awb_arctic/wav/arctic_b0013.wv',
       '/home/dzubke/awni_speech/data/voxforge/archive/bjb-20120325/flac/vf17-17.wv',
       '/home/dzubke/awni_speech/data/voxforge/archive/chrisspen-20120505-inl/wav/b0176.wv',
       '/home/dzubke/awni_speech/data/voxforge/archive/anonymous-20110917-pdb/wav/a0022.wv',
       '/home/dzubke/awni_speech/data/voxforge/archive/anonymous-20110718-zqg/wav/a0240.wv',
       '/home/dzubke/awni_speech/data/voxforge/archive/cmu_us_bdl_arctic/wav/arctic_b0539.wv',
       '/home/dzubke/awni_speech/data/voxforge/archive/cmu_us_slt_arctic/wav/arctic_a0029.wv',
       '/home/dzubke/awni_speech/data/voxforge/archive/anonymous-20120504-gmt/wav/a0268.wv',
       '/home/dzubke/awni_speech/data/voxforge/archive/AslakKnutsen-20140315-tkd/wav/a0196.wv',
       '/home/dzubke/awni_speech/data/voxforge/archive/MSypkensSmit-20080802-xda/wav/b0521.wv',
       '/home/dzubke/awni_speech/data/voxforge/archive/bloomtom-20080612-mvm/wav/b0167.wv',
       '/home/dzubke/awni_speech/data/voxforge/archive/delibab-20071029/wav/md0007.wv',
       '/home/dzubke/awni_speech/data/voxforge/archive/anonymous-20100609-cuw/wav/a0422.wv',
       '/home/dzubke/awni_speech/data/voxforge/archive/rortiz-20121211-wke/wav/a0345.wv',
       '/home/dzubke/awni_speech/data/voxforge/archive/knotyouraveragejo-20080524-mtn/wav/mtn0089.wv',
       '/home/dzubke/awni_speech/data/voxforge/archive/kayray-20070611-ele/wav/ele0129.wv',
       '/home/dzubke/awni_speech/data/voxforge/archive/camdixon-20141207-kkp/wav/b0087.wv',
       '/home/dzubke/awni_speech/data/voxforge/archive/jaiger-20070209-vf12/wav/vf12-15.wv',
       '/home/dzubke/awni_speech/data/voxforge/archive/anonymous-20090927-soe/wav/b0535.wv',
       '/home/dzubke/awni_speech/data/voxforge/archive/farmerjack-20080915-aeh/wav/a0240.wv',
       '/home/dzubke/awni_speech/data/voxforge/archive/anonymous-20081015-rdx/wav/b0080.wv',
       '/home/dzubke/awni_speech/data/voxforge/archive/glenton-20090924-iaz/wav/b0075.wv']
303/112: dst_dir = os.path.join(LOCAL_DIR,"tatoeba")
303/113: os.mkdir(dst_dir)
303/114: cmd_str = "gcloud compute scp "+GCP_USER_PROJECT+"{audio_file} "+dst_dir
303/115:
        for audio_file in tat_audio_choices:
            os.system(cmd_str.format(audio_file=audio_file))
303/116: tat_audio_choices
303/117:
tat_audio_choices=['/home/dzubke/awni_speech/data/tatoeba/tatoeba_audio_eng/audio/CK/3469951.wv',
       '/home/dzubke/awni_speech/data/tatoeba/tatoeba_audio_eng/audio/CK/2539779.wv',
       '/home/dzubke/awni_speech/data/tatoeba/tatoeba_audio_eng/audio/CK/288502.wv',
       '/home/dzubke/awni_speech/data/tatoeba/tatoeba_audio_eng/audio/CK/2955897.wv',
       '/home/dzubke/awni_speech/data/tatoeba/tatoeba_audio_eng/audio/CK/1895753.wv',
       '/home/dzubke/awni_speech/data/tatoeba/tatoeba_audio_eng/audio/CK/2544359.wv',
       '/home/dzubke/awni_speech/data/tatoeba/tatoeba_audio_eng/audio/CK/2548391.wv',
       '/home/dzubke/awni_speech/data/tatoeba/tatoeba_audio_eng/audio/CK/3825457.wv',
       '/home/dzubke/awni_speech/data/tatoeba/tatoeba_audio_eng/audio/CK/3098912.wv',
       '/home/dzubke/awni_speech/data/tatoeba/tatoeba_audio_eng/audio/CK/5272940.wv',
       '/home/dzubke/awni_speech/data/tatoeba/tatoeba_audio_eng/audio/CK/16950.wv',
       '/home/dzubke/awni_speech/data/tatoeba/tatoeba_audio_eng/audio/CK/2016571.wv',
       '/home/dzubke/awni_speech/data/tatoeba/tatoeba_audio_eng/audio/CK/2497792.wv',
       '/home/dzubke/awni_speech/data/tatoeba/tatoeba_audio_eng/audio/CK/2272688.wv',
       '/home/dzubke/awni_speech/data/tatoeba/tatoeba_audio_eng/audio/CK/1092159.wv',
       '/home/dzubke/awni_speech/data/tatoeba/tatoeba_audio_eng/audio/CK/2276675.wv',
       '/home/dzubke/awni_speech/data/tatoeba/tatoeba_audio_eng/audio/CK/6090797.wv',
       '/home/dzubke/awni_speech/data/tatoeba/tatoeba_audio_eng/audio/CK/288872.wv',
       '/home/dzubke/awni_speech/data/tatoeba/tatoeba_audio_eng/audio/CK/1025002.wv',
       '/home/dzubke/awni_speech/data/tatoeba/tatoeba_audio_eng/audio/CK/1092800.wv',
       '/home/dzubke/awni_speech/data/tatoeba/tatoeba_audio_eng/audio/CK/1093373.wv',
       '/home/dzubke/awni_speech/data/tatoeba/tatoeba_audio_eng/audio/CK/3313172.wv',
       '/home/dzubke/awni_speech/data/tatoeba/tatoeba_audio_eng/audio/CK/1893763.wv',
       '/home/dzubke/awni_speech/data/tatoeba/tatoeba_audio_eng/audio/CK/3738046.wv',
       '/home/dzubke/awni_speech/data/tatoeba/tatoeba_audio_eng/audio/CK/6102337.wv',
       '/home/dzubke/awni_speech/data/tatoeba/tatoeba_audio_eng/audio/CK/3619571.wv',
       '/home/dzubke/awni_speech/data/tatoeba/tatoeba_audio_eng/audio/CK/3736858.wv',
       '/home/dzubke/awni_speech/data/tatoeba/tatoeba_audio_eng/audio/CK/2545926.wv',
       '/home/dzubke/awni_speech/data/tatoeba/tatoeba_audio_eng/audio/CK/2954573.wv',
       '/home/dzubke/awni_speech/data/tatoeba/tatoeba_audio_eng/audio/CK/3826544.wv',
       '/home/dzubke/awni_speech/data/tatoeba/tatoeba_audio_eng/audio/CK/6126245.wv',
       '/home/dzubke/awni_speech/data/tatoeba/tatoeba_audio_eng/audio/CK/4662744.wv',
       '/home/dzubke/awni_speech/data/tatoeba/tatoeba_audio_eng/audio/CK/2375957.wv',
       '/home/dzubke/awni_speech/data/tatoeba/tatoeba_audio_eng/audio/CK/5853072.wv',
       '/home/dzubke/awni_speech/data/tatoeba/tatoeba_audio_eng/audio/CK/3619520.wv',
       '/home/dzubke/awni_speech/data/tatoeba/tatoeba_audio_eng/audio/CK/1825289.wv',
       '/home/dzubke/awni_speech/data/tatoeba/tatoeba_audio_eng/audio/CK/4494964.wv',
       '/home/dzubke/awni_speech/data/tatoeba/tatoeba_audio_eng/audio/CK/2361959.wv',
       '/home/dzubke/awni_speech/data/tatoeba/tatoeba_audio_eng/audio/CK/2538187.wv',
       '/home/dzubke/awni_speech/data/tatoeba/tatoeba_audio_eng/audio/CK/3023876.wv',
       '/home/dzubke/awni_speech/data/tatoeba/tatoeba_audio_eng/audio/CK/4496153.wv',
       '/home/dzubke/awni_speech/data/tatoeba/tatoeba_audio_eng/audio/CK/3735118.wv',
       '/home/dzubke/awni_speech/data/tatoeba/tatoeba_audio_eng/audio/CK/2545387.wv',
       '/home/dzubke/awni_speech/data/tatoeba/tatoeba_audio_eng/audio/CK/2543053.wv',
       '/home/dzubke/awni_speech/data/tatoeba/tatoeba_audio_eng/audio/CK/2647469.wv',
       '/home/dzubke/awni_speech/data/tatoeba/tatoeba_audio_eng/audio/CK/2123449.wv',
       '/home/dzubke/awni_speech/data/tatoeba/tatoeba_audio_eng/audio/CK/5565240.wv',
       '/home/dzubke/awni_speech/data/tatoeba/tatoeba_audio_eng/audio/CK/4759315.wv',
       '/home/dzubke/awni_speech/data/tatoeba/tatoeba_audio_eng/audio/CK/253458.wv',
       '/home/dzubke/awni_speech/data/tatoeba/tatoeba_audio_eng/audio/CK/2281726.wv']
303/118:
for audio_file in ta_audio_choices:
            os.system(cmd_str.format(audio_file=audio_file))
303/119:
for audio_file in tat_audio_choices:
            os.system(cmd_str.format(audio_file=audio_file))
305/1: import apache_beam.runners.dataflow.internal
306/1: import apache_beam.runners.dataflow.internal
305/2: import apitools.base.py
307/1: import torch
307/2: :q
308/1: import torch
309/1: import torch
310/1: import torch
311/1: import sys
311/2: print(sys.path)
313/1: 1 == True
314/1: import time
314/2: time.now()
314/3: time.time.now()
314/4: import datetiem
314/5: import datetime
314/6: from datetime import date
314/7: date.today()
314/8: str1=date.today()+"_hello"
314/9: str1=str(date.today())+"_hello"
314/10: print(str1)
314/11: dataset_dir="../data/"
314/12: entso_df = pd.read_csv(prim_data_path)
314/13: import pandas as pd
314/14: entso_df = pd.read_csv(prim_data_path)
314/15: entso_path = os.path.join(dataset_dir, "entso.csv")
314/16: import os
314/17: entso_path = os.path.join(dataset_dir, "entso.csv")
314/18: entso_df = pd.read_csv(entso_path)
314/19: ls
314/20: cd src/
314/21: entso_df = pd.read_csv(entso_path)
314/22: entso_df.colummns
314/23: entso_df.columns
314/24: entso_df.str.lower()
314/25:
for series in entso_df:
    print(series.head)
314/26:
for series in entso_df:
    print(series)
314/27:
for series in entso_df:
    print(df.series)
314/28:
for series in entso_df:
    print(entso_df.series)
314/29:
for series in entso_df:
    print(entso_df[series].head())
314/30:
for series in entso_df:
    print(entso_df[series].dtype)
314/31:
for series in entso_df:
    print(series)
314/32:
for series in entso_df:
    print(entso_df[series].dtype)
314/33:
for series in entso_df:
    if entso_df[series].dtype==object:
        print(series)
314/34:
for series in entso_df:
    if entso_df[series].dtype==object:
        print(entso_df[series].str.lower.head())
314/35:
for series in entso_df:
    if entso_df[series].dtype==object:
        print(entso_df[series].str.lower().head())
314/36: entso_df.headd()
314/37: entso_df.head()
314/38: entso_df.country.head()
314/39: entso_df.country.str.replace("\([A-Z][A-Z]\)", '').head()
314/40: entso_df.country.str.replace("\([A-Z][A-Z]\)", '').strip().head()
314/41: entso_df.country.str.replace("\([A-Z][A-Z]\)", '').str.strip().head()
314/42: entso_df["country"]
314/43: platts_df.head()
314/44: platts_path = os.path.join(dataset_dir, "platts.csv")
314/45: platts_df = pd.read_csv(platts_path)
314/46:
    merged_df = entso_df.merge(platts_df,
        left_on=['country'], right_on=['COUNTRY'], how='left')
314/47: merged_df.shape
314/48: entso_df.shape
314/49: platts_df.shape
314/50:
    merged_df = entso_df.merge(platts_df,
        left_on='country', right_on='COUNTRY', how='left')
314/51: merged_df.shape
314/52:
    merged_df = entso_df.merge(platts_df,
        left_on='country', right_on='COUNTRY', how='inner')
314/53: merged_df.shape
314/54:
    merged_df = entso_df.merge(platts_df,
        left_on='country', right_on='COUNTRY', how='left')
314/55: merged_df.head()
314/56: import csv
314/57:     fuel_thes_path = os.path.join(dataset_dir, "fuel_thesaurus.csv")
314/58:
with open(fuel_thes_path, 'r') as fid:
    reader = csv.DictReader(fid, fieldnames='unit_fuel_platts_entsoe')
    for line in reader:
        print(line)
314/59:
with open(fuel_thes_path, 'r') as fid:
    reader = csv.Reader(fid)
    for line in reader:
        print(line)
314/60:
with open(fuel_thes_path, 'r') as fid:
    reader = csv.reader(fid)
    for line in reader:
        print(line)
314/61:
with open(fuel_thes_path, 'r') as fid:
    reader = csv.reader(fid)
    for line in reader:
        print(line)
314/62:
with open(fuel_thes_path, 'r') as fid:
    reader = csv.reader(fid, delimiter=',')
    for line in reader:
        print(line)
314/63:
with open(fuel_thes_path, 'r') as fid:
    reader = csv.reader(fid, delimiter=',')
    header = next(reader)
    for line in reader:
        print(line)
314/64:
with open(fuel_thes_path, 'r') as fid:
    reader = csv.reader(fid, delimiter=',')
    header = next(reader)
    dct = dict()
    for line in reader:
        key=line[0]
        value=line[1]
        dct.update((key,value))
    print(dct)
314/65:
with open(fuel_thes_path, 'r') as fid:
    reader = csv.reader(fid, delimiter=',')
    header = next(reader)
    dct = dict()
    for line in reader:
        key=line[0]
        value=line[1]
        dct.update({key:value})
    print(dct)
314/66:
with open(fuel_thes_path, 'r') as fid:
    reader = csv.reader(fid, delimiter=',')
    header = next(reader)
    dct = dict()
    for line in reader:
        key=line[0].lower().strip()
        value=line[1].lower().strip()
        dct.update({key:value})
    print(dct)
314/67: import edit_distance as ed
314/68: plant="Kraftwerk Voerde Block B"
314/69: temp = "LIPPENDORF VEAG A"
314/70: plant=plant.lower()
314/71: temp=temp.lower()
314/72: ed.eval(plant,temp)
314/73: import editdistance as ed
314/74: import editdistance as ed
314/75: ed.eval(plant,temp)
314/76: temp_2 = "LIPPENDORF VEAG IKW"
314/77: temp = "LIPPENDORF VEAG IKW"
314/78: ed.eval(plant,temp.lower())
314/79: temp.lower()
314/80: temp = LIPPENDORF VEAG A
314/81: temp = "LIPPENDORF VEAG A"
314/82: ed.eval(plant,temp.lower())
314/83: temp ="LIPPENDORF-R"
314/84: ed.eval(plant,temp.lower())
314/85: temp="LIPPENDORF-S"
314/86: ed.eval(plant,temp.lower())
314/87: plant
314/88: plant="KW Lippendorf Block R"
314/89: plant=plant.lower()
314/90: ed.eval(plant,temp.lower())
314/91: temp ="LIPPENDORF-R"
314/92: ed.eval(plant,temp.lower())
314/93: temp = "LIPPENDORF VEAG A"
314/94: ed.eval(plant,temp.lower())
314/95: temp = "LIPPENDORF VEAG IKW"
314/96: ed.eval(plant,temp.lower())
314/97: temp = "LIPPENDORF VEAG C"
314/98: ed.eval(plant,temp.lower())
314/99: plant = "GDK-Mellach Linie 20"
314/100: plant = plant.lower()
314/101: entso_df
314/102: lt(5,3)
314/103: entso_df["unit_capacity_mw" > "plant_capacity_mw"]
314/104: entso_df[("unit_capacity_mw" > "plant_capacity_mw")]
314/105: entso_df.apply(lambda row: row.unit_capacity_mw > row.plant_capacity_mw, axis=1)
314/106: entso_df.apply(lambda row: row.unit_capacity_mw > row.plant_capacity_mw, axis=1).sum()
314/107: ed.eval(" ", "  ")
314/108: entso_df.unit_name.replace("-_", '').str.strip()
314/109: entso_df.unit_name.replace("_", '').str.strip()
314/110: entso_df.unit_name.replace("_", '').str.strip()
314/111: entso_df.unit_name.replace("*_*", '').str.strip()
314/112: entso_df.unit_name.replace("\*_\*", '').str.strip()
314/113: entso_df.unit_name.replace("\*_\*", '', regex=True).str.strip()
314/114: entso_df.unit_name.replace("*_*", '', regex=True).str.strip()
314/115: entso_df.unit_name.replace("_", '', regex=True)
314/116: entso_df.unit_name.replace("-_", '', regex=True)
314/117: entso_df.unit_name.replace("[-_]", '', regex=True)
314/118: entso_df.unit_name.replace("[-_ ]", '', regex=True)
314/119: entso_df.unit_name.replace("[-_ ]", '', regex=True)    country_pattern = "\([a-z][a-z]\)"
314/120: country_pattern = "\([a-z][a-z]\)"
314/121:
for row in entso_df.itertuples():
    if row.unit_name == "Ingolstadt 3":
        print(row)
314/122: new_df= pd.DataFrame
314/123:
for row in entso_df.itertuples():
    if row.unit_name == "Ingolstadt 3":
        new_df.append(row)
    print(new_df)
314/124:
for row in entso_df.itertuples():
    if row.unit_name == "Ingolstadt 3":
        new_df = new_df.append(row)
    print(new_df)
314/125: new_df
314/126:
for row in entso_df.itertuples():
    if row.unit_name == "Ingolstadt 3":
        new_df = new_df.append(row)
    print(new_df)
314/127: new_df
314/128: new_df.append([1,2,3])
314/129:
for row in entso_df.itertuples():
    if row.unit_name == "Ingolstadt 3":
        new_df = new_df.append(row, ignore_index=True)
    print(new_df)
314/130:
for row in entso_df.itertuples():
    if row.unit_name == "Ingolstadt 3":
        pd.concat(new_df, row)
    print(new_df)
314/131:
for row in entso_df.itertuples():
    if row.unit_name == "Ingolstadt 3":
        print(row)
    print(new_df)
314/132:
for row in entso_df.itertuples():
    if row.unit_name == "Ingolstadt 3":
        print(row)
print(new_df)
314/133:
for row in entso_df.itertuples():
    if row.unit_name == "Ingolstadt 3":
        print(type(row))
print(new_df)
314/134:
for row in entso_df.itertuples():
    if row.unit_name == "Ingolstadt 3":
        print(type(pd.DataFrame(row)))
print(new_df)
314/135:
for row in entso_df.itertuples():
    if row.unit_name == "Ingolstadt 3":
        print(pd.DataFrame(row))
print(new_df)
314/136:
for row in entso_df.itertuples():
    if row.unit_name == "Ingolstadt 3":
        new_df = new_df.append(pd.DataFrame(row))
print(new_df)
314/137: new_df=pd.DataFrame()
314/138:
for row in entso_df.itertuples():
    if row.unit_name == "Ingolstadt 3":
        new_df = new_df.append(pd.DataFrame(row))
print(new_df)
314/139:
for row in entso_df.itertuples():
    if row.unit_name.isin(new_df.unit_name):
        row.unit_name
print(new_df)
314/140:
for row in entso_df.itertuples():
    if new_df.unit_name.isin(row.unit_name):
        row.unit_name
print(new_df)
314/141: entso_df.columns
314/142: new_df = pd.DataFrame(columns=entso_df.columns)
314/143:
for row in entso_df.itertuples():
    if row.unit_name == "Ingolstadt 3":
        new_df = new_df.append(pd.DataFrame(row))
print(new_df)
314/144: new_df = pd.DataFrame(columns=entso_df.columns)
314/145:
for row in entso_df.itertuples():
    if row.unit_name == "Ingolstadt 3":
        new_df = new_df.append(pd.DataFrame(row), ignore_index=True)
print(new_df)
314/146: new_df = pd.DataFrame(columns=entso_df.columns)
314/147: new_df
314/148:
for row in entso_df.itertuples():
    if row.unit_name == "Ingolstadt 3":
        new_df = new_df.append(row, ignore_index=True)
print(new_df)
314/149:
for row in entso_df.iterrows():
    if row.unit_name == "Ingolstadt 3":
        new_df = new_df.append(row, ignore_index=True)
print(new_df)
314/150:
for row in entso_df.iterrows():
    if row[4] == "Ingolstadt 3":
        new_df = new_df.append(row, ignore_index=True)
print(new_df)
314/151:
for index, row in entso_df.iterrows():
    if row[4] == "Ingolstadt 3":
        new_df = new_df.append(row, ignore_index=True)
print(new_df)
314/152: new_df
314/153:
for index, row in entso_df.iterrows():
    if row[4] == "Ingolstadt 3":
        print(type(row))
print(new_df)
314/154:
for index, row in entso_df.iterrows():
    if row[4] == "Ingolstadt 3":
        print(row)
print(new_df)
314/155:
for row in entso_df.iterrows():
    if row.unit_name == "Ingolstadt 3":
        print(row)
        print(type(row))
print(new_df)
314/156:
for row in entso_df.itertuples():
    if row.unit_name == "Ingolstadt 3":
        print(row)
        print(type(row))
print(new_df)
314/157:
for row in entso_df.itertuples():
    if row.unit_name == "Ingolstadt 3":
        print(row)
        print(pd.Series(row))
print(new_df)
314/158: new_df
314/159: new_df.unit_name.isin("Ingolstadt 3")
314/160: new_df.unit_name.isin(["Ingolstadt 3"])
314/161: new_df.index
314/162: new_df.indexs
314/163: new_df.unit_name.isin(["Ingolstadt 3"])
314/164: type(new_df.unit_name.isin(["Ingolstadt 3"]))
314/165:
if new_df.unit_name.isin(["Ingolstadt 3"]):
    print("hello")
314/166:
if new_df.unit_name.isin(["Ingolstadt 3"]).values():
    print("hello")
314/167:
if new_df.unit_name.isin(["Ingolstadt 3"]).values:
    print("hello")
314/168: any(new_df.unit_name.isin(["Ingolstadt 3"]))
314/169: "unit_name" in entso_df
314/170: entso_df["unit_name"].dtype
314/171: entso_df["unit_fuel"].dtype
314/172: entso_df["unit_capacity_mw"].dtype
314/173: entso_df["unit_fuel"].dtype=='O'
314/174: isinstance(entso_df["unit_fuel"].dtype,object)
314/175: entso_df["unit_fuel"].dtype==object
314/176:     country_pattern = "\([a-z][a-z]\)"
314/177: entso_df.country = entso_df.country.str.replace(country_pattern, '').str.strip()
314/178: entso_df.country.str.replace(country_pattern, '').str.strip().head()
314/179:     country_pattern = "\([A-Z][A-Z]\)"
314/180: entso_df.country.str.replace(country_pattern, '').str.strip().head()
314/181: entso_df.country.str.replace(country_pattern, '', regex=True).str.strip().head()
314/182:     country_pattern = "\([a-z][a-z]\)"
314/183: entso_df.country.str.replace(country_pattern, '', regex=True).str.strip().head()
314/184:     today = str(date.today())
314/185: file=f"../result/{today}_mapping_drz.csv"
314/186: os.path.basename(args.output_csv)
314/187: os.path.basename(file)
314/188: os.path.dirname(file)
313/2: from tempfile import NamedTemporaryFile
313/3: audio_path="/Users/dustin/CS/consulting/firstlayerai/data/dataset_samples/librispeech/348-132611-0037.wav"
313/4: import os
313/5:
with NamedTemporaryFile(suffix=".wav") as tar_file: 
    tar_filename = tar_file.name 
    sox_params = "sox \"{}\" -r {} -c 1 -b 16 -e si {}".format(audio_path,16000, tar_filename) 
    os.system(sox_params)
    print(os.path.exists(tar_filename))
313/6: (10-5) / 2
313/7: (10-5) // 2
313/8: (9-5) // 3
313/9: (5) // 3
313/10: round(5.5)
313/11: round(5.3)
313/12: round(5.45)
313/13: import numpy as np
313/14: np.zeros((100,))
313/15: np.zeros((100,)).shape
313/16: type(np.zeros((100,)))
313/17: noise_dst=np.zeros((100,))
313/18: noise_energy = np.sqrt(noise_dst.dot(noise_dst) / noise_dst.size)
313/19: noise_energy
313/20: noise_energy == 0
313/21: noise_energy != 0
315/1: import numpy as np
316/1: import numpy as np
313/22: import time
313/23: time.sleep(1)
313/24:
try:
    for i in range(10):
        print(i)
        time.sleep(1)
except KeyboardInterrupt:
    print("done")
313/25: np
313/26: np.concat(np.zeros((10,)), np.zeros((100,)))
313/27: np.concatenate(np.zeros((10,)), np.zeros((100,)))
313/28: np.concatenate((np.zeros((10,)), np.zeros((100,))))
313/29: import time
313/30: time.time()
313/31: time.time()
313/32: np
313/33: buffer = b'\x9b\x01`\x01\xf8\x00\xa4\x00\x82\x00y\x00i\x00Z\x00k\x00\x8e\x00\xd0\x00\xcc\x00p\x00\x04\x00\xaf\xffb\xffe\xff\xb6\xff\x01\x00T\x00{\x00H\x00\xc7\xffO\xff\x0c\xff\x0e\xffv\xff\xe3\xff0\x00l\x00u\x001\x00\xd9\xff\xd3\xff\xe2\xff\xec\xff\xe4\xff\xf3\xff\x04\x00\x16\x00~\x00\xd8\x00\xd7\x00V\x00\xbc\xfft\xff\x85\xff\xce\xff\x0b\x00`\x00r\x00\x02\x00\x90\xff\x85\xff\xd6\xff\x1a\x00e\x00\x84\x00=\x00\xc9\xff\x93\xff\xd0\xff\x11\x00N\x00i\x00\x89\x00w\x00!\x00\xfc\xff\xca\xff\xa6\xff\xa1\xff\xea\xff\xfc\xff\xd0\xff\xe8\xff\x14\x00N\x00<\x00\x00\x00\xae\xffj\xff\x89\xff\xb1\xff\x06\x00\x9d\x00\xee\x00\xe4\x00\xb8\x00R\x00\xde\xff\xc5\xff\x18\x00B\x00J\x00\x81\x00u\x00:\x00i\x00\xdb\x00\xc2\x00C\x00\x01\x00\x0e\x00H\x00\x95\x00\x87\x00%\x00\x0b\x00,\x00,\x00\xf6\xff\xd3\xff\xd9\xff\xe0\xff\xf3\xff\xc9\xff\x91\xff\xae\xff\xd3\xff\xbd\xffo\xff[\xff\xcd\xff\x86\x00\xe4\x00\x89\x00\xd0\xffc\xff\x9c\xff\xcd\xff\x93\xff^\xff\xad\xff:\x00m\x00m\x003\x00\xea\xff\xa7\xffP\xff\x15\xff#\xff\x9d\xff\x16\x00i\x00e\x00\x04\x00\x93\xffg\xff\x9a\xff\x97\xff\x03\xff\x91\xfe\xbf\xfe-\xffo\xffG\xff\x07\xff\xa0\xfen\xfe\xa1\xfe\xce\xfe\xb8\xfe\x82\xfe\xd2\xfeB\xffG\xff\x0b\xff\xde\xfe\x10\xffo\xff\xbf\xff\xdc\xff\xce\xff\xbc\xff\xdc\xff\xf8\xff\xaf\xffB\xff\x0b\xffq\xff\xf0\xff\xf6\xff\xb2\xff\xa5\xff\xe4\xff\xf4\xff\xc0\xff/\xff}\xfe=\xfe\xa9\xfes\xff\xfe\xffD\x00L\x00\xe2\xff\xff\xfe:\xfe\xed\xfd/\xfe\xf0\xfe\x8b\xff\x80\xff"\xff\r\xff8\xffT\xff\x15\xff\x85\xfe\xee\xfd\xfd\xfd\x90\xfe+\xff\xb1\xff\x14\x00O\x00\x17\x00\xb7\xff<\xff\t\xff\x9d\xff\x85\x00\xf1\x00\xc2\x00\x8a\x00P\x00\r\x00\xf2\xff\xb8\xffz\xff\xc0\xffH\x00\x89\x00\x86\x00\x8b\x00\xb9\x00\xbe\x00\xb7\x00\xa9\x00\x9d\x00\xba\x00\xe8\x00\x11\x01\r\x01\x1d\x01\x0c\x01\x7f\x00\xb9\xff\x02\xff\xa1\xfe\x1e\xff\x00\x00\xa2\x00\xb9\x00b\x00\x07\x00\xd0\xff\xe6\xff\x13\x00?\x00U\x00{\x00\xc8\x00\xfc\x00\x03\x01\x97\x00\xcd\xff\x1c\xff\xb6\xfe\xc1\xfe\x1a\xff\xa4\xfff\x00\x02\x01\x01\x01\x90\x00&\x00\x00\x00)\x00z\x00\xd1\x00\x08\x019\x01~\x01x\x01\xde\x00G\x00\xd4\xff\x89\xff\x96\xff\xc7\xff\x01\x00 \x00S\x00`\x00M\x00"\x00\xc1\xff\xb0\xff\xfd\xff^\x00Q\x00\x11\x009\x00n\x00-\x00\x7f\xff\x05\xff\xef\xfeB\xff\xb4\xff\xc1\xffy\xff\xde\xfe\x89\xfe\xde\xfe#\xff\xe6\xfe'
313/34: frame = np.frombuffer(buffer, np.int16)
313/35: frame
313/36: int("\x9b")
313/37: int(b"\x9b")
313/38: int("\x9b", 16)
313/39: b'\x9b'.decode('utf-8')
313/40: b'\x9b'.decode('utf-8', 'strict')
313/41: b'x9b'.decode('utf-8', 'strict')
313/42: b'\x9b'.decode('utf-8', 'ignore')
313/43: '411'.encode()
313/44: import time
313/45: s = time.time()
313/46:
for i in range(1):
    s = time.time()
    time.sleep(1)
    print(time.time() - s)
313/47: frame.shape
313/48: np.frombuffer(buffer, np.int16, -1).shape
313/49: np.frombuffer(buffer, np.int16, 'h').shape
313/50: np.frombuffer(b'one two three ','b',14)
313/51: np.frombuffer(b'one two three ','b',14).shape
313/52: np.frombuffer(b'one two three ','b',-1).shape
313/53: np.frombuffer(b'one two three ','h',-1).shape
313/54: np.frombuffer(buffer, 'b', -1).shape
313/55: np.frombuffer(buffer, 'h', -1).shape
313/56: np.frombuffer(buffer, np.int32, -1).shape
313/57: np.frombuffer(buffer, np.int32, -1)
313/58: np.frombuffer(buffer, np.int64, -1)
313/59: np.frombuffer(buffer, np.int64, -1).shape
313/60: np.frombuffer(buffer, np.int8, -1).shape
313/61: np.frombuffer(buffer, np.int8, -1).shape
313/62: np.frombuffer(buffer, np.float32, -1).shape
313/63: np.frombuffer(buffer, np.float64, -1).shape
313/64: np.frombuffer(buffer, np.int32, -1)
313/65: np.frombuffer(buffer, np.int32, -1).shape
313/66: np.frombuffer(buffer, np.int16, -1).shape
313/67: buffer
313/68: b'0x0'.decode()
313/69: codecs.decode(b'0x0')
313/70: import codecs
313/71: codecs.decode(b'0x0')
313/72: codecs.encode(b'0x0')
313/73: codecs.encode('h')
313/74: codecs.encode('123')
313/75: codecs.encode('123asldkf')
313/76: codecs.decode(b'123asldkf')
313/77: codecs.decode(b'\x9b')
313/78: codecs.decode(b'x9b')
313/79: codecs.decode(b'9b\')
313/80: u = chr(40960) + 'abcd' + chr(1972)
313/81: u.encode('utf-8')
313/82: b'\xea\x80\x80abcd\xde\xb4'.decode('utf-8')
313/83: 16000/float(62.5)
313/84: 256//16000
313/85: 256//16000*1000
313/86: 1000*256//16000
313/87: 300//16
313/88: 300//20
313/89: l = [1,2,3]
313/90: l+l
318/1: import numpy as np
318/2: x = np.array([1,2,3])
318/3: np.expand_dims(x)
318/4: np.expand_dims(x, axis=0)
318/5: np.expand_dims(x, axis=0).shape
318/6: x.shpae
318/7: x.shape
318/8: np.expand_dims(x, axis=1).shape
318/9: np.expand_dims(x, axis=0).shape
318/10: np.expand_dims(x, axis=0).shape
318/11: (1,) + x.shape
318/12: (1,3) + x.shape
318/13: import os
318/14: os.getcwd()
318/15: pritn(f"{os.getcwd()}/../")
318/16: print(f"{os.getcwd()}/../")
318/17: audio_ring_buffer = collections.deque(maxlen=2)
318/18: import collections
318/19: audio_ring_buffer = collections.deque(maxlen=2)
318/20: audio_ring_buffer.push(5)
318/21: audio_ring_buffer.add(5)
318/22: audio_ring_buffer.append(5)
318/23: audio_ring_buffer.append(3)
318/24: list(audio_ring_buffer)
318/25: audio_ring_buffer.count(3)
318/26: audio_ring_buffer.count(4)
318/27: audio_ring_buffer.count(5)
318/28: audio_ring_buffer.appendleft(2)
318/29: list(audio_ring_buffer)
318/30: audio_ring_buffer.appendleft(2)
318/31: audio_ring_buffer.appendleft(1)
318/32: list(audio_ring_buffer)
318/33: np
318/34: x1 = np.array([1,2,3])
318/35: audio_ring_buffer.appendleft(x1)
318/36: x2 = np.array([4,5,6])
318/37: audio_ring_buffer.appendleft(x2)
318/38: list(audio_ring_buffer)
318/39: audio_ring_buffer.appendleft(x1)
318/40: list(audio_ring_buffer)
318/41: x3 = np.arange(100)
318/42: audio_ring_buffer.appendleft(x3)
318/43: list(audio_ring_buffer)
318/44: np.concatenate(audio_ring_buffer)
318/45: len(audio_ring_buffer)
318/46: wav_data = bytearray()
318/47: wav_data.append(b'1')
318/48: wav_data.append(b'11')
318/49: type(b'11)
318/50: type(b'11')
318/51: frame = b'\x00\x00\x00\x00\xfd\xff\xff\xff\xfd\xff\xfc\xff\xf9\xff\xf3\xff\xfa\xff\xf4\xff\xf0\xff\xf2\xff\xf8\xff\xf8\xff\xf2\xff\xf0\xff\xee\xff\xed\xff\xf6\xff\xfc\xff\xfd\xff\x04\x00\x01\x00\x00\x00\xf7\xff\xfa\xff\x00\x00\xf9\xff\xf8\xff\xf4\xff\xf5\xff\xf8\xff\xf9\xff\xf6\xff\xfa\xff\x00\x00\x04\x00\x03\x00\xff\xff\x01\x00\x07\x00\x02\x00\x02\x00\x07\x00\x12\x00\x17\x00\x12\x00\x13\x00\x17\x00\x1d\x00\x1f\x00\x17\x00\x16\x00\x17\x00\x14\x00\x11\x00\x0f\x00\x0e\x00\x0e\x00\x14\x00\r\x00\x05\x00\x06\x00\x0c\x00\x10\x00\x04\x00\x06\x00\x0c\x00\x06\x00\xfc\xff\xf5\xff\xf3\xff\xf3\xff\xe9\xff\xe9\xff\xea\xff\xe5\xff\xe8\xff\xe0\xff\xe7\xff\xe7\xff\xe7\xff\xec\xff\xe7\xff\xee\xff\xf2\xff\xf3\xff\xf0\xff\xf6\xff\xf5\xff\xfb\xff\xff\xff\xff\xff\x99\x00\x08\x01\x03\x01\t\x01\xf8\x00\xe5\x00\xfa\x00\x05\x01\n\x01\x18\x015\x01>\x01;\x01L\x01G\x01A\x01\x17\x01\xe9\x00\xd8\x00\xc4\x00\xa6\x00\x8c\x00w\x00y\x00\x9c\x00\x7f\x00\x80\x00\xc2\x00\xe8\x00\xec\x00\xc3\x00\xc0\x00\xfb\x00\x1d\x01\xe9\x00\xe2\x00\xfb\x00\x01\x01\x06\x01\xdd\x00\xce\x00\xde\x00\xb8\x00}\x00\x87\x00\x97\x00\x85\x00r\x00q\x00e\x00k\x00q\x00\x7f\x00\x97\x00v\x00\x82\x00\x95\x00\x8b\x00\xa3\x00\xa0\x00\xaf\x00\xc0\x00\xd0\x00\xb4\x00\xba\x00\xe5\x00\xab\x00t\x00B\x001\x00-\x00\xfd\xff\xd9\xff\xe0\xff\x04\x00\xe8\xff\xe3\xff\x13\x00?\x00W\x00U\x00_\x00o\x00i\x00|\x00\x8a\x00\x94\x00\x8d\x00\x82\x00\x96\x00\x93\x00\x9c\x00q\x00]\x00=\x00\x0b\x00\x01\x00\xf3\xff\xe8\xff\xb1\xff\x97\xff\xcf\xff\xf1\xff\xe8\xff\xe9\xff4\x00K\x00I\x00<\x00(\x00j\x00<\x00\t\x00%\x00#\x00)\x00<\x00)\x00\xf4\xff\xc4\xff\x92\xffr\xfft\xffK\xff,\xff,\xff3\xff{\xff\x8b\xff\x9b\xff\xe8\xff\xf7\xff\xcb\xff\xeb\xff\x00\x00\xeb\xff\xd9\xff\x82\xfft\xff\x94\xffl\xffn\xffK\xff\x1a\xff\r\xff\xeb\xfe\xe6\xfe\xd8\xfe\xdf\xfe\xe5\xfe\x01\xff\x1a\xff*\xff\x8c\xff\xbb\xff\xef\xff\x03\x00\xf4\xff.\x00H\x00H\x00\x15\x00\xde\xff'
318/52: np.frombuffer(frame, np.int16)
318/53: audio_ring_buffer.appendleft(frame)
318/54: audio_ring_buffer.appendleft(frame)
318/55: list(audio_ring_buffer)
318/56: list(audio_ring_buffer)[0]
318/57:
for buff in audio_ring_buffer:
    print(buff)
318/58:
for buff in audio_ring_buffer:
    print(1)
318/59: nd1 = np.arange(100)
318/60: nd1.reshape(10,10)
318/61: nd1 = nd1.reshape(10,10)
318/62: nd2 = np.arange(100)
318/63: nd2 = nd2.reshape(10,10)
318/64: np.concatenate(nd1, nd2)
318/65: np.concatenate(nd1, nd2, axis=0)
318/66: np.concatenate((nd1, nd2), axis=0)
318/67: np.concatenate((nd1, nd2), axis=1)
318/68: np.concatenate((nd1, nd2))
318/69: np.concatenate((nd1, nd2), axis=1)
318/70: import shutil
318/71: src= "~/CS/consulting/firstlayerai/phoneme_classification/src/awni_speech/speech/requirements.txt"
318/72: dst = ~/CS/consulting/firstlayerai/phoneme_classification/src/awni_speech/speech/streaming
318/73: dst = '~/CS/consulting/firstlayerai/phoneme_classification/src/awni_speech/speech/streaming'
318/74: shutil.copyfile(src,dst)
318/75: os.path.exists('~/CS/consulting/firstlayerai/phoneme_classification/src/awni_speech/speech/requirements.txt')
318/76: ls -lh ~/CS/consulting/firstlayerai/phoneme_classification/src/awni_speech/speech/requirements.txt
318/77: os.path.exists('/Users/dustin/CS/consulting/firstlayerai/phoneme_classification/src/awni_speech/speech')
318/78: os.path.exists('/Users/dustin/CS/consulting/firstlayerai/phoneme_classification/src/awni_speech/speech/requirements.txt')
318/79: src ='/Users/dustin/CS/consulting/firstlayerai/phoneme_classification/src/awni_speech/speech/requirements.txt'
318/80: dst='/Users/dustin/CS/consulting/firstlayerai/phoneme_classification/src/awni_speech/speech/streaming/
318/81: dst='/Users/dustin/CS/consulting/firstlayerai/phoneme_classification/src/awni_speech/speech/streaming/'
318/82: shutil.copyfile(src,dst)
318/83: dst='/Users/dustin/CS/consulting/firstlayerai/phoneme_classification/src/awni_speech/speech/streaming/test.txt'
318/84: shutil.copyfile(src,dst)
318/85: os.getcwd()
318/86: os.listdir(os.getcwd())
318/87: 5 <= 5
318/88: ~True
318/89: not True
318/90: True or False
319/1: import numpy as np
319/2: np.arange(7967)
319/3: arr=np.arange(7967)
319/4: arr.reshape(31, 257)
319/5: arr= arr.reshape(31, 257)
319/6: arr.shape
319/7: np.expand_dims(arr, axis=0)
319/8: np.expand_dims(arr, axis=0).shape
319/9: np.expand_dims(arr, axis=1).shape
319/10: np.expand_dims(arr, axis=2).shape
319/11: np.expand_dims(arr, axis=0).shape
319/12: ax1 = np.arange(40)
319/13: ax1=ax1.reshape(1,1,40)
319/14: ax1
319/15: all_probs = np.empty(1,0,40)
319/16: all_probs = np.empty((1,0,40), dtype=np.float32)
319/17: all_probs.shape
319/18: all_probs = np.concatenate(all_probs, ax1)
319/19: all_probs = np.concatenate((all_probs, ax1))
319/20: probs_list = list()
319/21: probs_list.append(ax1)
319/22: probs_list.append(ax1)
319/23: probs_list.append(ax1)
319/24: probs_list.append(ax1)
319/25: probs_list.append(ax1)
319/26: len(probs_list)
319/27: np.concatenate(probs_list)
319/28: np.concatenate(probs_list).shape
319/29: np.concatenate(probs_list, axis=1).shape
319/30: 0%10
319/31: True & False
319/32: True && False
319/33: 1!=1
319/34: 2!=1
319/35: 110%10==0
319/36: 110!=0
319/37: 110%10==0 &110!=0
319/38: 110%10==0 & 110!=0
319/39: (110%10==0) & (110!=0)
319/40: 110%10==0 and 110!=0
319/41: (110%10==0) & (110!=0)
319/42: (110%10==0) and (110!=0)
319/43: ROOT = "/Users/dustin/CS/consulting/firstlayerai/phoneme_classification/src/awni_speech/speech/model_convert"
319/44: model_name="20200406-0409_31f"
319/45: onnx_path = os.path.join(ROOT, "onnx_models", model_name+"_model.onnx")
319/46: import os
319/47: onnx_path = os.path.join(ROOT, "onnx_models", model_name+"_model.onnx")
319/48: onnx_path
319/49: os.path.exists(onnx_path)
319/50: Round(5.5)
319/51: round(5.5)
319/52: type(round(5.5))
319/53: ls
319/54: from speech.loader import log_specgram_from_file
319/55: import os
319/56: audio_path = os.path.join('/Users/dustin/CS/consulting/firstlayerai/phoneme_classification/src/awni_speech/speech/model_convert/audio_files/', "ST-out.wav")
319/57: log_spec = log_specgram_from_file(audio_path)
319/58: log_spec.shape
319/59: x = b'\x1e\x00'
319/60: import struct
319/61: struct.unpack(format="<H", x)
319/62: struct.unpack(format="<H", buffer=x)
319/63: struct.unpack("<H",x)
319/64: [ord(i) for i in x]
319/65: [ord(i) for i in '\x1e\x00']
319/66: '\x1e\x00'.tostring()
319/67: b'\x1e\x00'.tostring()
319/68: __dir__()
319/69: .__dir__()
319/70: %who
319/71: x1 = b'\xdf\xfd\x14\xfe4\xfe\xeb\xfe\xc1\x00\xd4\xffC\x00\xe0\x00\xfb\xfc1\xfd\x80\xfe\xfc\xfe\xe8\x00\x07\x00\x1d\x00D\xff\xe3\xfd\xa1\xfd\xda\xfd \x00\x82\x00+\x00\xe4\xff\xe3\xfe)\xfe\x97\xfe\xaa\xff\xf4\xfe\xa4\xff\x0f\xffb\xff\\\xfe\x07\xfe(\xfe\x90\xfd\x1b\xfe\x12\xfe\x14\xff\x82\xfe\xb6\xfc\x9c\xfe\x02\x00v\xfe7\xff\xeb\xfe|\xfd\xa6\xfe\x14\x00]\xfe\x97\xff0\x01\x8a\x00\xd6\x00|\xfe\x9a\xfcC\xfd\x85\xfe\x86\xfe\xdf\xfd1\x00\xe1\xfe\x18\xff\xd0\x00\x01\xfeu\xfd\xdc\xfd\x95\xfe\x1a\x00\xe2\x00\xd4\x014\x01p\xff\xdd\xfeL\xff\xd0\xfd\xdc\xfe7\x01\xa9\x01\xc8\x02w\x01\x1c\x01\xd7\xff\x9f\xfe\x83\x01A\x01h\x00\xa1\x01\xfc\x00\xa1\x01B\x02J\x00e\xfe\xad\xfe\xa4\xfe\\\x01\xf7\x02\x10\x01\xf4\x01\xf1\x01\xd4\xffU\xff\x12\xffL\xff\x9d\x00\x13\x01F\x02S\x01l\x00\xd5\xff\xdf\xff\xa4\x00|\x01\x02\x02\xe1\x01\xf7\x01\x07\x02\xb1\x03\xf9\x02c\x01\xf2\x01\xaf\x01\xb4\x00\xd1\x01\xd2\x02\xe1\x010\x02\xf3\x01\xee\xff<\x00\xb8\x00n\x00\x0f\x01l\x00/\x01\x1d\x02\xbc\x00\x83\x00`\x01_\x00\x98\x00\x85\x00(\xffO\xff\xac\xfe\xfd\xfev\x00\xac\x00Z\x01\xa6\x01z\x00\x9b\xffE\x00\x14\x00\xaa\xff\xe9\xff=\x00\xe6\x01>\x02\xbf\x01\xa3\x01m\x00\xd9\xff\xf0\xff\xbc\xff\x0f\xff+\x02,\x03`\x02+\x02\xe9\x00\x9d\xff$\xff~\xff\xb6\xffm\x01\xb2\x02i\x02X\x01\x18\x00\xfb\xff\xfe\xff\x90\xff\xac\x00a\x012\x03\xe1\x02\xe5\x01\x18\x02\xc0\x00d\x00+\x01D\x01\xf6\x01Z\x03>\x02 \x02\x04\x02u\x01w\x02\xd4\x02^\x02\t\x02s\x02\x10\x01\xba\x00z\x00\x1c\x00*\x01\x13\x01\xdd\x01J\x00s\xff\xab\xff\x16\xff\x9c\xff\x17\x00\x9c\x00\xa9\x01\xa7\x01\x1e\x01_\x00E\x00\xb2\xffq\x00j\x00\xd5\xff\xde\x00\x19\x01]\x00\xec\xff"\x00\xd5\x00\x06\x02\x84\x01/\x00\x1c\x00\x8d\xff\x7f\x00K\x01\xf4\x00\x0f\x01e\x006\xff\xb9\xfe\xf7\xfe\x04\xff\xb8\x00\xff\x00)\x01d\x01\x8f\xffI\xffp\xff\xf7\xfe\x05\x00\xa8\x00'
319/72: struct.unpack("<H",x1)
319/73:
for i in x1:
    print(i)
319/74: [i for i in x1]
319/75: ord('\xdf')
319/76: [i for i in x1][0]
319/77: ord('\xdf\xfd')
319/78: ord(b'\xdf\xfd')
319/79: x1[0]
319/80: x1[0].bytes()
319/81: x1
319/82: struct.unpack("<H",x1)
319/83: struct.unpack("<H",'\xdf\xfd')
319/84: struct.unpack("<H",b'\xdf\xfd')
319/85: import numpy as np
319/86: np.frombuffer(x1, np.int16)
319/87: np.frombuffer(x1, np.int16)[0]
319/88: x1[0]
319/89: x1[0:2]
319/90: ord(b'\xdf\xfd')
319/91: ord(b'\xdf')
319/92: np.frombuffer(b'\xdf\xfd', np.int16)
319/93: np.frombuffer(b'\xdf', np.int16)
319/94: '0xdf'
319/95: ord('0xdf')
319/96: int('0xdf')
319/97: int('16xdf')
319/98: int('df', 16)
319/99: audio_path
319/100:
with open(audio_path, 'rb') as wf:
    print("channels: ", wf.getnchannels())
    print("samplewidth: ", wf.getsampwidth())
319/101: import wavee
319/102: import wave
319/103:
with wave.open(audio_path, 'rb') as wf:
    print("channels: ", wf.getnchannels())
    print("samplewidth: ", wf.getsampwidth())
319/104:
with wave.open(audio_path, 'rb') as wf:
    print("channels: ", wf.getnchannels())
    print("samplewidth: ", wf.getsampwidth())
319/105:
with wave.open(audio_path, 'rb') as wf:
    print("channels: ", wf.getnchannels())
    print("samplewidth: ", wf.getsampwidth())
    print("frames: ", wf.readframes(1))
319/106:
with wave.open(audio_path, 'rb') as wf:
    print("channels: ", wf.getnchannels())
    print("samplewidth: ", wf.getsampwidth())
    print("frames: ", wf.readframes(1))
319/107:
with wave.open(audio_path, 'rb') as wf:
    print("channels: ", wf.getnchannels())
    print("samplewidth: ", wf.getsampwidth())
    print("frames: ", wf.readframes(1))
319/108:
with wave.open(audio_path, 'rb') as wf:
    print("channels: ", wf.getnchannels())
    print("samplewidth: ", wf.getsampwidth())
    print("frames: ", wf.readframes(1))
319/109:
with wave.open(audio_path, 'rb') as wf:
    print("channels: ", wf.getnchannels())
    print("samplewidth: ", wf.getsampwidth())
    print("frames: ", wf.readframes(1))
319/110:
with wave.open(audio_path, 'rb') as wf:
    print("channels: ", wf.getnchannels())
    print("samplewidth: ", wf.getsampwidth())
    print("frames: ", wf.readframes(1))
319/111:
with wave.open(audio_path, 'rb') as wf:
    print("channels: ", wf.getnchannels())
    print("samplewidth: ", wf.getsampwidth())
    print("frames: ", wf.readframes(1))
319/112:
with wave.open(audio_path, 'rb') as wf:
    print("channels: ", wf.getnchannels())
    print("samplewidth: ", wf.getsampwidth())
    print("frames: ", wf.readframes(1))
319/113:
with wave.open(audio_path, 'rb') as wf:
    print("channels: ", wf.getnchannels())
    print("samplewidth: ", wf.getsampwidth())
    print("frames: ", wf.readframes(1))
319/114:
with wave.open(audio_path, 'rb') as wf:
    print("channels: ", wf.getnchannels())
    print("samplewidth: ", wf.getsampwidth())
    print("frames: ", wf.readframes(1))
319/115:
with wave.open(audio_path, 'rb') as wf:
    print("channels: ", wf.getnchannels())
    print("samplewidth: ", wf.getsampwidth())
    print("frames: ", wf.readframes(10))
319/116:
with wave.open(audio_path, 'rb') as wf:
    print("channels: ", wf.getnchannels())
    print("samplewidth: ", wf.getsampwidth())
    print("frames: ", wf.readframes(100))
319/117:
with wave.open(audio_path, 'rb') as wf:
    print("channels: ", wf.getnchannels())
    print("samplewidth: ", wf.getsampwidth())
    print("frames: ", wf.readframes(1000))
319/118: x
319/119: struct.unpack("<H", x)
319/120: np.frombuffer(x, np.int16)
319/121: struct.unpack("<H", b'\xdf\xfd')
319/122: struct.unpack("<H",'\xdf\xfd')
319/123: struct.unpack("<",b'\xdf\xfd')
319/124: struct.unpack("<H",b'\xdf\xfd')
319/125: np.frombuffer(b'\xdf\xfd', np.int16)
319/126: struct.unpack("<h",b'\xdf\xfd')
319/127: x1
319/128:
with wave.open(audio_path, 'rb') as wf:
    print("channels: ", wf.getnchannels())
    print("samplewidth: ", wf.getsampwidth())
    print("frames: ", wf.readframes(1))
319/129:
with wave.open(audio_path, 'rb') as wf:
    print("channels: ", wf.getnchannels())
    print("samplewidth: ", wf.getsampwidth())
    print("frames: ", wf.readframes(500))
    print("frames: ", wf.readframes(500))
319/130:
with wave.open(audio_path, 'rb') as wf:
    print("channels: ", wf.getnchannels())
    print("samplewidth: ", wf.getsampwidth())
    print("frames: ", wf.readframes(500))
    print("frames: ", wf.readframes(500))
319/131: wf = wave.open(audio_path, 'rb')
319/132: log_spec.shape
319/133: log_spec.head()
319/134: np.head(log_spec)
319/135: log_spec[:10, :10]
319/136: wf.close()
319/137:
with wave.open(audio_path, 'rb') as wf
    print(wf.getnframes())
    np.frombuffer(wf.readframes(256), np.int16)
    print(wf.getnframes())
319/138:
with wave.open(audio_path, 'rb') as wf:
    print(wf.getnframes())
    np.frombuffer(wf.readframes(256), np.int16)
    print(wf.getnframes())
319/139:
with wave.open(audio_path, 'rb') as wf:
    print(wf.tell())
    np.frombuffer(wf.readframes(256), np.int16)
    print(wf.tell())
319/140:
with wave.open(audio_path, 'rb') as wf:
    chunk_size = 265
    num_samples = wf.getnframes()//chunk_size
    for i in range(num_samples):
        np.frombuffer(wf.readframes(256), np.int16)
        print(wf.tell())
319/141:
with wave.open(audio_path, 'rb') as wf:
    chunk_size = 265
    num_samples = wf.getnframes()//chunk_size
    print("num_samples: ," num_samples)
    for i in range(num_samples):
        np.frombuffer(wf.readframes(256), np.int16)
319/142:
with wave.open(audio_path, 'rb') as wf:
    chunk_size = 265
    num_samples = wf.getnframes()//chunk_size
    print("num_samples: ", num_samples)
    for i in range(num_samples):
        np.frombuffer(wf.readframes(256), np.int16)
319/143:
with wave.open(audio_path, 'rb') as wf:
    chunk_size = 265
    num_samples = wf.getnframes()//chunk_size
    print("num frames: ", wf.getnframes())
    print("num_samples: ", num_samples)
    for i in range(num_samples):
        np.frombuffer(wf.readframes(256), np.int16)
319/144:
with wave.open(audio_path, 'rb') as wf:
    chunk_size = 256
    num_samples = wf.getnframes()//chunk_size
    print("num frames: ", wf.getnframes())
    print("num_samples: ", num_samples)
    for i in range(num_samples):
        np.frombuffer(wf.readframes(256), np.int16)
319/145: from speech.loader import log_specgram_from_data
319/146: import Queue
319/147: from collections import deque
319/148: import queue
319/149: collections.deque(maxlen=2)
319/150: audio_buffer = deque(maxlen=2)
319/151:
with wave.open(audio_path, 'rb') as wf:
    chunk_size = 256
    audio_buffer = deque(maxlen=2) 
    num_samples = wf.getnframes()//chunk_size
    print("num frames: ", wf.getnframes())
    print("num_samples: ", num_samples)
    for i in range(num_samples):
        if len(audio_buffer) < 1:
            audio_ring_buffer.appendleft(wf.readframes(256))
        else:
            audio_ring_buffer.appendleft(wf.readframes(256))
            buffer_list = list(audio_ring_buffer)
            numpy_buffer = np.concatenate(
                    (np.frombuffer(buffer_list[0], np.int16), 
                    np.frombuffer(buffer_list[1], np.int16)))
            log_spec_step = log_specgram_from_data(numpy_buffer, samp_rate=16000)
319/152:
with wave.open(audio_path, 'rb') as wf:
    chunk_size = 256
    audio_ring_buffer = deque(maxlen=2) 
    num_samples = wf.getnframes()//chunk_size
    print("num frames: ", wf.getnframes())
    print("num_samples: ", num_samples)
    for i in range(num_samples):
        if len(audio_buffer) < 1:
            audio_ring_buffer.appendleft(wf.readframes(256))
        else:
            audio_ring_buffer.appendleft(wf.readframes(256))
            buffer_list = list(audio_ring_buffer)
            numpy_buffer = np.concatenate(
                    (np.frombuffer(buffer_list[0], np.int16), 
                    np.frombuffer(buffer_list[1], np.int16)))
            log_spec_step = log_specgram_from_data(numpy_buffer, samp_rate=16000)
319/153:
log_spec = 0
with wave.open(audio_path, 'rb') as wf:
    chunk_size = 256
    audio_ring_buffer = deque(maxlen=2) 
    num_samples = wf.getnframes()//chunk_size
    print("num frames: ", wf.getnframes())
    print("num_samples: ", num_samples)
    for i in range(num_samples):
        if len(audio_buffer) < 1:
            audio_ring_buffer.appendleft(wf.readframes(256))
        else:
            audio_ring_buffer.appendleft(wf.readframes(256))
            buffer_list = list(audio_ring_buffer)
            numpy_buffer = np.concatenate(
                    (np.frombuffer(buffer_list[0], np.int16), 
                    np.frombuffer(buffer_list[1], np.int16)))
            log_spec_step = log_specgram_from_data(numpy_buffer, samp_rate=16000)
319/154: x = np.empty((0,))
319/155: x.append(1)
319/156: np.append(x, 1)
319/157: x
319/158: x = np.append(x, 1)
319/159: x
319/160: x = np.empty((0,)).astype(np.int16)
319/161: x
319/162: x = np.append(x, 1)
319/163: x
319/164: x.dtype
319/165: x.dtype
319/166: x = np.append(x, '1')
319/167: x
319/168: x = np.append(x, float('1'))
319/169: x
319/170: x.astype(np.float32)
319/171: x
319/172: l = [1,2,3,4,5,6,7,8,9]
319/173: np.array_split(l, 3)
319/174: np.array_split(l, 3).astype(np.float32)
319/175: np.concatenate(np.array_split(l, 3), axis=0)
319/176: np.concatenate(np.array_split(l, 3), axis=1)
319/177: np.array(np.array_split(l, 3))
319/178: pixels = np.arange(784).reshape(28,28)
319/179: pixels.shape
319/180: images = np.empty((0,28,28))
319/181: np.append(images, pixels)
319/182: np.append(images, pixels).shape
319/183: pixels.shape
319/184: images.shape
319/185: np.concatenate(images, pixels, axis=0)
319/186: np.concatenate((images, pixels), axis=0)
319/187: np.expand_dims(pixels, axis=0).shape
319/188: np.concatenate((images, np.expand_dims(pixels, axis=0) ), axis=0)
319/189: np.concatenate((images, np.expand_dims(pixels, axis=0) ), axis=0).shape
319/190: images = np.concatenate((images, np.expand_dims(pixels, axis=0) ), axis=0)
319/191: images.shape
319/192: images
319/193: images = np.concatenate((images, np.expand_dims(pixels, axis=0) ), axis=0)
319/194: images.shape
319/195: images = np.concatenate((images, np.zeros((1,28,28))), axis=0)
319/196: images.shape
319/197: images[2].sum()
319/198: images[1].sum()
319/199: np.expand_dims(pixels, axis=0).astype(np.float32)
319/200: images.dtype
319/201: pixels.dtype
319/202: pixels.dtype == dtype('int64')
319/203: pixels.dtype == 'int64'
319/204: pixels.dtype == 'int'
319/205: pixels.dtype == 'float'
319/206: images.dtype == 'float'
319/207: labels = np.empty((0,)).astype(np.float32)
319/208: np.append(labels, float('3'))
319/209: labels = np.append(labels, float('3'))
319/210: labels = np.append(labels, float('3'))
319/211: labels
319/212: labels = np.append(labels, float('3'))
319/213: labels = np.append(labels, float('3'))
319/214: labels = np.append(labels, float('3'))
319/215: labels = np.append(labels, float('3'))
319/216: labels
319/217: labels.shape
319/218: np.expand_dims(labels, axis=-1)
319/219: np.expand_dims(labels, axis=-1).shape
319/220: len(labels)
319/221: len(labels)//4
319/222: labels.dtype
319/223: labels.dtype == 'float'
319/224: labels = labels.astype(np.float32)
319/225: labels.dtype
319/226: labels.dtype == 'float'
319/227: labels.dtype == 'double'
319/228: labels.dtype == 'single'
319/229: labels = labels.astype(np.float64)
319/230: labels.dtype == 'double'
319/231: labels.dtype == 'float'
319/232: labels.dtype == np.float64
319/233: labels.dtype == np.float
319/234: labels.dtype == np.float
319/235: labels = labels.astype(np.float32)
319/236: labels.dtype == np.float
319/237: labels.dtype
319/238: dtype('float32') is np.float32
319/239: labels.dtype is np.float32
319/240: labels.dtype is np.float32.type
319/241: labels.dtype is np.dtype('float32')
319/242: labels.dtype == np.dtype('float32')
319/243: labels.dtype == np.dtype('float')
319/244: labels.dtype == np.dtype('float32')
319/245: arr = np.array([1, 2, 3, 4, 5])
319/246: np_utils.to_categorical(arr, 5).shape
319/247: import tensorflow as tf
319/248: from tf.keras.utils import to_categorical
319/249: from tensorflow.keras.utils import to_categorical
319/250: import tensorflow.keras.utils
319/251: import tensorflow.keras
319/252: import tensorflow.keras
319/253: import tensorflow
319/254: import tensorflow.keras as keras
319/255:
log_spec = 0 
with wave.open(audio_path, 'rb') as wf: 
        chunk_size = 256 
        audio_ring_buffer = deque(maxlen=2)  
        num_samples = wf.getnframes()//chunk_size 
        print("num frames: ", wf.getnframes()) 
        print("num_samples: ", num_samples) 
        for i in range(num_samples): 
            if len(audio_buffer) < 1: 
                audio_ring_buffer.appendleft(wf.readframes(256)) 
            else: 
                audio_ring_buffer.appendleft(wf.readframes(256)) 
                buffer_list = list(audio_ring_buffer) 
                numpy_buffer = np.concatenate( 
                        (np.frombuffer(buffer_list[0], np.int16),  
                        np.frombuffer(buffer_list[1], np.int16))) 
                log_spec_step = log_specgram_from_data(numpy_buffer, samp_rate=16000)
319/256:
log_spec = 0 
with wave.open(audio_path, 'rb') as wf: 
        chunk_size = 256 
        audio_ring_buffer = deque(maxlen=2)  
        num_samples = wf.getnframes()//chunk_size 
        print("num frames: ", wf.getnframes()) 
        print("num_samples: ", num_samples) 
        for i in range(num_samples): 
            if len(audio_buffer) < 1: 
                audio_ring_buffer.appendleft(wf.readframes(256)) 
            else: 
                audio_ring_buffer.appendleft(wf.readframes(256)) 
                buffer_list = list(audio_ring_buffer) 
                numpy_buffer = np.concatenate( 
                        (np.frombuffer(buffer_list[0], np.int16),  
                        np.frombuffer(buffer_list[1], np.int16))) 
                log_spec_step = log_specgram_from_data(numpy_buffer, samp_rate=16000)
                print(log_spec_step.shape)
319/257:
log_spec = 0 
with wave.open(audio_path, 'rb') as wf: 
        chunk_size = 256 
        audio_ring_buffer = deque(maxlen=2)  
        num_samples = wf.getnframes()//chunk_size 
        print("num frames: ", wf.getnframes()) 
        print("num_samples: ", num_samples) 
        for i in range(num_samples): 
            if len(audio_ring_buffer) < 1: 
                audio_ring_buffer.appendleft(wf.readframes(256)) 
            else: 
                audio_ring_buffer.appendleft(wf.readframes(256)) 
                buffer_list = list(audio_ring_buffer) 
                numpy_buffer = np.concatenate( 
                        (np.frombuffer(buffer_list[0], np.int16),  
                        np.frombuffer(buffer_list[1], np.int16))) 
                log_spec_step = log_specgram_from_data(numpy_buffer, samp_rate=16000)
                print(log_spec_step.shape)
319/258:
log_spec = np.empty((0,257)).astype(np.float32)
with wave.open(audio_path, 'rb') as wf: 
        chunk_size = 256 
        audio_ring_buffer = deque(maxlen=2)  
        num_samples = wf.getnframes()//chunk_size 
        print("num frames: ", wf.getnframes()) 
        print("num_samples: ", num_samples) 
        for i in range(num_samples): 
            if len(audio_ring_buffer) < 1: 
                audio_ring_buffer.appendleft(wf.readframes(256)) 
            else: 
                audio_ring_buffer.appendleft(wf.readframes(256)) 
                buffer_list = list(audio_ring_buffer) 
                numpy_buffer = np.concatenate( 
                        (np.frombuffer(buffer_list[0], np.int16),  
                        np.frombuffer(buffer_list[1], np.int16))) 
                log_spec_step = log_specgram_from_data(numpy_buffer, samp_rate=16000)
                log_spec = np.concatenate((log_spec, log_spec_step), axis=0)
319/259: log_spec.shape
319/260:
stream_log_spec = np.empty((0,257)).astype(np.float32)
with wave.open(audio_path, 'rb') as wf: 
        chunk_size = 256 
        audio_ring_buffer = deque(maxlen=2)  
        num_samples = wf.getnframes()//chunk_size 
        print("num frames: ", wf.getnframes()) 
        print("num_samples: ", num_samples) 
        for i in range(num_samples): 
            if len(audio_ring_buffer) < 1: 
                audio_ring_buffer.appendleft(wf.readframes(256)) 
            else: 
                audio_ring_buffer.appendleft(wf.readframes(256)) 
                buffer_list = list(audio_ring_buffer) 
                numpy_buffer = np.concatenate( 
                        (np.frombuffer(buffer_list[0], np.int16),  
                        np.frombuffer(buffer_list[1], np.int16))) 
                log_spec_step = log_specgram_from_data(numpy_buffer, samp_rate=16000)
                steam_log_spec = np.concatenate((log_spec, log_spec_step), axis=0)
319/261: stream_log_spec.shape
319/262:
stream_log_spec = np.empty((0,257)).astype(np.float32)
with wave.open(audio_path, 'rb') as wf: 
        chunk_size = 256 
        audio_ring_buffer = deque(maxlen=2)  
        num_samples = wf.getnframes()//chunk_size 
        print("num frames: ", wf.getnframes()) 
        print("num_samples: ", num_samples) 
        for i in range(num_samples): 
            if len(audio_ring_buffer) < 1: 
                audio_ring_buffer.appendleft(wf.readframes(256)) 
            else: 
                audio_ring_buffer.appendleft(wf.readframes(256)) 
                buffer_list = list(audio_ring_buffer) 
                numpy_buffer = np.concatenate( 
                        (np.frombuffer(buffer_list[0], np.int16),  
                        np.frombuffer(buffer_list[1], np.int16))) 
                log_spec_step = log_specgram_from_data(numpy_buffer, samp_rate=16000)
                stream_log_spec = np.concatenate((stream_log_spec, log_spec_step), axis=0)
319/263: stream_log_spec.shape
319/264: log_spec_from_file
319/265: from speech.loader import log_spec_from_file
319/266: from speech.loader import log_specgram_from_file
319/267: wav_log_spec = log_specgram_from_file(audio_path)
319/268: wav_log_spec.shape
319/269: stream_log_spec.shape
319/270: all(wav_log_spec==stream_log_spec)
319/271: wav_log_spec.all(stream_log_spec)
319/272: wav_log_spec==stream_log_spec
319/273: (wav_log_spec==stream_log_spec).sum()
319/274: wav_log_spec[:100,:100]
319/275: wav_log_spec[:100,:100]
319/276: stream_log_spec[:100,:100]
319/277:
stream_log_spec = np.empty((0,257)).astype(np.float32)
with wave.open(audio_path, 'rb') as wf: 
        chunk_size = 256
        audio_ring_buffer = deque(maxlen=2)  
        num_samples = wf.getnframes()//chunk_size 
        print("num frames: ", wf.getnframes()) 
        print("num_samples: ", num_samples) 
        for i in range(num_samples): 
            if len(audio_ring_buffer) < 1: 
                audio_ring_buffer.appendleft(wf.readframes(chunk_size)) 
            else: 
                audio_ring_buffer.appendleft(wf.readframes(chunk_size)) 
                buffer_list = list(audio_ring_buffer) 
                numpy_buffer = np.concatenate( 
                        (np.frombuffer(buffer_list[0], np.int16),  
                        np.frombuffer(buffer_list[1], np.int16))) 
                log_spec_step = log_specgram_from_data(numpy_buffer, samp_rate=16000)
                stream_log_spec = np.concatenate((stream_log_spec, log_spec_step), axis=0)
319/278:
stream_log_spec = np.empty((0,257)).astype(np.float32)
with wave.open(audio_path, 'rb') as wf: 
        chunk_size = 320
        audio_ring_buffer = deque(maxlen=2)  
        num_samples = wf.getnframes()//chunk_size 
        print("num frames: ", wf.getnframes()) 
        print("num_samples: ", num_samples) 
        for i in range(num_samples): 
            if len(audio_ring_buffer) < 1: 
                audio_ring_buffer.appendleft(wf.readframes(chunk_size)) 
            else: 
                audio_ring_buffer.appendleft(wf.readframes(chunk_size)) 
                buffer_list = list(audio_ring_buffer) 
                numpy_buffer = np.concatenate( 
                        (np.frombuffer(buffer_list[0], np.int16),  
                        np.frombuffer(buffer_list[1], np.int16))) 
                log_spec_step = log_specgram_from_data(numpy_buffer, samp_rate=16000)
                stream_log_spec = np.concatenate((stream_log_spec, log_spec_step), axis=0)
319/279: stream_log_spec.shape
319/280:
stream_log_spec = np.empty((0,257)).astype(np.float32)
with wave.open(audio_path, 'rb') as wf: 
        chunk_size = 256
        audio_ring_buffer = deque(maxlen=2)  
        num_samples = wf.getnframes()//chunk_size 
        print("num frames: ", wf.getnframes()) 
        print("num_samples: ", num_samples) 
        for i in range(num_samples): 
            if len(audio_ring_buffer) < 1: 
                audio_ring_buffer.append(wf.readframes(chunk_size)) 
            else: 
                audio_ring_buffer.append(wf.readframes(chunk_size)) 
                buffer_list = list(audio_ring_buffer) 
                numpy_buffer = np.concatenate( 
                        (np.frombuffer(buffer_list[0], np.int16),  
                        np.frombuffer(buffer_list[1], np.int16))) 
                log_spec_step = log_specgram_from_data(numpy_buffer, samp_rate=16000, window_size=32, step_size=16)
                stream_log_spec = np.concatenate((stream_log_spec, log_spec_step), axis=0)
319/281: stream_log_spec.shape
319/282: stream_log_spec==wav_log_spec
319/283: (stream_log_spec==wav_log_spec).sum()
319/284:     audio_ring_buffer   = collections.deque(maxlen=2)
319/285: import collections
319/286:     audio_ring_buffer   = collections.deque(maxlen=2)
319/287: l = range(10)
319/288: l
319/289: print(audio_ring_buffer)
319/290:
for i in range(10):
    audio_ring_buffer.append(i)
    print(audio_ring_buffer)
319/291:
stream_log_spec = np.empty((0,257)).astype(np.float32)
with wave.open(audio_path, 'rb') as wf: 
        chunk_size = 256
        audio_ring_buffer = deque(maxlen=2)  
        conv_ring_buffer    = collections.deque(maxlen=31)
        probs_list          = list()
        hidden_in           = torch.zeros((5, 1, 512), dtype=torch.float32)
        cell_in             = torch.zeros((5, 1, 512), dtype=torch.float32)
        num_samples = wf.getnframes()//chunk_size 
        print("num frames: ", wf.getnframes()) 
        print("num_samples: ", num_samples) 
        for i in range(num_samples): 
            if len(audio_ring_buffer) < 1: 
                audio_ring_buffer.append(wf.readframes(chunk_size)) 
            else: 
                audio_ring_buffer.append(wf.readframes(chunk_size)) 
                buffer_list = list(audio_ring_buffer) 
                numpy_buffer = np.concatenate( 
                        (np.frombuffer(buffer_list[0], np.int16),  
                        np.frombuffer(buffer_list[1], np.int16))) 
                log_spec_step = log_specgram_from_data(numpy_buffer, samp_rate=16000, window_size=32, step_size=16)
                stream_log_spec = np.concatenate((stream_log_spec, log`en(conv_ring_buffer) < 30:
                conv_ring_buffer.append(log_spec_step)
            else:
319/292:
stream_log_spec = np.empty((0,257)).astype(np.float32)
with wave.open(audio_path, 'rb') as wf: 
        chunk_size = 256
        audio_ring_buffer = deque(maxlen=2)  
        conv_ring_buffer    = collections.deque(maxlen=31)
        probs_list          = list()
        hidden_in           = torch.zeros((5, 1, 512), dtype=torch.float32)
        cell_in             = torch.zeros((5, 1, 512), dtype=torch.float32)
        num_samples = wf.getnframes()//chunk_size 
        print("num frames: ", wf.getnframes()) 
        print("num_samples: ", num_samples) 
        for i in range(num_samples): 
            if len(audio_ring_buffer) < 1: 
                audio_ring_buffer.append(wf.readframes(chunk_size)) 
            else: 
                audio_ring_buffer.append(wf.readframes(chunk_size)) 
                buffer_list = list(audio_ring_buffer) 
                numpy_buffer = np.concatenate( 
                        (np.frombuffer(buffer_list[0], np.int16),  
                        np.frombuffer(buffer_list[1], np.int16))) 
                log_spec_step = log_specgram_from_data(numpy_buffer, samp_rate=16000, window_size=32, step_size=16)
                stream_log_spec = np.concatenate((stream_log_spec, log_spec_step), axis=0)
            if len(conv_ring_buffer) < 30:
                conv_ring_buffer.append(log_spec_step)
            else:
                conv_ring_buffer.append(log_spec_step)
                conv_context = np.concatenate(list(conv_ring_buffer), axis=0)
                conv_context = np.expand_dims(conv_context, axis=0)
319/293: import torch
319/294:
stream_log_spec = np.empty((0,257)).astype(np.float32)
with wave.open(audio_path, 'rb') as wf: 
        chunk_size = 256
        audio_ring_buffer = deque(maxlen=2)  
        conv_ring_buffer    = collections.deque(maxlen=31)
        probs_list          = list()
        hidden_in           = torch.zeros((5, 1, 512), dtype=torch.float32)
        cell_in             = torch.zeros((5, 1, 512), dtype=torch.float32)
        num_samples = wf.getnframes()//chunk_size 
        print("num frames: ", wf.getnframes()) 
        print("num_samples: ", num_samples) 
        for i in range(num_samples): 
            if len(audio_ring_buffer) < 1: 
                audio_ring_buffer.append(wf.readframes(chunk_size)) 
            else: 
                audio_ring_buffer.append(wf.readframes(chunk_size)) 
                buffer_list = list(audio_ring_buffer) 
                numpy_buffer = np.concatenate( 
                        (np.frombuffer(buffer_list[0], np.int16),  
                        np.frombuffer(buffer_list[1], np.int16))) 
                log_spec_step = log_specgram_from_data(numpy_buffer, samp_rate=16000, window_size=32, step_size=16)
                stream_log_spec = np.concatenate((stream_log_spec, log_spec_step), axis=0)
            if len(conv_ring_buffer) < 30:
                conv_ring_buffer.append(log_spec_step)
            else:
                conv_ring_buffer.append(log_spec_step)
                conv_context = np.concatenate(list(conv_ring_buffer), axis=0)
                conv_context = np.expand_dims(conv_context, axis=0)
319/295:
stream_log_spec = np.empty((0,257)).astype(np.float32)
with wave.open(audio_path, 'rb') as wf: 
        chunk_size = 256
        audio_ring_buffer = deque(maxlen=2)  
        conv_ring_buffer    = collections.deque(maxlen=31)
        probs_list          = list()
        hidden_in           = torch.zeros((5, 1, 512), dtype=torch.float32)
        cell_in             = torch.zeros((5, 1, 512), dtype=torch.float32)
        num_samples = wf.getnframes()//chunk_size 
        print("num frames: ", wf.getnframes()) 
        print("num_samples: ", num_samples) 
        for i in range(num_samples): 
            if len(audio_ring_buffer) < 1: 
                audio_ring_buffer.append(wf.readframes(chunk_size)) 
            else: 
                audio_ring_buffer.append(wf.readframes(chunk_size)) 
                buffer_list = list(audio_ring_buffer) 
                numpy_buffer = np.concatenate( 
                        (np.frombuffer(buffer_list[0], np.int16),  
                        np.frombuffer(buffer_list[1], np.int16))) 
                log_spec_step = log_specgram_from_data(numpy_buffer, samp_rate=16000, window_size=32, step_size=16)
                stream_log_spec = np.concatenate((stream_log_spec, log_spec_step), axis=0)
            if len(conv_ring_buffer) < 30:
                conv_ring_buffer.append(log_spec_step)
            else:
                conv_ring_buffer.append(log_spec_step)
                conv_context = np.concatenate(list(conv_ring_buffer), axis=0)
                conv_context = np.expand_dims(conv_context, axis=0)
                print(conv_context.shape)
319/296: ls
319/297: model_path = "/examples/librispeech/models/ctc_models/20200406/20200409"
319/298: model, preproc = speech.load(model_path, tag='best')
319/299: import speech
319/300: model, preproc = speech.load(model_path, tag='best')
319/301: ls /examples/librispeech/models/ctc_models/20200406/20200409/best_model
319/302: ls /examples/librispeech/models/ctc_models/20200406/20200409/
319/303: ls
319/304: model_path = "examples/librispeech/models/ctc_models/20200406/20200409"
319/305: model, preproc = speech.load(model_path, tag='best')
319/306:
stream_log_spec = np.empty((0,257)).astype(np.float32)
with wave.open(audio_path, 'rb') as wf: 
        chunk_size = 256
        audio_ring_buffer = deque(maxlen=2)  
        conv_ring_buffer    = collections.deque(maxlen=31)
        probs_list          = list()
        hidden_in           = torch.zeros((5, 1, 512), dtype=torch.float32)
        cell_in             = torch.zeros((5, 1, 512), dtype=torch.float32)
        num_samples = wf.getnframes()//chunk_size 
        print("num frames: ", wf.getnframes()) 
        print("num_samples: ", num_samples) 
        for i in range(num_samples): 
            if len(audio_ring_buffer) < 1: 
                audio_ring_buffer.append(wf.readframes(chunk_size)) 
            else: 
                audio_ring_buffer.append(wf.readframes(chunk_size)) 
                buffer_list = list(audio_ring_buffer) 
                numpy_buffer = np.concatenate( 
                        (np.frombuffer(buffer_list[0], np.int16),  
                        np.frombuffer(buffer_list[1], np.int16))) 
                log_spec_step = log_specgram_from_data(numpy_buffer, samp_rate=16000, window_size=32, step_size=16)
                stream_log_spec = np.concatenate((stream_log_spec, log_spec_step), axis=0)
            if len(conv_ring_buffer) < 30:
                conv_ring_buffer.append(log_spec_step)
            else:
                conv_ring_buffer.append(log_spec_step)
                conv_context = np.concatenate(list(conv_ring_buffer), axis=0)
                conv_context = np.expand_dims(conv_context, axis=0)
                model_out = model(torch.from_numpy(conv_context), (hidden_in, cell_in))
                probs, (hidden_out, cell_out) = model_out
                probs_list.append(probs)
                hidden_in, cell_in = hidden_out, cell_out
319/307: from speech.utils.convert import to_numpy
319/308:
stream_log_spec = np.empty((0,257)).astype(np.float32)
with wave.open(audio_path, 'rb') as wf: 
        chunk_size = 256
        audio_ring_buffer = deque(maxlen=2)  
        conv_ring_buffer    = collections.deque(maxlen=31)
        probs_list          = list()
        hidden_in           = torch.zeros((5, 1, 512), dtype=torch.float32)
        cell_in             = torch.zeros((5, 1, 512), dtype=torch.float32)
        num_samples = wf.getnframes()//chunk_size 
        print("num frames: ", wf.getnframes()) 
        print("num_samples: ", num_samples) 
        for i in range(num_samples): 
            if len(audio_ring_buffer) < 1: 
                audio_ring_buffer.append(wf.readframes(chunk_size)) 
            else: 
                audio_ring_buffer.append(wf.readframes(chunk_size)) 
                buffer_list = list(audio_ring_buffer) 
                numpy_buffer = np.concatenate( 
                        (np.frombuffer(buffer_list[0], np.int16),  
                        np.frombuffer(buffer_list[1], np.int16))) 
                log_spec_step = log_specgram_from_data(numpy_buffer, samp_rate=16000, window_size=32, step_size=16)
                stream_log_spec = np.concatenate((stream_log_spec, log_spec_step), axis=0)
            if len(conv_ring_buffer) < 30:
                conv_ring_buffer.append(log_spec_step)
            else:
                conv_ring_buffer.append(log_spec_step)
                conv_context = np.concatenate(list(conv_ring_buffer), axis=0)
                conv_context = np.expand_dims(conv_context, axis=0)
                model_out = model(torch.from_numpy(conv_context), (hidden_in, cell_in))
                probs, (hidden_out, cell_out) = model_out
                probs = to_numpy(probs)
                probs_list.append(probs)
                hidden_in, cell_in = hidden_out, cell_out
319/309: len(probs_list)
319/310: probs_list[0].shape
319/311: all_probs = np.concatenate(probs_list, axis=0)
319/312: all_probs.shape
319/313: all_probs = np.concatenate(probs_list, axis=1)
319/314: all_probs.shape
319/315: hidden_zero = torch.zeros((5, 1, 512), dtype=torch.float32)
319/316: cell_zero = torch.zeros((5, 1, 512), dtype=torch.float32)
319/317: method = getattr(preproc, "encode", None)
319/318: callable(method)
319/319: method = getattr(preproc, "normalize", None)
319/320: method
319/321: callable(method)
319/322: method = getattr(preproc, "encode", None)
319/323: method
319/324: callable(method)
319/325: hasattr(preproc, "encode")
319/326: hasattr(preproc, "normalize")
319/327: from speech.utils.compat import normalize
319/328:
stream_log_spec = np.empty((0,257)).astype(np.float32)
with wave.open(audio_path, 'rb') as wf: 
        chunk_size = 256
        audio_ring_buffer = deque(maxlen=2)  
        conv_ring_buffer    = collections.deque(maxlen=31)
        probs_list          = list()
        hidden_in           = torch.zeros((5, 1, 512), dtype=torch.float32)
        cell_in             = torch.zeros((5, 1, 512), dtype=torch.float32)
        num_samples = wf.getnframes()//chunk_size 
        print("num frames: ", wf.getnframes()) 
        print("num_samples: ", num_samples) 
        for i in range(num_samples): 
            if len(audio_ring_buffer) < 1: 
                audio_ring_buffer.append(wf.readframes(chunk_size)) 
            else: 
                audio_ring_buffer.append(wf.readframes(chunk_size)) 
                buffer_list = list(audio_ring_buffer) 
                numpy_buffer = np.concatenate( 
                        (np.frombuffer(buffer_list[0], np.int16),  
                        np.frombuffer(buffer_list[1], np.int16))) 
                log_spec_step = log_specgram_from_data(numpy_buffer, samp_rate=16000, window_size=32, step_size=16)
                norm_log_spec = normalize(preproc, log_spec_step)
                stream_log_spec = np.concatenate((stream_log_spec, log_spec_step), axis=0)
            if len(conv_ring_buffer) < 30:
                conv_ring_buffer.append(norm_log_spec)
            else:
                conv_ring_buffer.append(norm_log_spec)
                conv_context = np.concatenate(list(conv_ring_buffer), axis=0)
                conv_context = np.expand_dims(conv_context, axis=0)
                model_out = model(torch.from_numpy(conv_context), (hidden_in, cell_in))
                probs, (hidden_out, cell_out) = model_out
                probs = to_numpy(probs)
                probs_list.append(probs)
                hidden_in, cell_in = hidden_out, cell_out
319/329:
stream_log_spec = np.empty((0,257)).astype(np.float32)
with wave.open(audio_path, 'rb') as wf: 
        chunk_size = 256
        audio_ring_buffer = deque(maxlen=2)  
        conv_ring_buffer    = collections.deque(maxlen=31)
        probs_list          = list()
        hidden_in           = torch.zeros((5, 1, 512), dtype=torch.float32)
        cell_in             = torch.zeros((5, 1, 512), dtype=torch.float32)
        num_samples = wf.getnframes()//chunk_size 
        print("num frames: ", wf.getnframes()) 
        print("num_samples: ", num_samples) 
        for i in range(num_samples): 
            if len(audio_ring_buffer) < 1: 
                audio_ring_buffer.append(wf.readframes(chunk_size)) 
            else: 
                audio_ring_buffer.append(wf.readframes(chunk_size)) 
                buffer_list = list(audio_ring_buffer) 
                numpy_buffer = np.concatenate( 
                        (np.frombuffer(buffer_list[0], np.int16),  
                        np.frombuffer(buffer_list[1], np.int16))) 
                log_spec_step = log_specgram_from_data(numpy_buffer, samp_rate=16000, window_size=32, step_size=16)
                norm_log_spec = normalize(preproc, log_spec_step)
                stream_log_spec = np.concatenate((stream_log_spec, log_spec_step), axis=0)
            if len(conv_ring_buffer) < 30:
                conv_ring_buffer.append(norm_log_spec)
            else:
                conv_ring_buffer.append(norm_log_spec)
                conv_context = np.concatenate(list(conv_ring_buffer), axis=0)
                conv_context = np.expand_dims(conv_context, axis=0)
                model_out = model(torch.from_numpy(conv_context), (hidden_in, cell_in))
                probs, (hidden_out, cell_out) = model_out
                probs = to_numpy(probs)
                probs_list.append(probs)
                hidden_in, cell_in = hidden_out, cell_out
319/330:
stream_log_spec = np.empty((0,257)).astype(np.float32)
with wave.open(audio_path, 'rb') as wf: 
        chunk_size = 256
        audio_ring_buffer = deque(maxlen=2)  
        conv_ring_buffer    = collections.deque(maxlen=31)
        probs_list          = list()
        hidden_in           = torch.zeros((5, 1, 512), dtype=torch.float32)
        cell_in             = torch.zeros((5, 1, 512), dtype=torch.float32)
        num_samples = wf.getnframes()//chunk_size 
        print("num frames: ", wf.getnframes()) 
        print("num_samples: ", num_samples) 
        for i in range(num_samples): 
            if len(audio_ring_buffer) < 1: 
                audio_ring_buffer.append(wf.readframes(chunk_size)) 
            else: 
                audio_ring_buffer.append(wf.readframes(chunk_size)) 
                buffer_list = list(audio_ring_buffer) 
                numpy_buffer = np.concatenate( 
                        (np.frombuffer(buffer_list[0], np.int16),  
                        np.frombuffer(buffer_list[1], np.int16))) 
                log_spec_step = log_specgram_from_data(
                    numpy_buffer, samp_rate=16000, window_size=32, step_size=16)
                norm_log_spec = normalize(preproc, log_spec_step)
                stream_log_spec = np.concatenate((stream_log_spec, log_spec_step), axis=0)
            if len(conv_ring_buffer) < 30:
                conv_ring_buffer.append(norm_log_spec)
            else:
                conv_ring_buffer.append(norm_log_spec)
                conv_context = np.concatenate(list(conv_ring_buffer), axis=0)
                conv_context = np.expand_dims(conv_context, axis=0)
                model_out = model(torch.from_numpy(conv_context), (hidden_in, cell_in))
                probs, (hidden_out, cell_out) = model_out
                probs = to_numpy(probs)
                probs_list.append(probs)
                hidden_in, cell_in = hidden_out, cell_out
319/331:
stream_log_spec = np.empty((0,257)).astype(np.float32)
with wave.open(audio_path, 'rb') as wf: 
        chunk_size = 256
        audio_ring_buffer = deque(maxlen=2)  
        conv_ring_buffer    = collections.deque(maxlen=31)
        probs_list          = list()
        hidden_in           = torch.zeros((5, 1, 512), dtype=torch.float32)
        cell_in             = torch.zeros((5, 1, 512), dtype=torch.float32)
        num_samples = wf.getnframes()//chunk_size 
        print("num frames: ", wf.getnframes()) 
        print("num_samples: ", num_samples) 
        for i in range(num_samples): 
            if len(audio_ring_buffer) < 1: 
                audio_ring_buffer.append(wf.readframes(chunk_size)) 
            else: 
                audio_ring_buffer.append(wf.readframes(chunk_size)) 
                buffer_list = list(audio_ring_buffer) 
                numpy_buffer = np.concatenate( 
                        (np.frombuffer(buffer_list[0], np.int16),  
                        np.frombuffer(buffer_list[1], np.int16))) 
                log_spec_step = log_specgram_from_data(
                    numpy_buffer, samp_rate=16000, window_size=32, step_size=16)
                norm_log_spec = normalize(preproc, log_spec_step)
                stream_log_spec = np.concatenate((stream_log_spec, log_spec_step), axis=0)
                if len(conv_ring_buffer) < 30:
                    conv_ring_buffer.append(norm_log_spec)
                else:
                    conv_ring_buffer.append(norm_log_spec)
                    conv_context = np.concatenate(list(conv_ring_buffer), axis=0)
                    conv_context = np.expand_dims(conv_context, axis=0)
                    model_out = model(torch.from_numpy(conv_context), (hidden_in, cell_in))
                    probs, (hidden_out, cell_out) = model_out
                    probs = to_numpy(probs)
                    probs_list.append(probs)
                    hidden_in, cell_in = hidden_out, cell_out
319/332: wav_norm_log_spec = normalize(preproc, log_specgram_from_file(audio_path))
319/333:
def wav_inference():
    norm_log_spec = normalize(preproc, log_specgram_from_file(audio_path))
    hidden_in= torch.zeros((5, 1, 512), dtype=torch.float32)
    cell_in = torch.zeros((5, 1, 512), dtype=torch.float32)
    output = model(torch.from_numpy(norm_log_spec), (hidden_in, cell_in))
    probs, (hidden_out, cell_out) = output
    return probs
319/334: wav_probs = wav_inference()
319/335:
def wav_inference():
    norm_log_spec = normalize(preproc, log_specgram_from_file(audio_path))
    norm_log_spec = np.expand_dims(norm_log_spec, axis=0)
    hidden_in= torch.zeros((5, 1, 512), dtype=torch.float32)
    cell_in = torch.zeros((5, 1, 512), dtype=torch.float32)
    output = model(torch.from_numpy(norm_log_spec), (hidden_in, cell_in))
    probs, (hidden_out, cell_out) = output
    return probs
319/336: wav_probs = wav_inference()
319/337: wav_probs.shape
319/338:
def wav_inference():
    norm_log_spec = normalize(preproc, log_specgram_from_file(audio_path))
    norm_log_spec = np.expand_dims(norm_log_spec, axis=0)
    hidden_in= torch.zeros((5, 1, 512), dtype=torch.float32)
    cell_in = torch.zeros((5, 1, 512), dtype=torch.float32)
    output = model(torch.from_numpy(norm_log_spec), (hidden_in, cell_in))
    probs, (hidden_out, cell_out) = output
    probs = to_numpy(probs)
    return probs
319/339: wav_probs = wav_inference()
319/340: wav_probs.shape
319/341: type(wav_probs)
319/342: steam_probs=np.concatenate(probs_list, axis=1)
319/343: stream_probs=np.concatenate(probs_list, axis=1)
319/344: stream_probs.shape
319/345: wav_probs == stream_probs
319/346:
stream_log_spec = np.empty((0,257)).astype(np.float32)
with wave.open(audio_path, 'rb') as wf: 
        chunk_size = 256
        audio_ring_buffer = deque(maxlen=2)  
        conv_ring_buffer    = collections.deque(maxlen=31)
        probs_list          = list()
        hidden_in           = torch.zeros((5, 1, 512), dtype=torch.float32)
        cell_in             = torch.zeros((5, 1, 512), dtype=torch.float32)
        num_samples = wf.getnframes()//chunk_size 
        print("num frames: ", wf.getnframes()) 
        print("num_samples: ", num_samples) 
        for i in range(num_samples): 
            if len(audio_ring_buffer) < 1: 
                audio_ring_buffer.append(wf.readframes(chunk_size)) 
            else: 
                audio_ring_buffer.append(wf.readframes(chunk_size)) 
                buffer_list = list(audio_ring_buffer) 
                numpy_buffer = np.concatenate( 
                        (np.frombuffer(buffer_list[0], np.int16),  
                        np.frombuffer(buffer_list[1], np.int16))) 
                log_spec_step = log_specgram_from_data(
                    numpy_buffer, samp_rate=16000, window_size=32, step_size=16)
                norm_log_spec = normalize(preproc, log_spec_step)
                stream_log_spec = np.concatenate((stream_log_spec, norm_log_spec), axis=0)
                if len(conv_ring_buffer) < 30:
                    conv_ring_buffer.append(norm_log_spec)
                else:
                    conv_ring_buffer.append(norm_log_spec)
                    conv_context = np.concatenate(list(conv_ring_buffer), axis=0)
                    conv_context = np.expand_dims(conv_context, axis=0)
                    model_out = model(torch.from_numpy(conv_context), (hidden_in, cell_in))
                    probs, (hidden_out, cell_out) = model_out
                    probs = to_numpy(probs)
                    probs_list.append(probs)
                    hidden_in, cell_in = hidden_out, cell_out
319/347: stream_log_spec.shape
319/348: wav_norm_log_spec  = normalize(preproc, log_specgram_from_file(audio_path))
319/349: stream_log_spec == wav_norm_log_spec
319/350: (stream_log_spec == wav_norm_log_spec).sum()
319/351: stream_log_spec.shape
319/352: len(stream_log_spec)
319/353: stream_log_spec.size
319/354:
def wav_inference():
    norm_log_spec = normalize(preproc, log_specgram_from_file(audio_path))
    norm_log_spec = np.expand_dims(norm_log_spec, axis=0)
    hidden_in= torch.zeros((5, 1, 512), dtype=torch.float32)
    cell_in = torch.zeros((5, 1, 512), dtype=torch.float32)
    output = model(torch.from_numpy(norm_log_spec), (hidden_in, cell_in))
    probs, (hidden_out, cell_out) = output
    probs = to_numpy(probs)
    return probs
319/355:
stream_log_spec = np.empty((0,257)).astype(np.float32)
with wave.open(audio_path, 'rb') as wf: 
        chunk_size = 256
        audio_ring_buffer = deque(maxlen=2)  
        conv_ring_buffer    = collections.deque(maxlen=31)
        probs_list          = list()
        hidden_in           = torch.zeros((5, 1, 512), dtype=torch.float32)
        cell_in             = torch.zeros((5, 1, 512), dtype=torch.float32)
        num_samples = wf.getnframes()//chunk_size 
        print("num frames: ", wf.getnframes()) 
        print("num_samples: ", num_samples) 
        for i in range(num_samples): 
            if len(audio_ring_buffer) < 1: 
                audio_ring_buffer.append(wf.readframes(chunk_size)) 
            else: 
                audio_ring_buffer.append(wf.readframes(chunk_size)) 
                buffer_list = list(audio_ring_buffer) 
                numpy_buffer = np.concatenate( 
                        (np.frombuffer(buffer_list[0], np.int16),  
                        np.frombuffer(buffer_list[1], np.int16))) 
                log_spec_step = log_specgram_from_data(
                    numpy_buffer, samp_rate=16000, window_size=32, step_size=16)
                norm_log_spec = normalize(preproc, log_spec_step)
                stream_log_spec = np.concatenate((stream_log_spec, norm_log_spec), axis=0)
                if len(conv_ring_buffer) < 30:
                    conv_ring_buffer.append(norm_log_spec)
                else:
                    conv_ring_buffer.append(norm_log_spec)
                    conv_context = np.concatenate(list(conv_ring_buffer), axis=0)
                    conv_context = np.expand_dims(conv_context, axis=0)
                    model_out = model(torch.from_numpy(conv_context), (hidden_in, cell_in))
                    probs, (hidden_out, cell_out) = model_out
                    probs = to_numpy(probs)
                    probs_list.append(probs)
                    hidden_in, cell_in = hidden_out, cell_out
319/356: probs.shape
319/357: stream_probs = np.concatenate(probs_list, axis=1)
319/358: stream_probs.shape
319/359: wav_probs
319/360: wav_probs.shape
319/361: stream_probs
319/362: wav_probs[:,0,:].shape
319/363: wav_probs[:,0,:]==stream_probs[:,0,:]
319/364: wav_probs[:,0,:]==stream_probs[:,217,:]
319/365: wav_probs[:,0,:]==stream_probs[:,216,:]
319/366: context_size
319/367: context_size=31
319/368:
hidden_in           = torch.zeros((5, 1, 512), dtype=torch.float32)
cell_in             = torch.zeros((5, 1, 512), dtype=torch.float32)
chunk_probs_list = list()
for i in range(217):
    conv_buffer = stream_log_spec[i:i+context_size, :]
    conv_buffer = np.expand_dims(conv_buffer, axis=0)
    output = model(torch.from_numpy(conv_buffer), (hidden_in, cell_in))
    probs, (hidden_out, cell_out) = output
    chunk_probs_list.append(probs)
    hidden_in, cell_in = hidden_out, cell_out
319/369: len(chunk_probs_list)
319/370: chunk_probs_list[0].shape
319/371: chunk_probs = np.concatenate(chunk_probs_list, axis=1)
319/372:
hidden_in           = torch.zeros((5, 1, 512), dtype=torch.float32)
cell_in             = torch.zeros((5, 1, 512), dtype=torch.float32)
chunk_probs_list = list()
for i in range(217):
    conv_buffer = stream_log_spec[i:i+context_size, :]
    conv_buffer = np.expand_dims(conv_buffer, axis=0)
    output = model(torch.from_numpy(conv_buffer), (hidden_in, cell_in))
    probs, (hidden_out, cell_out) = output
    chunk_probs_list.append(to_numpy(probs))
    hidden_in, cell_in = hidden_out, cell_out
319/373: chunk_probs = np.concatenate(chunk_probs_list, axis=1)
319/374: chunk_probs.shape
319/375: chunk_probs == wav_probs
319/376: wav_probs.shape
319/377: chunk_probs == stream_probs
319/378: stream_probs.shape
319/379: np.testing.assert_allclose(stream_probs, wav_probs, rtol=1e-3, atol=1e-3)
319/380: np.testing.assert_allclose(chunk_probs, wav_probs, rtol=1e-3, atol=1e-3)
319/381: np.testing.assert_allclose(chunk_probs, stream_probs, rtol=1e-3, atol=1e-3)
319/382:
hidden_in           = torch.zeros((5, 1, 512), dtype=torch.float32)
cell_in             = torch.zeros((5, 1, 512), dtype=torch.float32)
chunk_probs_list = list()
for i in range(217):
    model.eval()
    conv_buffer = stream_log_spec[i:i+context_size, :]
    conv_buffer = np.expand_dims(conv_buffer, axis=0)
    output = model(torch.from_numpy(conv_buffer), (hidden_in, cell_in))
    probs, (hidden_out, cell_out) = output
    chunk_probs_list.append(to_numpy(probs))
    hidden_in, cell_in = hidden_out, cell_out
319/383:
def wav_inference():
    model.eval()
    norm_log_spec = normalize(preproc, log_specgram_from_file(audio_path))
    norm_log_spec = np.expand_dims(norm_log_spec, axis=0)
    hidden_in= torch.zeros((5, 1, 512), dtype=torch.float32)
    cell_in = torch.zeros((5, 1, 512), dtype=torch.float32)
    output = model(torch.from_numpy(norm_log_spec), (hidden_in, cell_in))
    probs, (hidden_out, cell_out) = output
    probs = to_numpy(probs)
    return probs
319/384: wav_probs = wav_inference()
319/385: wav_probs.shape
319/386: chunk_probs = np.concatenate(chunk_probs_list, axis=1)
319/387: chunk_probs.shape
319/388: np.testing.assert_allclose(chunk_probs, wav_probs, rtol=1e-3, atol=1e-3)
319/389: chunk_probs == wav_probs
319/390:
stream_log_spec = np.empty((0,257)).astype(np.float32)
model.eval()
with wave.open(audio_path, 'rb') as wf: 
        chunk_size = 256
        audio_ring_buffer = deque(maxlen=2)  
        conv_ring_buffer    = collections.deque(maxlen=31)
        probs_list          = list()
        hidden_in           = torch.zeros((5, 1, 512), dtype=torch.float32)
        cell_in             = torch.zeros((5, 1, 512), dtype=torch.float32)
        num_samples = wf.getnframes()//chunk_size 
        print("num frames: ", wf.getnframes()) 
        print("num_samples: ", num_samples) 
        for i in range(num_samples): 
            if len(audio_ring_buffer) < 1: 
                audio_ring_buffer.append(wf.readframes(chunk_size)) 
            else: 
                audio_ring_buffer.append(wf.readframes(chunk_size)) 
                buffer_list = list(audio_ring_buffer) 
                numpy_buffer = np.concatenate( 
                        (np.frombuffer(buffer_list[0], np.int16),  
                        np.frombuffer(buffer_list[1], np.int16))) 
                log_spec_step = log_specgram_from_data(
                    numpy_buffer, samp_rate=16000, window_size=32, step_size=16)
                norm_log_spec = normalize(preproc, log_spec_step)
                stream_log_spec = np.concatenate((stream_log_spec, norm_log_spec), axis=0)
                if len(conv_ring_buffer) < 30:
                    conv_ring_buffer.append(norm_log_spec)
                else:
                    conv_ring_buffer.append(norm_log_spec)
                    conv_context = np.concatenate(list(conv_ring_buffer), axis=0)
                    conv_context = np.expand_dims(conv_context, axis=0)
                    model_out = model(torch.from_numpy(conv_context), (hidden_in, cell_in))
                    probs, (hidden_out, cell_out) = model_out
                    probs = to_numpy(probs)
                    probs_list.append(probs)
                    hidden_in, cell_in = hidden_out, cell_out
319/391:  stream_pred = ['k', 'uh', 'd', 'ah', 't', 'ih', 'l', 'm', 'iy', 'hh', 'w', 'eh', 'r', 'dh', 'ah', 'ah', 'v', 'f', 'ey', 't', 'ih', 'ng', 'r', 'uw', 'm']
319/392: wav_pred = ['k', 'uh', 'd', 'ah', 't', 'ih', 'l', 'm', 'iy', 'hh', 'w', 'eh', 'r', 'dh', 'ah', 'ah', 'v', 'f', 'ey', 't', 'ih', 'ng', 'r', 'uw', 'm']
319/393: import evaldistance as ed
319/394: import editdistance as ed
319/395: ed.eval(stream_pred, wav_pred)
319/396: stream_probs = np.concatenate(probs_list, axis=1)
319/397: stream_probs.shape
319/398: np.testing.assert_allclose(chunk_probs, steam_probs, rtol=1e-3, atol=1e-3)
319/399: np.testing.assert_allclose(chunk_probs, stream_probs, rtol=1e-3, atol=1e-3)
319/400: np.testing.assert_allclose(wav_probs, stream_probs, rtol=1e-3, atol=1e-3)
319/401: steam_probs=0
319/402: stream_probs.shape
319/403: start = 0
319/404: end=10
319/405: l = [range(11)]
319/406: l[start, end]
319/407: l[0:10]
319/408: l[start:end]
319/409: wav_preds
319/410: wav_pred
319/411: ed.eval(wav_prob, ['k', 'uh', 'd', 'ah', 't', 'ih', 'l', 'm', 'iy', 'hh', 'w', 'eh', 'r', 'dh', 'ah', 'ah', 'v', 'f', 'ey', 't', 'ih', 'ng', 'r', 'uw', 'm'])
319/412: ed.eval(wav_probs, ['k', 'uh', 'd', 'ah', 't', 'ih', 'l', 'm', 'iy', 'hh', 'w', 'eh', 'r', 'dh', 'ah', 'ah', 'v', 'f', 'ey', 't', 'ih', 'ng', 'r', 'uw', 'm'])
319/413: wav_pred
319/414: ed.eval(wav_pred, ['k', 'uh', 'd', 'ah', 't', 'ih', 'l', 'm', 'iy', 'hh', 'w', 'eh', 'r', 'dh', 'ah', 'ah', 'v', 'f', 'ey', 't', 'ih', 'ng', 'r', 'uw', 'm'])
320/1: import os
320/2: os.path.splitext('common_voice_en_17831799.mp3')
320/3: os.extsep
320/4:     extension = "wv"
320/5: filename='common_voice_en_17831799.mp3'
320/6:         wv_filename = os.path.splitext(filename)[0] + os.extsep + extension
320/7: wv_filename
320/8:     src_dir = "dzubke@phoneme-2:~/awni_speech/data/common-voice/clips"
320/9:     dst_dir = "/Users/dustin/CS/consulting/firstlayerai/data/dataset_samples/common-voice_canada"
320/10:     gcloud_command = "gcloud compute scp {src_path} {dst_path}"
320/11: src_path = os.path.join(src_dir, wv_filename)
320/12:         dst_path = os.path.join(dst_dir, wv_filename)
320/13: os.system(gcloud_command.format(src_path=src_path, dst_path=dst_path))
320/14: x, y = 5
320/15: x, y = [0]*2
320/16: x
320/17: y
320/18: x, y = [1]*2
320/19: x
320/20: y
320/21: x, y = [5]*2
320/22: x
320/23: y
320/24: x, y = [5.0]*2
320/25: x
320/26: from time import time
320/27: time.now()
320/28: time.time()
320/29: time.time.now()
320/30: time.get_clock_info()
320/31: time.get_clock_info(time.clock())
320/32: clock = timem
320/33: clock = time
320/34: clock.time()
320/35: time()
320/36: import time
320/37: time.time()
320/38: round(0.003,3)
320/39: round(0.003,2)
320/40: round(0.003,1)
320/41:
stream_log_spec = np.empty((0,257)).astype(np.float32)
model.eval()
with wave.open(audio_path, 'rb') as wf: 
        chunk_size = 256
        audio_ring_buffer = deque(maxlen=2)  
        conv_ring_buffer    = collections.deque(maxlen=31)
        probs_list          = list()
        hidden_in           = torch.zeros((5, 1, 512), dtype=torch.float32)
        cell_in             = torch.zeros((5, 1, 512), dtype=torch.float32)
        num_samples = wf.getnframes()//chunk_size 
        print("num frames: ", wf.getnframes()) 
        print("num_samples: ", num_samples) 
        for i in range(num_samples): 
            if len(audio_ring_buffer) < 1: 
                audio_ring_buffer.append(wf.readframes(chunk_size)) 
            else: 
                audio_ring_buffer.append(wf.readframes(chunk_size)) 
                buffer_list = list(audio_ring_buffer) 
                numpy_buffer = np.concatenate( 
                        (np.frombuffer(buffer_list[0], np.int16),  
                        np.frombuffer(buffer_list[1], np.int16))) 
                log_spec_step = log_specgram_from_data(
                    numpy_buffer, samp_rate=16000, window_size=32, step_size=16)
                norm_log_spec = normalize(preproc, log_spec_step)
                stream_log_spec = np.concatenate((stream_log_spec, norm_log_spec), axis=0)
                if len(conv_ring_buffer) < 30:
                    conv_ring_buffer.append(norm_log_spec)
                else:
                    conv_ring_buffer.append(norm_log_spec)
                    conv_context = np.concatenate(list(conv_ring_buffer), axis=0)
                    conv_context = np.expand_dims(conv_context, axis=0)
                    model_out = model(torch.from_numpy(conv_context), (hidden_in, cell_in))
                    probs, (hidden_out, cell_out) = model_out
                    probs = to_numpy(probs)
                    probs_list.append(probs)
                    hidden_in, cell_in = hidden_out, cell_out
320/42: import numpy as np
320/43: audio_path = /Users/dustin/CS/consulting/firstlayerai/phoneme_classification/src/awni_speech/speech/model_convert/audio_files/ST-out.wav
320/44: audio_path = "/Users/dustin/CS/consulting/firstlayerai/phoneme_classification/src/awni_speech/speech/model_convert/audio_files/ST-out.wav'
320/45: audio_path = "/Users/dustin/CS/consulting/firstlayerai/phoneme_classification/src/awni_speech/speech/model_convert/audio_files/ST-out.wav"
320/46: ls
320/47: from speech.utils import io
320/48: model, preproc = io.load("examples/librispeech/models/ctc_models/20200406/20200409")
320/49:
stream_log_spec = np.empty((0,257)).astype(np.float32)
model.eval()
with wave.open(audio_path, 'rb') as wf: 
        chunk_size = 256
        audio_ring_buffer = deque(maxlen=2)  
        conv_ring_buffer    = collections.deque(maxlen=31)
        probs_list          = list()
        hidden_in           = torch.zeros((5, 1, 512), dtype=torch.float32)
        cell_in             = torch.zeros((5, 1, 512), dtype=torch.float32)
        num_samples = wf.getnframes()//chunk_size 
        print("num frames: ", wf.getnframes()) 
        print("num_samples: ", num_samples) 
        for i in range(num_samples): 
            if len(audio_ring_buffer) < 1: 
                audio_ring_buffer.append(wf.readframes(chunk_size)) 
            else: 
                audio_ring_buffer.append(wf.readframes(chunk_size)) 
                buffer_list = list(audio_ring_buffer) 
                numpy_buffer = np.concatenate( 
                        (np.frombuffer(buffer_list[0], np.int16),  
                        np.frombuffer(buffer_list[1], np.int16))) 
                log_spec_step = log_specgram_from_data(
                    numpy_buffer, samp_rate=16000, window_size=32, step_size=16)
                norm_log_spec = normalize(preproc, log_spec_step)
                stream_log_spec = np.concatenate((stream_log_spec, norm_log_spec), axis=0)
                if len(conv_ring_buffer) < 30:
                    conv_ring_buffer.append(norm_log_spec)
                else:
                    conv_ring_buffer.append(norm_log_spec)
                    conv_context = np.concatenate(list(conv_ring_buffer), axis=0)
                    conv_context = np.expand_dims(conv_context, axis=0)
                    model_out = model(torch.from_numpy(conv_context), (hidden_in, cell_in))
                    probs, (hidden_out, cell_out) = model_out
                    probs = to_numpy(probs)
                    probs_list.append(probs)
                    hidden_in, cell_in = hidden_out, cell_out
                    probs_steps = np.concatenate(probs_list, axis=1)
                    int_labels = max_decode(probs_steps[0], blank=39)
                    print(preproc.decode(int_labels))
320/50: from speech.utils import wave
320/51: from speech.utils import compat
320/52:
stream_log_spec = np.empty((0,257)).astype(np.float32)
model.eval()
with wave.open(audio_path, 'rb') as wf: 
        chunk_size = 256
        audio_ring_buffer = deque(maxlen=2)  
        conv_ring_buffer    = collections.deque(maxlen=31)
        probs_list          = list()
        hidden_in           = torch.zeros((5, 1, 512), dtype=torch.float32)
        cell_in             = torch.zeros((5, 1, 512), dtype=torch.float32)
        num_samples = wf.getnframes()//chunk_size 
        print("num frames: ", wf.getnframes()) 
        print("num_samples: ", num_samples) 
        for i in range(num_samples): 
            if len(audio_ring_buffer) < 1: 
                audio_ring_buffer.append(wf.readframes(chunk_size)) 
            else: 
                audio_ring_buffer.append(wf.readframes(chunk_size)) 
                buffer_list = list(audio_ring_buffer) 
                numpy_buffer = np.concatenate( 
                        (np.frombuffer(buffer_list[0], np.int16),  
                        np.frombuffer(buffer_list[1], np.int16))) 
                log_spec_step = log_specgram_from_data(
                    numpy_buffer, samp_rate=16000, window_size=32, step_size=16)
                norm_log_spec = normalize(preproc, log_spec_step)
                stream_log_spec = np.concatenate((stream_log_spec, norm_log_spec), axis=0)
                if len(conv_ring_buffer) < 30:
                    conv_ring_buffer.append(norm_log_spec)
                else:
                    conv_ring_buffer.append(norm_log_spec)
                    conv_context = np.concatenate(list(conv_ring_buffer), axis=0)
                    conv_context = np.expand_dims(conv_context, axis=0)
                    model_out = model(torch.from_numpy(conv_context), (hidden_in, cell_in))
                    probs, (hidden_out, cell_out) = model_out
                    probs = to_numpy(probs)
                    probs_list.append(probs)
                    hidden_in, cell_in = hidden_out, cell_out
                    probs_steps = np.concatenate(probs_list, axis=1)
                    int_labels = max_decode(probs_steps[0], blank=39)
                    print(preproc.decode(int_labels))
320/53: import wave
320/54:
stream_log_spec = np.empty((0,257)).astype(np.float32)
model.eval()
with wave.open(audio_path, 'rb') as wf: 
        chunk_size = 256
        audio_ring_buffer = deque(maxlen=2)  
        conv_ring_buffer    = collections.deque(maxlen=31)
        probs_list          = list()
        hidden_in           = torch.zeros((5, 1, 512), dtype=torch.float32)
        cell_in             = torch.zeros((5, 1, 512), dtype=torch.float32)
        num_samples = wf.getnframes()//chunk_size 
        print("num frames: ", wf.getnframes()) 
        print("num_samples: ", num_samples) 
        for i in range(num_samples): 
            if len(audio_ring_buffer) < 1: 
                audio_ring_buffer.append(wf.readframes(chunk_size)) 
            else: 
                audio_ring_buffer.append(wf.readframes(chunk_size)) 
                buffer_list = list(audio_ring_buffer) 
                numpy_buffer = np.concatenate( 
                        (np.frombuffer(buffer_list[0], np.int16),  
                        np.frombuffer(buffer_list[1], np.int16))) 
                log_spec_step = log_specgram_from_data(
                    numpy_buffer, samp_rate=16000, window_size=32, step_size=16)
                norm_log_spec = normalize(preproc, log_spec_step)
                stream_log_spec = np.concatenate((stream_log_spec, norm_log_spec), axis=0)
                if len(conv_ring_buffer) < 30:
                    conv_ring_buffer.append(norm_log_spec)
                else:
                    conv_ring_buffer.append(norm_log_spec)
                    conv_context = np.concatenate(list(conv_ring_buffer), axis=0)
                    conv_context = np.expand_dims(conv_context, axis=0)
                    model_out = model(torch.from_numpy(conv_context), (hidden_in, cell_in))
                    probs, (hidden_out, cell_out) = model_out
                    probs = to_numpy(probs)
                    probs_list.append(probs)
                    hidden_in, cell_in = hidden_out, cell_out
                    probs_steps = np.concatenate(probs_list, axis=1)
                    int_labels = max_decode(probs_steps[0], blank=39)
                    print(preproc.decode(int_labels))
320/55: import collections
320/56:
stream_log_spec = np.empty((0,257)).astype(np.float32)
model.eval()
with wave.open(audio_path, 'rb') as wf: 
        chunk_size = 256
        audio_ring_buffer = collections.deque(maxlen=2)  
        conv_ring_buffer    = collections.deque(maxlen=31)
        probs_list          = list()
        hidden_in           = torch.zeros((5, 1, 512), dtype=torch.float32)
        cell_in             = torch.zeros((5, 1, 512), dtype=torch.float32)
        num_samples = wf.getnframes()//chunk_size 
        print("num frames: ", wf.getnframes()) 
        print("num_samples: ", num_samples) 
        for i in range(num_samples): 
            if len(audio_ring_buffer) < 1: 
                audio_ring_buffer.append(wf.readframes(chunk_size)) 
            else: 
                audio_ring_buffer.append(wf.readframes(chunk_size)) 
                buffer_list = list(audio_ring_buffer) 
                numpy_buffer = np.concatenate( 
                        (np.frombuffer(buffer_list[0], np.int16),  
                        np.frombuffer(buffer_list[1], np.int16))) 
                log_spec_step = log_specgram_from_data(
                    numpy_buffer, samp_rate=16000, window_size=32, step_size=16)
                norm_log_spec = normalize(preproc, log_spec_step)
                stream_log_spec = np.concatenate((stream_log_spec, norm_log_spec), axis=0)
                if len(conv_ring_buffer) < 30:
                    conv_ring_buffer.append(norm_log_spec)
                else:
                    conv_ring_buffer.append(norm_log_spec)
                    conv_context = np.concatenate(list(conv_ring_buffer), axis=0)
                    conv_context = np.expand_dims(conv_context, axis=0)
                    model_out = model(torch.from_numpy(conv_context), (hidden_in, cell_in))
                    probs, (hidden_out, cell_out) = model_out
                    probs = to_numpy(probs)
                    probs_list.append(probs)
                    hidden_in, cell_in = hidden_out, cell_out
                    probs_steps = np.concatenate(probs_list, axis=1)
                    int_labels = max_decode(probs_steps[0], blank=39)
                    print(preproc.decode(int_labels))
320/57: import torch
320/58:
stream_log_spec = np.empty((0,257)).astype(np.float32)
model.eval()
with wave.open(audio_path, 'rb') as wf: 
        chunk_size = 256
        audio_ring_buffer = collections.deque(maxlen=2)  
        conv_ring_buffer    = collections.deque(maxlen=31)
        probs_list          = list()
        hidden_in           = torch.zeros((5, 1, 512), dtype=torch.float32)
        cell_in             = torch.zeros((5, 1, 512), dtype=torch.float32)
        num_samples = wf.getnframes()//chunk_size 
        print("num frames: ", wf.getnframes()) 
        print("num_samples: ", num_samples) 
        for i in range(num_samples): 
            if len(audio_ring_buffer) < 1: 
                audio_ring_buffer.append(wf.readframes(chunk_size)) 
            else: 
                audio_ring_buffer.append(wf.readframes(chunk_size)) 
                buffer_list = list(audio_ring_buffer) 
                numpy_buffer = np.concatenate( 
                        (np.frombuffer(buffer_list[0], np.int16),  
                        np.frombuffer(buffer_list[1], np.int16))) 
                log_spec_step = log_specgram_from_data(
                    numpy_buffer, samp_rate=16000, window_size=32, step_size=16)
                norm_log_spec = normalize(preproc, log_spec_step)
                stream_log_spec = np.concatenate((stream_log_spec, norm_log_spec), axis=0)
                if len(conv_ring_buffer) < 30:
                    conv_ring_buffer.append(norm_log_spec)
                else:
                    conv_ring_buffer.append(norm_log_spec)
                    conv_context = np.concatenate(list(conv_ring_buffer), axis=0)
                    conv_context = np.expand_dims(conv_context, axis=0)
                    model_out = model(torch.from_numpy(conv_context), (hidden_in, cell_in))
                    probs, (hidden_out, cell_out) = model_out
                    probs = to_numpy(probs)
                    probs_list.append(probs)
                    hidden_in, cell_in = hidden_out, cell_out
                    probs_steps = np.concatenate(probs_list, axis=1)
                    int_labels = max_decode(probs_steps[0], blank=39)
                    print(preproc.decode(int_labels))
320/59: from speech.loader import log_specgram_from_data
320/60:
stream_log_spec = np.empty((0,257)).astype(np.float32)
model.eval()
with wave.open(audio_path, 'rb') as wf: 
        chunk_size = 256
        audio_ring_buffer = collections.deque(maxlen=2)  
        conv_ring_buffer    = collections.deque(maxlen=31)
        probs_list          = list()
        hidden_in           = torch.zeros((5, 1, 512), dtype=torch.float32)
        cell_in             = torch.zeros((5, 1, 512), dtype=torch.float32)
        num_samples = wf.getnframes()//chunk_size 
        print("num frames: ", wf.getnframes()) 
        print("num_samples: ", num_samples) 
        for i in range(num_samples): 
            if len(audio_ring_buffer) < 1: 
                audio_ring_buffer.append(wf.readframes(chunk_size)) 
            else: 
                audio_ring_buffer.append(wf.readframes(chunk_size)) 
                buffer_list = list(audio_ring_buffer) 
                numpy_buffer = np.concatenate( 
                        (np.frombuffer(buffer_list[0], np.int16),  
                        np.frombuffer(buffer_list[1], np.int16))) 
                log_spec_step = log_specgram_from_data(
                    numpy_buffer, samp_rate=16000, window_size=32, step_size=16)
                norm_log_spec = normalize(preproc, log_spec_step)
                stream_log_spec = np.concatenate((stream_log_spec, norm_log_spec), axis=0)
                if len(conv_ring_buffer) < 30:
                    conv_ring_buffer.append(norm_log_spec)
                else:
                    conv_ring_buffer.append(norm_log_spec)
                    conv_context = np.concatenate(list(conv_ring_buffer), axis=0)
                    conv_context = np.expand_dims(conv_context, axis=0)
                    model_out = model(torch.from_numpy(conv_context), (hidden_in, cell_in))
                    probs, (hidden_out, cell_out) = model_out
                    probs = to_numpy(probs)
                    probs_list.append(probs)
                    hidden_in, cell_in = hidden_out, cell_out
                    probs_steps = np.concatenate(probs_list, axis=1)
                    int_labels = max_decode(probs_steps[0], blank=39)
                    print(preproc.decode(int_labels))
320/61: from speech.utils.compat import normalize
320/62:
stream_log_spec = np.empty((0,257)).astype(np.float32)
model.eval()
with wave.open(audio_path, 'rb') as wf: 
        chunk_size = 256
        audio_ring_buffer = collections.deque(maxlen=2)  
        conv_ring_buffer    = collections.deque(maxlen=31)
        probs_list          = list()
        hidden_in           = torch.zeros((5, 1, 512), dtype=torch.float32)
        cell_in             = torch.zeros((5, 1, 512), dtype=torch.float32)
        num_samples = wf.getnframes()//chunk_size 
        print("num frames: ", wf.getnframes()) 
        print("num_samples: ", num_samples) 
        for i in range(num_samples): 
            if len(audio_ring_buffer) < 1: 
                audio_ring_buffer.append(wf.readframes(chunk_size)) 
            else: 
                audio_ring_buffer.append(wf.readframes(chunk_size)) 
                buffer_list = list(audio_ring_buffer) 
                numpy_buffer = np.concatenate( 
                        (np.frombuffer(buffer_list[0], np.int16),  
                        np.frombuffer(buffer_list[1], np.int16))) 
                log_spec_step = log_specgram_from_data(
                    numpy_buffer, samp_rate=16000, window_size=32, step_size=16)
                norm_log_spec = normalize(preproc, log_spec_step)
                stream_log_spec = np.concatenate((stream_log_spec, norm_log_spec), axis=0)
                if len(conv_ring_buffer) < 30:
                    conv_ring_buffer.append(norm_log_spec)
                else:
                    conv_ring_buffer.append(norm_log_spec)
                    conv_context = np.concatenate(list(conv_ring_buffer), axis=0)
                    conv_context = np.expand_dims(conv_context, axis=0)
                    model_out = model(torch.from_numpy(conv_context), (hidden_in, cell_in))
                    probs, (hidden_out, cell_out) = model_out
                    probs = to_numpy(probs)
                    probs_list.append(probs)
                    hidden_in, cell_in = hidden_out, cell_out
                    probs_steps = np.concatenate(probs_list, axis=1)
                    int_labels = max_decode(probs_steps[0], blank=39)
                    print(preproc.decode(int_labels))
320/63: from speech.utils.convert import to_numpy
320/64:
stream_log_spec = np.empty((0,257)).astype(np.float32)
model.eval()
with wave.open(audio_path, 'rb') as wf: 
        chunk_size = 256
        audio_ring_buffer = collections.deque(maxlen=2)  
        conv_ring_buffer    = collections.deque(maxlen=31)
        probs_list          = list()
        hidden_in           = torch.zeros((5, 1, 512), dtype=torch.float32)
        cell_in             = torch.zeros((5, 1, 512), dtype=torch.float32)
        num_samples = wf.getnframes()//chunk_size 
        print("num frames: ", wf.getnframes()) 
        print("num_samples: ", num_samples) 
        for i in range(num_samples): 
            if len(audio_ring_buffer) < 1: 
                audio_ring_buffer.append(wf.readframes(chunk_size)) 
            else: 
                audio_ring_buffer.append(wf.readframes(chunk_size)) 
                buffer_list = list(audio_ring_buffer) 
                numpy_buffer = np.concatenate( 
                        (np.frombuffer(buffer_list[0], np.int16),  
                        np.frombuffer(buffer_list[1], np.int16))) 
                log_spec_step = log_specgram_from_data(
                    numpy_buffer, samp_rate=16000, window_size=32, step_size=16)
                norm_log_spec = normalize(preproc, log_spec_step)
                stream_log_spec = np.concatenate((stream_log_spec, norm_log_spec), axis=0)
                if len(conv_ring_buffer) < 30:
                    conv_ring_buffer.append(norm_log_spec)
                else:
                    conv_ring_buffer.append(norm_log_spec)
                    conv_context = np.concatenate(list(conv_ring_buffer), axis=0)
                    conv_context = np.expand_dims(conv_context, axis=0)
                    model_out = model(torch.from_numpy(conv_context), (hidden_in, cell_in))
                    probs, (hidden_out, cell_out) = model_out
                    probs = to_numpy(probs)
                    probs_list.append(probs)
                    hidden_in, cell_in = hidden_out, cell_out
                    probs_steps = np.concatenate(probs_list, axis=1)
                    int_labels = max_decode(probs_steps[0], blank=39)
                    print(preproc.decode(int_labels))
320/65:
def max_decode(output, blank=39):
    pred = np.argmax(output, 1)
    prev = pred[0]
    seq = [prev] if prev != blank else []
    for p in pred[1:]:
        if p != blank and p != prev:
            seq.append(p)
        prev = p
    return seq
320/66:
stream_log_spec = np.empty((0,257)).astype(np.float32)
model.eval()
with wave.open(audio_path, 'rb') as wf: 
        chunk_size = 256
        audio_ring_buffer = collections.deque(maxlen=2)  
        conv_ring_buffer    = collections.deque(maxlen=31)
        probs_list          = list()
        hidden_in           = torch.zeros((5, 1, 512), dtype=torch.float32)
        cell_in             = torch.zeros((5, 1, 512), dtype=torch.float32)
        num_samples = wf.getnframes()//chunk_size 
        print("num frames: ", wf.getnframes()) 
        print("num_samples: ", num_samples) 
        for i in range(num_samples): 
            if len(audio_ring_buffer) < 1: 
                audio_ring_buffer.append(wf.readframes(chunk_size)) 
            else: 
                audio_ring_buffer.append(wf.readframes(chunk_size)) 
                buffer_list = list(audio_ring_buffer) 
                numpy_buffer = np.concatenate( 
                        (np.frombuffer(buffer_list[0], np.int16),  
                        np.frombuffer(buffer_list[1], np.int16))) 
                log_spec_step = log_specgram_from_data(
                    numpy_buffer, samp_rate=16000, window_size=32, step_size=16)
                norm_log_spec = normalize(preproc, log_spec_step)
                stream_log_spec = np.concatenate((stream_log_spec, norm_log_spec), axis=0)
                if len(conv_ring_buffer) < 30:
                    conv_ring_buffer.append(norm_log_spec)
                else:
                    conv_ring_buffer.append(norm_log_spec)
                    conv_context = np.concatenate(list(conv_ring_buffer), axis=0)
                    conv_context = np.expand_dims(conv_context, axis=0)
                    model_out = model(torch.from_numpy(conv_context), (hidden_in, cell_in))
                    probs, (hidden_out, cell_out) = model_out
                    probs = to_numpy(probs)
                    probs_list.append(probs)
                    hidden_in, cell_in = hidden_out, cell_out
                    probs_steps = np.concatenate(probs_list, axis=1)
                    int_labels = max_decode(probs_steps[0], blank=39)
                    print(preproc.decode(int_labels))
320/67:
stream_log_spec = np.empty((0,257)).astype(np.float32)
model.eval()
with wave.open(audio_path, 'rb') as wf: 
        chunk_size = 256
        audio_ring_buffer = collections.deque(maxlen=2)  
        conv_ring_buffer    = collections.deque(maxlen=31)
        probs_list          = list()
        hidden_in           = torch.zeros((5, 1, 512), dtype=torch.float32)
        cell_in             = torch.zeros((5, 1, 512), dtype=torch.float32)
        num_samples = wf.getnframes()//chunk_size 
        print("num frames: ", wf.getnframes()) 
        print("num_samples: ", num_samples) 
        for i in range(num_samples): 
            if len(audio_ring_buffer) < 1: 
                audio_ring_buffer.append(wf.readframes(chunk_size)) 
            else: 
                audio_ring_buffer.append(wf.readframes(chunk_size)) 
                buffer_list = list(audio_ring_buffer) 
                numpy_buffer = np.concatenate( 
                        (np.frombuffer(buffer_list[0], np.int16),  
                        np.frombuffer(buffer_list[1], np.int16))) 
                log_spec_step = log_specgram_from_data(
                    numpy_buffer, samp_rate=16000, window_size=32, step_size=16)
                norm_log_spec = normalize(preproc, log_spec_step)
                stream_log_spec = np.concatenate((stream_log_spec, norm_log_spec), axis=0)
                if len(conv_ring_buffer) < 30:
                    conv_ring_buffer.append(norm_log_spec)
                else:
                    conv_ring_buffer.append(norm_log_spec)
                    conv_context = np.concatenate(list(conv_ring_buffer), axis=0)
                    conv_context = np.expand_dims(conv_context, axis=0)
                    model_out = model(torch.from_numpy(conv_context), (hidden_in, cell_in))
                    probs, (hidden_out, cell_out) = model_out
                    probs = to_numpy(probs)
                    probs_list.append(probs)
                    hidden_in, cell_in = hidden_out, cell_out
                    probs_steps = np.concatenate(probs_list, axis=1)
                    int_labels = max_decode(probs_steps[0], blank=40)
                    print(preproc.decode(int_labels))
320/68:
stream_log_spec = np.empty((0,257)).astype(np.float32)
model.eval()
with wave.open(audio_path, 'rb') as wf: 
        chunk_size = 256
        audio_ring_buffer = collections.deque(maxlen=2)  
        conv_ring_buffer    = collections.deque(maxlen=31)
        probs_list          = list()
        hidden_in           = torch.zeros((5, 1, 512), dtype=torch.float32)
        cell_in             = torch.zeros((5, 1, 512), dtype=torch.float32)
        num_samples = wf.getnframes()//chunk_size 
        print("num frames: ", wf.getnframes()) 
        print("num_samples: ", num_samples) 
        for i in range(num_samples): 
            if len(audio_ring_buffer) < 1: 
                audio_ring_buffer.append(wf.readframes(chunk_size)) 
            else: 
                audio_ring_buffer.append(wf.readframes(chunk_size)) 
                buffer_list = list(audio_ring_buffer) 
                numpy_buffer = np.concatenate( 
                        (np.frombuffer(buffer_list[0], np.int16),  
                        np.frombuffer(buffer_list[1], np.int16))) 
                log_spec_step = log_specgram_from_data(
                    numpy_buffer, samp_rate=16000, window_size=32, step_size=16)
                norm_log_spec = normalize(preproc, log_spec_step)
                stream_log_spec = np.concatenate((stream_log_spec, norm_log_spec), axis=0)
                if len(conv_ring_buffer) < 30:
                    conv_ring_buffer.append(norm_log_spec)
                else:
                    conv_ring_buffer.append(norm_log_spec)
                    conv_context = np.concatenate(list(conv_ring_buffer), axis=0)
                    conv_context = np.expand_dims(conv_context, axis=0)
                    model_out = model(torch.from_numpy(conv_context), (hidden_in, cell_in))
                    probs, (hidden_out, cell_out) = model_out
                    probs = to_numpy(probs)
                    probs_list.append(probs)
                    hidden_in, cell_in = hidden_out, cell_out
                    probs_steps = np.concatenate(probs_list, axis=1)
                    int_labels = max_decode(probs_steps[0], blank=39)
                    print(preproc.decode(int_labels))
321/1: import editdistance as ed
321/2: stream_pred = ['k', 'uh', 'd', 'ah', 't', 'ih', 'l', 'm', 'iy', 'hh', 'w', 'eh', 'r', 'dh', 'ah', 'ah', 'v', 'f', 'ey', 't', 'ih', 'ng', 'r', 'uw', 'm']
321/3: ed.eval(stream_pred, wav_pred)
321/4: wav_pred = ['k', 'uh', 'd', 'ah', 't', 'ih', 'l', 'm', 'iy', 'hh', 'w', 'eh', 'r', 'dh', 'ah', 'ah', 'v', 'f', 'ey', 't', 'ih', 'ng', 'r', 'uw', 'm']
321/5: ed.eval(stream_pred, wav_pred)
321/6: l = [ed.eval(stream_pred, wav_pred), ed.eval(stream_pred, wav_pred)]
321/7: *l
321/8:
def unpack(in_list:list):
    for call in in_list:
        call
321/9: unpack(l)
321/10: x=0
321/11: y=0
321/12: l = [x+5, y+7]
321/13: unpack(l)
321/14: x
321/15: y
321/16: l = [x=x+5, y=y+7]
321/17: def wrapper(func)
321/18:
def wrapper(func):
    func
321/19: wrapper(x=x+5)
321/20: wrapper("x=x+5")
321/21:
def wrapper(func):
    eval(func)
321/22: wrapper("x=x+5")
321/23: x=x+5
321/24: x
321/25:
def wrapper(func):
    exec(func)
321/26: wrapper("x=x+5")
321/27: x
321/28: config_path = "~/CS/consulting/firstlayerai/phoneme_classification/src/awni_speech/speech/model_convert/config/20200406-0409_31f_config.json"
321/29: import json
321/30:
with open(config_path, 'r') as fid:
    config = json.loads(fid)
321/31: iport os
321/32: import os
321/33: os.exists("~/CS/consulting/firstlayerai/phoneme_classification/src/awni_speech/speech/model_convert/config/20200406-0409_31f_config.json")
321/34: os.path.exists("~/CS/consulting/firstlayerai/phoneme_classification/src/awni_speech/speech/model_convert/config/20200406-0409_31f_config.json")
321/35: config_path = "/Users/dustin/CS/consulting/firstlayerai/phoneme_classification/src/awni_speech/speech/model_convert/config/20200406-0409_31f_config.json"
321/36:
with open(config_path, 'r') as fid:
    config = json.loads(fid)
321/37:
with open(config_path, 'r') as fid:
    config = json.load(fid)
321/38: config
321/39: from speech.models.ctc_model_pyt14 import CTC_pyt14
321/40: from speech.models.ctc_model_pyt14 import CTC_pyt14
321/41: model = CTC_pyt14(257, 39, config)
321/42: config
321/43: config['encooder']
321/44: config['encoder']
321/45: len(config)
321/46: config
321/47: model_config = config["model"]
321/48: model = CTC_pyt14(257, 39, model_config)
321/49: model.input_dim
321/50: model
321/51: model.max_decoder
321/52: model.max_decode
321/53: hasattr(model, "max_decode")
321/54: stream_pred=['k', 'uh', 'd', 'ah', 't', 'ih', 'l', 'm', 'iy', 'hh', 'w', 'eh', 'r', 'dh', 'ah', 'ah', 'v', 'f', 'ey', 't', 'ih', 'ng', 'r', 'uw', 'm']
321/55: ed.eval(stream_pred, wav_pred)
321/56: stream_pred=['k', 'uh', 'd', 'ah', 't', 'ih', 'l', 'm', 'iy', 'hh', 'w', 'eh', 'r', 'dh', 'ah', 'ah', 'v', 'f', 'ey', 't', 'ih', 'ng', 'r', 'uw', 'm']
321/57: ed.eval(stream_pred, wav_pred)
321/58: stream_pred = ['k', 'uh', 'd', 'ah', 't', 'ih', 'l', 'm', 'iy', 'hh', 'w', 'eh', 'r', 'dh', 'ah', 'ah', 'v', 'f', 'ey', 't', 'ih', 'ng', 'r', 'uw', 'm']
321/59: ed.eval(stream_pred, wav_pred)
321/60: stream_pred=['k', 'uh', 'd', 'ah', 't', 'ih', 'l', 'm', 'iy', 'hh', 'w', 'eh', 'r', 'dh', 'ah', 'ah', 'v', 'f', 'ey', 't', 'ih', 'ng', 'r', 'uw', 'm']
321/61: ed.eval(stream_pred, wav_pred)
322/1: import editdistance as ed
322/2: stream_pred = ['k', 'uh', 'd', 'ah', 't', 'ih', 'l', 'm', 'iy', 'hh', 'w', 'eh', 'r', 'dh', 'ah', 'ah', 'v', 'f', 'ey', 't', 'ih', 'ng', 'r', 'uw', 'm']
322/3: fullaudio_pred = ['k', 'uh', 'd', 'ah', 't', 'ih', 'l', 'm', 'iy', 'hh', 'w', 'eh', 'r', 'dh', 'ah', 'ah', 'v', 'f', 'ey', 't', 'ih', 'ng', 'r', 'uw', 'm']
322/4: ed.eval(stream_pred, fullaudio_pred)
322/5: import torch
322/6: arr = torch.arange(100)
322/7: arr.shape
322/8: arr.reshape(10,10)
322/9: arr = torch.arange(63479)
322/10: arr = arr.reshape(1,247, 257)
322/11: arr.shape
322/12: arr[:, 0:31, :].shape
322/13: arr.shape[1]
322/14: chunk_preds = ['k', 'uh', 'd', 'ah', 't', 'ih', 'l', 'm', 'iy', 'hh', 'w', 'eh', 'r', 'dh', 'ah', 'ah', 'v', 'f', 'ey', 't', 'ih', 'ng', 'r', 'uw']
322/15: ed.eval(stream_preds, chunk_preds)
322/16: ed.eval(stream_pred, chunk_preds)
322/17: stream_pred
322/18: arr[:,216:247,:]
322/19: [0,1,2,3,4,5,6][0:7]
322/20: arr.shape
322/21: arr[:,216:,:]
322/22: divmod(10,5)
322/23: 10%5
322/24: arr[:,5:248, :]
322/25: arr[:,5:249, :]
322/26: arr[:,5:500, :]
322/27: st = "hello my friend Peter"
322/28: st.replace("hello", "")
322/29: st.replace("hello ", "")
322/30: divmod(5, 3)
322/31: divmod(7-5, 3)
322/32: divmod(41-12, 2)
322/33: divmod(42-12, 2)
322/34: 5>=5
322/35: 5=>5
322/36: divmod(42-12, 2)
322/37: x, y = divmod(42-12, 2)
322/38: x
322/39: y
322/40: for _ in range(5): print(_)
322/41: import wave
322/42: audio_path
322/43: audio_path="/Users/dustin/CS/consulting/firstlayerai/phoneme_classification/src/awni_speech/speech/model_convert/audio_files/ST-out.wav"
322/44: wf = wave.open(audio_path, 'rb')
322/45:
def generator():
    yeild wf.readframes(256)
322/46:
def generator(audio_read):
    yeild audio_read.readframes(256)
322/47:
def generator():
    yield wf.readframes(256)
322/48: gen = generator()
322/49:
def generator():
    yield wf.readframes(2560)
322/50: gen = generator()
322/51:
for frames in gen:
    print(type(frames))
322/52:
for frames in gen:
    print(type(frames))
322/53: gen = generator()
322/54:
for frames in gen:
    print(type(frames))
322/55: gen = generator()
322/56:
for frames in gen:
    print(frames)
322/57:
for frames in gen:
    print(len(frames))
322/58:
gen = generator() 
for frames in gen:
    print(type(frames))
322/59:
gen = generator() 
for frames in gen:
    print(len(frames))
322/60:
gen = generator() 
for frames in gen:
    print(bool(frames))
322/61:
for frames in gen:
    print(bool(frames))
322/62:
gen = generator()
while frames:
    print(i)
    yield gen
322/63:
i=0
gen = generator()
while frames:
    print(i)
    frames = next(gen)
    i+=1
322/64:
i=0
gen = generator()
while frames:
    print(i)
    frames = next(gen)
    i+=1
322/65:
i=0
gen = generator()
frames = 1
while frames:
    print(i)
    frames = next(gen)
    i+=1
322/66:
gen = generator() 
for i in range(3):
    print(i)
    frames = next(gen)
322/67:
gen = generator() 
for frames in gen:
    print(bool(frames))
322/68:
gen = generator() 
for frames in gen:
    print(bool(frames))
322/69:
gen = generator() 
for frames in gen:
    print(bool(frames))
322/70:
for i in []:
    print(i)
322/71: import numpy as np
322/72: np.frombuffer(b'\x00\x00', np.int16)
322/73:
if b'':
    print(True)
322/74:
if ~b'':
    print(True)
322/75:
if not b'':
    print(True)
322/76:
if not b'0':
    print(True)
322/77:
if b'0':
    print(True)
322/78: np.frombuffer(b'\x00\x00', np.int16)
322/79: b'\x00' + b'\x00'
322/80: buff = b'\x00'
322/81: buff + buff
322/82: buff*5
322/83: np.frombuffer(b'\x00\x00', np.int16).sum()
322/84: np.frombuffer(buff*5, np.int16).sum()
322/85: np.frombuffer((buff*5), np.int16).sum()
322/86: buff*5
322/87: np.frombuffer(exec(buff*5), np.int16).sum()
322/88: np.frombuffer(b'\x00\x00\x00\x00\x00', np.int16).sum()
322/89: np.frombuffer((buff*6), np.int16).sum()
322/90: np.frombuffer((buff*512), np.int16).sum()
322/91: np.frombuffer((buff*512.0), np.int16).sum()
323/1: divmod(41-12,2)
323/2: import torch
323/3: torch.zeros(5,5)
323/4: torch.zeros(5,5, dtype=torch.float32)
323/5: torch.zeros(5,5, dtype=torch.float32, requires_grad=False)
323/6: torch.zeros((5,5), dtype=torch.float32, requires_grad=False)
323/7: torch.zeros((1,5,5), dtype=torch.float32, requires_grad=False)
323/8: torch.zeros((1,5,257), dtype=torch.float32, requires_grad=False)
323/9: v_pad = 5
323/10: torch.zeros((1,0,257), dtype=torch.float32, requires_grad=False)
323/11: torch.zeros((1,1,257), dtype=torch.float32, requires_grad=False)
323/12: final_padding = torch.zeros(1,1, 257, dytpe=torch.float32, requires_grad=False)
323/13: final_padding = torch.zeros(1,1, 257, dtype=torch.float32, requires_grad=False)
323/14: divmod(278-32, 2)
323/15: dct={'x':5-4}
323/16: dct
323/17: y = 4
323/18: dct={'x':5-y}
323/19: dct
323/20: dct.update(('z', dct[x]+1))
323/21: dct.update(('z', dct['x']+1))
323/22: dct.update(('z', 2))
323/23: dct.update('z', 2)
323/24: dct.update(('z':2))
323/25: dct.update({'z':dct['x']+1})
323/26: dct
323/27: divmod(277-46, 15)
323/28: (277-46)% 15
323/29: 277-46% 15
323/30: from speech.utils.wave import array_from_wave
323/31: audio_path = '/Users/dustin/CS/consulting/firstlayerai/phoneme_classification/src/awni_speech/speech/model_convert/audio_files/ST-out.wav'
323/32: audio_data, samp_rate = array_from_wave(audio_path)
323/33: feature_step = 256
323/34: remainder = audio_data.shape[0] % feature_step
323/35: remainder
323/36: zero_steps = np.zeros((remainder, audio_data.shape[1]), dtype=np.float32)
323/37: import numpy as np
323/38: zero_steps = np.zeros((remainder, audio_data.shape[1]), dtype=np.float32)
323/39: audio_data.shape
323/40: audio_data.shape[0]
323/41: audio_data.size
323/42: feature_window=512
323/43: remainder = (audio_data.shape[0] - feature_window) % feature_step
323/44: remainder
323/45: zero_steps = np.zeros((remainder, ), dtype=np.float32)
323/46: np.concatentate((audio_data, zero_steps), dim=0).shape
323/47: np.concatenate((audio_data, zero_steps), dim=0).shape
323/48: np.concatenate((audio_data, zero_steps), axis=0).shape
323/49: feature_step
323/50:     num_zeros = feature_step - remainder
323/51:     zero_steps = np.zeros((num_zeros, ), dtype=np.float32)
323/52: np.concatenate((audio_data, zero_steps), axis=0).shape
323/53: final_sample =  b'\x01\x00\x02\x00\x03\x00\x04\x00\x05\x00\x06\x00\x07\x00\x07\x00\x07\x00\x06\x00\x04\x00\x02\x00\x00\x00\xff\xff\xfe\xff\xfe\xff\xfe\xff\xff\xff\x01\x00\x03\x00\x04\x00\x05\x00\x05\x00\x04\x00\x03\x00\x02\x00\x00\x00\xff\xff\xff\xff\xff\xff\x00\x00\x01\x00\x02\x00\x03\x00\x03\x00\x03\x00\x02\x00\x01\x00\x00\x00\xff\xff\xfe\xff\xfd\xff'
323/54: len(final_sample)
323/55: zero_byte = b'\x00'
323/56: feature_window
323/57:
for _ in range(feature_step*2-len(final_sample)):
    final_sample += final_sample
323/58: len(final_sample)
323/59: final_sample =  b'\x01\x00\x02\x00\x03\x00\x04\x00\x05\x00\x06\x00\x07\x00\x07\x00\x07\x00\x06\x00\x04\x00\x02\x00\x00\x00\xff\xff\xfe\xff\xfe\xff\xfe\xff\xff\xff\x01\x00\x03\x00\x04\x00\x05\x00\x05\x00\x04\x00\x03\x00\x02\x00\x00\x00\xff\xff\xff\xff\xff\xff\x00\x00\x01\x00\x02\x00\x03\x00\x03\x00\x03\x00\x02\x00\x01\x00\x00\x00\xff\xff\xfe\xff\xfd\xff'
323/60: zero_byte
323/61: final_sample.join(zero_byte)
323/62: final_sample+=zero_byte
323/63: final_sample
323/64: final_sample =  b'\x01\x00\x02\x00\x03\x00\x04\x00\x05\x00\x06\x00\x07\x00\x07\x00\x07\x00\x06\x00\x04\x00\x02\x00\x00\x00\xff\xff\xfe\xff\xfe\xff\xfe\xff\xff\xff\x01\x00\x03\x00\x04\x00\x05\x00\x05\x00\x04\x00\x03\x00\x02\x00\x00\x00\xff\xff\xff\xff\xff\xff\x00\x00\x01\x00\x02\x00\x03\x00\x03\x00\x03\x00\x02\x00\x01\x00\x00\x00\xff\xff\xfe\xff\xfd\xff'
323/65: num_bytes = feature_steps*2 - len(final_sample)
323/66: num_bytes = feature_step*2 - len(final_sample)
323/67: num_bytes
323/68: 512 - 84
323/69: final_sample += zero_byte * num_bytes
323/70: len(final_sample)
323/71: model_path = "examples/librispeech/models/ctc_models/20200406/20200409"
323/72: ls
323/73: from speech.utils.io import load
323/74: model, _ = load(model_path)
323/75: model.conv.children()
323/76: list(model.conv.children())
323/77: dir(model)
323/78: dir(model.conv)
323/79: model.conv.named_children()
323/80: list(model.conv.named_children())
323/81: dict(model.conv.named_children())
323/82: list(model.conv.children())
323/83: dict(model.conv.named_children())
323/84: list(model.conv.children())[0].kernel_size
323/85: 64.__div__(8)
323/86: .__div__(64,8)
323/87: 5.__add__ (5)
323/88: 5.__add__(5)
323/89: import operator
323/90: operator.__add__(4)
323/91: operator.__add__(2,4)
323/92: operator.__div__(64,4)
323/93: operator.__truediv__(64,4)
323/94: operator.__floordiv__(64,4)
323/95: operator.__truediv__(64,5)
323/96: operator.__floordiv__(64,5)
323/97: operator.floordiv(64,5)
323/98: operator.IndexOf("taam", "m")
323/99: operator.indexOf("taam", "m")
323/100: audiofile_dir = '/Users/dustin/CS/consulting/firstlayerai/phoneme_classification/src/awni_speech/speech/model_convert/audio_files/Validatio-audio_2020-05-21'
323/101: import os
323/102: os.listdir(audiofile_dir)
323/103: audio_path
323/104: from speech.utils.wave import wav_duration
323/105: from speech.utils.wave import array_from_wav
323/106: from speech.utils.wave import array_from_wave
323/107: audio_data, samp_rate = array_from_wave(audio_path)
323/108: audio_path="/Users/dustin/CS/consulting/firstlayerai/phoneme_classification/src/awni_speech/speech/model_convert/audio_files/Validatio-audio_2020-05-21/Speak-out.wav"
323/109: audio_data, samp_rate = array_from_wave(audio_path)
323/110: audio_data.shape
323/111: speak_test_path="/Users/dustin/Downloads/speak_test_1.wav"
323/112: speak_test_data, samp_rate = array_from_wave(speak_test_path)
323/113: speak_test_data.shape
323/114: import datetime
323/115: datetime.today()
323/116: from datetime import datetime
323/117: datetime.today()
323/118: str(datetime.today())
323/119: str(datetime.date())
323/120: from datetime import date
323/121: date.today()
323/122: str(date.today())
323/123: l=[(1,2), (3,4), (5,6)]
323/124:
for one, two in l:
    print(one)
    print(two)
    print("-")
323/125: isinstance('test', str)
323/126: type('test')==str
323/127: import unicodedata
323/128: "here i am".replace("i", "")
323/129: unk_token="<unk>"
323/130: "<unk> my husband orlando and i".replace(unk_token, "")
323/131: "<unk> my husband orlando and i".replace(unk_token, "").strip()
323/132: "my <unk> husband orlando and i".replace(unk_token, "").strip()
323/133: "my husband orlando and i <unk>".replace(unk_token, "").strip()
323/134: "my husband orlando and i <unk>           ".replace(unk_token, "").strip()
323/135: "my husband orlando and i <unk>           ".replace(unk_token, "").strip()
323/136: os
323/137: os.path.splitext("dustin.dustin")
323/138: [{"start":1, "end":5}, {"start":1, "end":2}]
323/139: [{"start_time":1, "end_time":5}, {"start_time":1, "end_time":2}]
323/140: min_len_sec = 2
323/141: (utterance_info["end_time"] - utterance_info["start_time"]) > min_len_sec
323/142: (utterance_info["end_time"] - utterance_info["start_time"]) > min_len_sec
323/143: date
323/144: "all_unk_words_{}.txt".format(str(date.today()))
323/145: ord('0x6F18DBA5')
323/146: hex(0x6F18DBA5)
323/147: hex('0x6F18DBA5')
323/148: int(0x6F18DBA5)
323/149: hex(1863900069)
323/150: ord('d)
323/151: ord('d')
323/152: ord('r')
323/153: ord('z')
324/1: "https://zenodo.org/record/1227121/files/{}?download=1"
324/2: x = "https://zenodo.org/record/1227121/files/{}?download=1"
324/3: x.format(4)
324/4: import urllib
324/5: download_link = "https://zenodo.org/record/1227121/files/{}?download=1"
324/6: download_link = "https://zenodo.org/record/1227121/files/{}?download=1".format("TMETRO_16k.zip")
324/7:  urllib.request.urlretrieve(download_link, filename="./")
324/8: ls
324/9:  urllib.request.urlretrieve(download_link, filename="./test.zip")
324/10: ls
324/11: ls
324/12: save_path="./test.zip"
324/13: import tarfile
324/14: with tarfile.open(save_path) as tf:tf.extractall(path=save_dir)
324/15: with tarfile.open(save_path) as tf:tf.extractall(path=save_dir)
324/16: with tarfile.open(save_path) as tf:tf.extractall(path=save_dir)
324/17:
with tarfile.open(save_path) as tf:
    tf.extractall(path=save_dir)
324/18: import unzip
324/19: import zipfile
324/20:
with zip.open(save_path) as tf:
    tf.extractall(path=save_dir)
324/21:
with zipfile.open(save_path) as tf:
    tf.extractall(path=save_dir)
324/22: from zipfile import Zipfile
324/23: from zipfile import ZipFile
324/24:
with ZipFile.open(save_path) as tf:
    tf.extractall(path=save_dir)
324/25:
with ZipFile.open(save_path, 'r') as tf:
    tf.extractall(path=save_dir)
324/26:
with ZipFile.open(save_path, 'r') as tf:
    tf.extractall(path=save_dir)
     
    
    save_path
324/27: save_path
324/28:
with ZipFile(save_path, 'r') as tf:
    tf.extractall(path=save_dir)
324/29:
with ZipFile(save_path, 'r') as tf:
    tf.extractall(path='./')
324/30: ls
324/31: pattern = "*.zip"
324/32: import glob
324/33: files = glob.glob("/"+pattern)
324/34: files
324/35: ls
324/36: files = glob.glob("./"+pattern)
324/37: files
324/38: demand_dir="/Users/dustin/CS/consulting/firstlayerai/data/background_noise/demand"
324/39: pattern = "*/*.wav"
324/40: wav_files = glob.glob(os.path.join(demand_dir, pattern))
324/41: import os
324/42: wav_files = glob.glob(os.path.join(demand_dir, pattern))
324/43: wav_files[0]
324/44: wav_path = '/Users/dustin/CS/consulting/firstlayerai/data/background_noise/demand/NRIVER/ch08.wav'
324/45: basename = os.path.split(wav_path)
324/46: basename
324/47: basename = os.path.basename(wav_path)
324/48: basename
324/49: basename = os.path.dirname(wav_path)
324/50: basename
324/51: dirname = os.path.basename(os.path.dirname(wav_path))
324/52: dirname
324/53:  filename = os.path.basename(wav_path)
324/54: filename
324/55: test_noise = "/Users/dustin/CS/consulting/firstlayerai/data/background_noise/demand/SCAFE/ch01.wav"
324/56: test_speak="/Users/dustin/Desktop/test_speak_test.txt"
324/57: import csv
324/58:
with open(test_speak, "r") as fid:
    reader = csv.reader(fid, delimiter=',')
    for line in reader:
        print(line[0])
        print(line[1])
        print("--------")
324/59:
with open(test_speak, "r") as fid:
    reader = csv.reader(fid, delimiter=',')
    for line in reader:
        assert(len(line)==2)
        print(line[0])
        print(line[1])
        print("--------")
324/60: import os
324/61: os.path.sep
324/62: os.path.ext
324/63: os.path.extsep
324/64: speak_test="/Users/dustin/CS/consulting/firstlayerai/data/speak_test_data/2020-05-27/speak-test_2020-05-27.json"
324/65: import json
324/66:
phone_set = {}
with open(speak_test, 'r') as fid: 
    for line in fid:
        sample = json.load(line)
        phone_set.add(sample['text'])
324/67: speak_test
324/68:
phone_set = {}
with open(speak_test, 'r') as fid: 
    for line in fid:
        sample = json.loads(line)
        phone_set.add(sample['text'])
324/69:
phone_set = set()
with open(speak_test, 'r') as fid: 
    for line in fid:
        sample = json.loads(line)
        phone_set.add(sample['text'])
324/70: set_1  = set()
324/71: set.add(1)
324/72: set.append(1)
324/73: set_1.add(1)
324/74: set_1
324/75:
phone_set = set()
with open(speak_test, 'r') as fid: 
    for line in fid:
        sample = json.loads(line)
        print(sample['text'])
324/76: set_1.add([1])
324/77: set_1.add(set([1]))
324/78: set_1.update(set([1]))
324/79: set_1
324/80: set_1.update([2])
324/81: set_1
324/82:
phone_set = set()
with open(speak_test, 'r') as fid: 
    for line in fid:
        sample = json.loads(line)
        phone_set.update(sample['text'])
324/83: len(phone_set)
324/84: phone_set
324/85: audiopath ="/Users/dustin/CS/consulting/firstlayerai/data/speak_test_data/2019-11-29/audio/8MGMTQST57Zb3BYmobE7V999Zi53-1574725149.wv"
324/86: audiopath ="/Users/dustin/CS/consulting/firstlayerai/data/speak_test_data/2019-11-29/audio/8MGMTQST57Zb3BYmobE7V999Zi53-1574725149.wv".split("data")
324/87: audiopath ="/Users/dustin/CS/consulting/firstlayerai/data/speak_test_data/2019-11-29/audio/8MGMTQST57Zb3BYmobE7V999Zi53-1574725149.wv"
324/88: "/Users/dustin/CS/consulting/firstlayerai/data/speak_test_data/2019-11-29/audio/8MGMTQST57Zb3BYmobE7V999Zi53-1574725149.wv".split("data")
324/89: "/Users/dustin/CS/consulting/firstlayerai/data/speak_test_data/2019-11-29/audio/8MGMTQST57Zb3BYmobE7V999Zi53-1574725149.wv".split("/data/")
324/90: l_1 = ["w", "eh", "n", "s", "d", "ah", "k", "l", "ow", "z", "ih", "ng", "t", "ay"]
324/91: l_1 ==["w", "eh", "n", "s", "d", "ah", "k", "l", "ow", "z", "ih", "ng", "t", "ay"]
324/92: 0 >0
324/93: audio_path
324/94: audiopath
324/95: audio_path = audiopath
324/96: os
324/97: os.path.split(audio_path)
324/98: audio_path
324/99: os.path.splitext(audio_path)
324/100: round(0.454545, 3)
324/101: pred_path ="~/CS/consulting/firstlayerai/phoneme_classification/src/awni_speech/speech/predictions/20200406-20200409_speak-test-20200527_predictions_fn.json"
324/102: from speech.utils.io import read_data_json
324/103: preds = read_data_json(pred_path)
324/104: pred_path ="./predictions/20200406-20200409_speak-test-20200527_predictions_fn.json"
324/105: preds = read_data_json(pred_path)
324/106: preds[0]
324/107: sort_preds = sorted(preds, key=lambda x: x['PER'], reverse=True)
324/108: sort_preds[0]
324/109: sort_preds[:10]
324/110: model_path = "examples/librispeech/models/ctc_models/20200406/20200408"
324/111: import speech
324/112: model, preproc = speech.load(model_path)
324/113: model
324/114: model.ctc_train
324/115: type(model)
324/116: os.path.extsep
324/117: " ".join(['d', 'ey', 'l', 'ah', 'k', 't', 'ih', 't'])
324/118: label = "ay n uw ah n d ay n uw d ey w uh d ah p eh n d ah p t ah g eh d er".split()
324/119: label
324/120: json_path = "/Users/dustin/CS/consulting/firstlayerai/phoneme_classification/src/awni_speech/speech/predictions/20200406-20200409_speak-test-20200527_predictions_fn.json"
324/121: dataset_path = "/Users/dustin/CS/consulting/firstlayerai/data/speak_test_data/2020-05-27/speak-test-clean_2020-05-27.csv"
324/122: import json
324/123: import csv
324/124:
with open(json_path, 'r') as jid:
    per_json = [json.loads(sample) for sample in jid]
324/125: per_json[0]
324/126:
import os
with open("per_filename.csv", 'w') as outid:
    with open(dataset_path, 'r') as did:
        reader = csv.reader(did, delimiter=',')
        for sample in reader:
            for line in per_json:
                filename = os.path.splitext(os.path.split(line['filename'])[1])[0]
                if sample[0] == filename:
                    outid.write(sample[0], line['PER'])
                    outid.write('\n')
324/127:
import os
with open("per_filename.csv", 'w') as outid:
    with open(dataset_path, 'r') as did:
        reader = csv.reader(did, delimiter=',')
        for sample in reader:
            for line in per_json:
                filename = os.path.splitext(os.path.split(line['filename'])[1])[0]
                if sample[0] == filename:
                    string = f"{sample[0]}, {line['PER']}"
                    outid.write(string)
                    outid.write('\n')
325/1: "f > r; y > blank; aw > ao; blank > l; blank > d, blank > v; dh > d; ah > ao; r > er"
325/2: string = "f > r; y > blank; aw > ao; blank > l; blank > d, blank > v; dh > d; ah > ao; r > er"
325/3: string.split(';')
325/4: string.split(';').strip()
325/5: string.split(';')[0].split('>')
325/6: import editdistance as ed
325/7: label = "ay k ae n m ae n ah jh t uw g eh t b ay aa n m ay s ey v ih ng".split()
325/8: label
325/9: pred = "ah                                            ah ah ah ah ah ah ah ah".split()
325/10: ed.eval(label, pred)
325/11: ed.eval(label, pred[:-1])
325/12: ed.eval(label, pred[:-2])
325/13: ed.eval(label, pred[:-3])
325/14: ed.eval(label, pred[:1])
325/15: ed.eval(label, pred[:1])
325/16: ed.eval(label, pred[:1])
325/17: ed.eval(label, pred[0])
325/18: 4 < None
326/1: from savings import *
326/2: import json
326/3: json_path = "/Users/dustin/CS/job_projects/prep/vorto/AIEngineerChallenge/testData/testBig.json"
326/4:
data_list = list()
with open(json_path, 'r') as fid:
    data_list = [json.loads(node) for node in fid]
326/5: len(data_list)
326/6: len(data_list[0])
326/7:
data_list = list()
with open(json_path, 'r') as fid:
    data_list = fid
326/8: data_list[1]
326/9:
data_list = list()
with open(json_path, 'r') as fid:
    data_list = json.loads(fid)
326/10:
data_list = list()
with open(json_path, 'r') as fid:
    data_list = json.load(fid)
326/11: data_list
326/12: data_list[1]
326/13: data_list[0]
326/14: data_list[0]['ID']
326/15: data_list[0]['Location']
326/16: len(data_list)
326/17: list(list())
326/18:
data_list = list()
with open(json_path, 'r') as fid:
    data_list = json.load(fid)
326/19: import math
326/20: math.sqrt(25)
326/21: math.sqrt(4^2)
326/22: 4^2
326/23: 4**6
326/24: 4**2
326/25: math.sqrt(4**2)
326/26:
def distance(from_location, to_location):
        delta_x = from_location[0] - to_location[0]
        delta_y = from_location[1] - to_location[1]

        return math.sqrt(delta_x**2 + delta_y**2)
326/27: distance([262.239249399863, 147.782999207266], [115.901949768886, 43.6036861734465])
326/28: 5 >=5
326/29: distance_arr = dict()
326/30: distance_arr[999].update({1000: 179.6327768740574})
326/31: distance_arr.update({999:{1000: 179.6327768740574}})
326/32: distance_arr
326/33: distance_arr.update({999:{998: 180}})
326/34: distance_arr
326/35: distance_arr[999].update({997: 179}})
326/36: distance_arr[999].update({997: 179})
326/37: distance_arr
326/38: distance_arr.get(1, {})
326/39: distance_arr.get(1, {}).update({2:32})
326/40: distance_arr
326/41: distance_arry.get(x, None)
326/42: distance_arr.get(x, None)
326/43: distance_arr.get(1, None)
326/44: distance_arr.get(1, None) is None
326/45: distance_arr.get(1, None) is None
326/46: len(distance_arr)
326/47: distance_arr
326/48: len({})
326/49: not (0 and 0)
326/50: not (1 and 0)
326/51: not (1 or 0)
326/52: not (0 or 0)
326/53: not (1 or 0)
326/54: not (1 or 1)
326/55: not (0 or 1)
326/56: l = []
326/57: l.append(5)
326/58: l
326/59: l.append(5)
326/60: l
326/61: l.append([5])
326/62: l
326/63: l
326/64: len(l)
326/65: del l[0]
326/66: l
326/67: del l[1]
326/68: l
326/69: None is not None
326/70: None is not None or None is None
326/71: l
326/72: l.append(4)
326/73: l
326/74: l
326/75: l.append(3)
326/76: l.append(2)
326/77: l.append(1)
326/78: l
326/79: l[-1]
326/80: l[-2]
326/81: l
326/82: l.insert(6, 1)
326/83: l
326/84: l.insert(1, 6)
326/85: l
326/86: l.insert(-1, 6)
326/87: l
326/88: l.insert(-1, 7)
326/89: l
326/90: l.insert(1, 8)
326/91: l
326/92: l.insert(0, 8)
326/93: l
326/94: l.insert(0, 1)
326/95: l
326/96: m = [1, 12, 13, 14, 7, 1]
326/97: l[:-2]
326/98: comboo = l[:-2] + m[1:]
326/99: comboo
326/100: l1 = [1,2,3,4,5,1]
326/101: l2 = [1,2,7,8,9,1]
326/102: l1.reverse()
326/103: l1
326/104: l1.reverse()[1]
326/105: l1.reversed()[1]
326/106: l1
326/107: l1.reverse()
326/108: l1
326/109: l1[:-2]
326/110: l2
326/111: l2[1:]
326/112: l2
326/113: l1
326/114: l2
326/115: l2.reverse()
326/116: l1
326/117: l1
326/118: l1.reverse()
326/119: l1
326/120: l2
326/121: l2[:-2]
326/122: l2[:-2] + l1[1:]
326/123: mit_example = {1:{1:0, 2:25, 3:43, 4:57, 5:43, 6:61, 7:29, 8:41, 9:48, 10:71}, 2:{1:25, 2:0, 3:29, 4:34, 5:43, 6:68, 7:49, 8:66, 9:72, 10:91}, 3:{1:43, 2:29, 3:0, 4:52, 5:72, 6:96, 7:72, 8:81, 9:89, 10:114}, 4:{1:57, 2:34, 3:52, 4:0, 5:45, 6:71, 7:71, 8:95, 9:99, 10:108}, 5:{1:43, 2:43, 3:72, 4:45, 5:0, 6:27, 7:36, 8:65, 9:65, 10:65}, 6:{1:61, 2:68, 3:96, 4:71, 5:27, 6:0, 7:40, 8:66, 9:62, 10:46}, 7:{1:29, 2:49, 3:72, 4:71, 5:36, 6:40, 7:0, 8:31, 9:31, 10:43}, 8:{1:41, 2:66, 3:81, 4:95, 5:65, 6:66, 7:31, 8:0, 9:11, 10:46}, 9:{1:48, 2:72, 3:89, 4:99, 5:65, 6:62, 7:31, 8:11, 9:0, 10:36}, 10:{1:71, 2:91, 3:114, 4:108, 5:65, 6:46, 7:43, 8:46, 9:36, 10:0}}
326/124:
for row in mit_example:
    for col in mit_example
        assert mit_exapmle[row][col] == mit_example[col][row]
326/125:
for row in mit_example:
    for col in mit_example:
        assert mit_example[row][col] == mit_example[col][row]
326/126: ls
326/127: import savings
326/128: import savings
326/129: import importlib
326/130: importlib.reload(savings)
326/131: from importlib import importlib
326/132: reload(savings)
326/133: savings
326/134: import savings
326/135: import savings
326/136: import savings
326/137: l
326/138: check_exterior(l, 8)
326/139: savings.check_exterior(l, 8)
326/140: savings.check_exterior(l, 7)
326/141: savings.check_exterior(l, 1)
326/142: savings.check_exterior(l, 1) is None
326/143: savings.check_exterior(l, 19) is None
326/144: savings.check_exterior(l, 4) is None
326/145: m
326/146: shifts= [l, m, [1, 38, 46, 58, 27, 1]]
326/147: reload(savings)
326/148: savings.find_shifts(shifts, 38)
326/149: savings.find_shifts(shifts, 58)
326/150: l
326/151: m
326/152: savings.find_shifts(shifts, 6)
326/153: savings.find_shifts(shifts, 77)
326/154: reload(savings)
326/155: savings.find_shifts(shifts, 77)
326/156: savings.find_shifts(shifts, 6)
326/157: shifts
326/158: del shifts[0][4]
326/159: shifts
326/160: savings.find_shifts(shifts, 6)
326/161: shifts
326/162: del shifts[0][1]
326/163: shifts
326/164: del shifts[0][8]
326/165: shifts
326/166: del shifts[0][6]
326/167: shifts
326/168: savings.find_shifts(shifts, 6)
326/169: reload(savings)
326/170: savings.find_shifts(shifts, 6)
326/171: shifts
326/172: reload(savings)
326/173: savings.find_shifts(shifts, 6)
326/174: savings.find_shifts(shifts, 12)
326/175: savings.find_shifts(shifts, 38)
326/176: savings.find_shifts(shifts, 1)
326/177: reload(savings)
326/178: savings.find_shifts(shifts, 6)
326/179: savings.find_shifts(shifts, 38)
326/180: savings.find_shifts(shifts, 1)
326/181: shifts
326/182: mit_example
326/183: json_path = "/Users/dustin/CS/job_projects/prep/vorto/AIEngineerChallenge/testData/testBig.json"
326/184: nodes  = NodeData(json_path)
326/185: nodes  = savings.NodeData(json_path)
326/186: nodes.distance_arr
326/187: reload(savings)
326/188: nodes  = savings.NodeData(json_path)
326/189: nodes.set_distance_arr(mit_example)
326/190: nodes.distance_arr
326/191: nodes.calc_ranked_savings()
326/192: mit_example
326/193: mit_copy = mit_example
326/194: mit_copy
326/195:
for row_index in mit_copy:
    mit_copy[row_index-1] = mit_copy[row_index] 
    for col in mit_copy[row_index-1]:
        mit_copy[row_index-1][col-1] = mit_copy[row_index-1][col]
326/196:
out_copy = dict()
for row_index in mit_copy:
    out_copy[row_index-1] = mit_copy[row_index] 
    for col in mit_copy[row_index-1]:
        out_copy[row_index-1][col-1] = mit_copy[row_index-1][col]
326/197: mit_copy
326/198: mit_example
326/199: mit_example
326/200: mit_example
326/201: mit_example
326/202: np.core.arrayprint._line_width = 160
326/203: import numpy as np
326/204: np.core.arrayprint._line_width = 160
326/205: mit_example
326/206: mit_example
326/207: mit_example
326/208: c.PlainTextFormatter.max_width = 160
326/209: import c
326/210: mit_example
326/211: len(mit_example)
326/212: mit_example[0]
326/213: mit_example[1]
326/214: del mit_example[1]
326/215: mit_example[0]
326/216: mit_example[1]
326/217: mit_example[1] = mit_example[0]
326/218: mit_example[1]
326/219: del mit_example[0]
326/220: del mit_example[1][0]
326/221: mit_example[1]
326/222: mit_example
326/223:
out_copy = dict()
for row_index in mit_example:
    if row_index ==1:
        out_copy.update({row_index-1: mit_example})
    else:
        out_copy[row_index-1] = mit_example[row_index] 

    for col in mit_example[row_index-1]:
        out_copy[row_index-1][col-1] = mit_example[row_index-1][col]
326/224: row_index
326/225: mit_example
326/226: mit_example_backup = mit_example
326/227: mit_example[1][1] = -40
326/228: mit_example
326/229: mit_example_backup
326/230: mit_example[1][1] = 0
326/231: mit_example.copy()
326/232: mit_example_backup_mit_example.copy()
326/233: mit_example_backup = mit_example.copy()
326/234: mit_example_backup
326/235: mit_example[1][1] = -40
326/236: mit_example
326/237: mit_example_backup
326/238: mit_example_test = mit_example_backup.deepcopy()
326/239: mit_example_test = deepcopy(mit_example_backup)
326/240: coopy
326/241: copy
326/242: import copy
326/243: exp_copy = copy.deepcopy(mit_example_backup)
326/244: exp_copy
326/245: mit_example[1][1] = 0
326/246: exp_copy
326/247: mit_example
326/248: mit_example_backup
326/249: mit_example_backup = copy.deepcopy(mit_example)
326/250: mit_example_backup
326/251:
out_copy = dict()
for row_index in mit_example:
    if row_index ==1:
        out_copy.update({row_index-1: mit_example})
    else:
        out_copy[row_index-1] = mit_example[row_index] 

    for col in mit_example[row_index-1]:
        out_copy[row_index-1][col-1] = mit_example[row_index-1][col]
326/252:
out_copy = dict()
for row_index in mit_example:
    if row_index ==1:
        out_copy.update({row_index-1: mit_example})
    else:
        out_copy[row_index-1] = mit_example[row_index] 

    for col in out_copy[row_index-1]:
        out_copy[row_index-1][col-1] = mit_example[row_index][col]
326/253: mit_example
326/254: mit_example[0]
326/255: del mit_example[0]
326/256: mit_example
326/257:
out_copy = dict()
for row_index in mit_example:
    if row_index ==1:
        out_copy.update({row_index-1: mit_example})
    else:
        out_copy[row_index-1] = copy.deepcopy(mit_example[row_index])

    for col in out_copy[row_index-1]:
        out_copy[row_index-1][col-1] = copy.deepcopy(mit_example[row_index][col])
326/258: mit_example
326/259: del mit_example[0]
326/260:
out_copy = dict()
for row_index in mit_example:
    if row_index ==1:
        out_copy.update({row_index-1: mit_example[row_index]})
    else:
        out_copy[row_index-1] = copy.deepcopy(mit_example[row_index])

    for col in out_copy[row_index-1]:
        out_copy[row_index-1][col-1] = copy.deepcopy(mit_example[row_index][col])
326/261:
out_copy = dict()
for row_index in mit_example:
    if row_index ==1:
        out_copy.update({row_index-1: mit_example[row_index]})
    else:
        out_copy[row_index-1] = copy.deepcopy(mit_example[row_index])

    for col in out_copy[row_index-1]:
        out_copy[row_index-1][col-1] = copy.deepcopy(mit_example[row_index][col])
326/262: mit_example
326/263: del mit_example[1][-1]
326/264: del mit_example[1][0]
326/265: mit_example
326/266:
mit_example = {0: {0: 0, 1: 25, 2: 43, 3: 57, 4: 43, 5: 61, 6: 29, 7: 41, 8: 48, 9: 71},
 1: {0: 25, 1: 0, 2: 29, 3: 34, 4: 43, 5: 68, 6: 49, 7: 66, 8: 72, 9: 91},
 2: {0: 43, 1: 29, 2: 0, 3: 52, 4: 72, 5: 96, 6: 72, 7: 81, 8: 89, 9: 114},
 3: {0: 57, 1: 34, 2: 52, 3: 0, 4: 45, 5: 71, 6: 71, 7: 95, 8: 99, 9: 108},
 4: {0: 43, 1: 43, 2: 72, 3: 45, 4: 0, 5: 27, 6: 36, 7: 65, 8: 65, 9: 65},
 5: {0: 61, 1: 68, 2: 96, 3: 71, 4: 27, 5: 0, 6: 40, 7: 66, 8: 62, 9: 46},
 6: {0: 29, 1: 49, 2: 72, 3: 71, 4: 36, 5: 40, 6: 0, 7: 31, 8: 31, 9: 43},
 7: {0: 41, 1: 66, 2: 81, 3: 95, 4: 65, 5: 66, 6: 31, 7: 0, 8: 11, 9: 46},
 8: {0: 48, 1: 72, 2: 89, 3: 99, 4: 65, 5: 62, 6: 31, 7: 11, 8: 0, 9: 36}, 9: {0: 71, 1: 91, 2: 114, 3: 108, 4: 65, 5: 46, 6: 43, 7: 46, 8: 36, 9: 0}}
326/267: mit_example
326/268: nodes
326/269: nodes.set_distance_arr(mit_example)
326/270: nodes.distance_arr
326/271: nodes.calc_ranked_savings()
326/272: shifts_book = clarke_wright_routing(nodes.savings_list, len(nodes.distance_arr))
326/273: shifts_book = savings.clarke_wright_routing(nodes.savings_list, len(nodes.distance_arr))
326/274: shifts_book
326/275: import logging
326/276: x=5
326/277: logging.debug("test")
326/278: logging.debug("test")
326/279: def test():logging.debug("test")
326/280: test()
326/281: reload(savings)
326/282: shifts_book = savings.clarke_wright_routing(nodes.savings_list, len(nodes.distance_arr))
326/283: reload(savings)
326/284: shifts_book = savings.clarke_wright_routing(nodes.savings_list, len(nodes.distance_arr))
326/285: nodes.savings_list
326/286: shifts_book = savings.clarke_wright_routing(nodes.savings_list, len(nodes.distance_arr))
326/287: shifts_book = savings.clarke_wright_routing(nodes.savings_list, len(nodes.distance_arr))
326/288: reload(savings)
326/289: shifts_book = savings.clarke_wright_routing(nodes.savings_list, len(nodes.distance_arr))
326/290: shifts_book = savings.clarke_wright_routing(nodes.savings_list, len(nodes.distance_arr))
326/291: reload(savings)
326/292: shifts_book = savings.clarke_wright_routing(nodes.savings_list, len(nodes.distance_arr))
326/293: reload(savings)
326/294: shifts_book = savings.clarke_wright_routing(nodes.savings_list, len(nodes.distance_arr))
326/295: reload(savings)
326/296: shifts_book = savings.clarke_wright_routing(nodes.savings_list, len(nodes.distance_arr))
326/297: shifts_book = savings.clarke_wright_routing(nodes.savings_list, len(nodes.distance_arr))
326/298: shifts_book = savings.clarke_wright_routing(nodes.savings_list, len(nodes.distance_arr))
326/299: shifts_book = savings.clarke_wright_routing(nodes.savings_list, len(nodes.distance_arr))
326/300: shifts_book = savings.clarke_wright_routing(nodes.savings_list, len(nodes.distance_arr))
326/301: shifts_book = savings.clarke_wright_routing(nodes.savings_list, len(nodes.distance_arr))
326/302: reload(savings)
326/303: shifts_book = savings.clarke_wright_routing(nodes.savings_list, len(nodes.distance_arr))
326/304: reload(savings)
326/305: shifts_book = savings.clarke_wright_routing(nodes.savings_list, len(nodes.distance_arr))
326/306: reload(savings)
326/307: shifts_book = savings.clarke_wright_routing(nodes.savings_list, len(nodes.distance_arr))
326/308: shifts_book = savings.clarke_wright_routing(nodes.savings_list, len(nodes.distance_arr))
326/309: reload(savings)
326/310: shifts_book = savings.clarke_wright_routing(nodes.savings_list, len(nodes.distance_arr))
326/311: reload(savings)
326/312: shifts_book = savings.clarke_wright_routing(nodes.savings_list, len(nodes.distance_arr))
326/313: reload(savings)
326/314: shifts_book = savings.clarke_wright_routing(nodes.savings_list, len(nodes.distance_arr))
326/315: shifts_book = savings.clarke_wright_routing(nodes.savings_list, len(nodes.distance_arr))
326/316: reload(savings)
326/317: reload(savings)
326/318: shifts_book = savings.clarke_wright_routing(nodes.savings_list, len(nodes.distance_arr))
326/319: 10 < 10
326/320: shifts_book = savings.clarke_wright_routing(nodes.savings_list, len(nodes.distance_arr))
326/321: reload(savings)
326/322: shifts_book = savings.clarke_wright_routing(nodes.savings_list, len(nodes.distance_arr))
326/323: reload(savings)
326/324: shifts_book = savings.clarke_wright_routing(nodes.savings_list, len(nodes.distance_arr))
326/325: shifts_book = savings.clarke_wright_routing(nodes.savings_list, len(nodes.distance_arr))
326/326: reload(savings)
326/327: shifts_book = savings.clarke_wright_routing(nodes.savings_list, len(nodes.distance_arr))
326/328: reload(savings)
326/329: shifts_book = savings.clarke_wright_routing(nodes.savings_list, len(nodes.distance_arr))
326/330: reload(savings)
326/331: shifts_book = savings.clarke_wright_routing(nodes.savings_list, len(nodes.distance_arr))
326/332: reload(savings)
326/333: shifts_book = savings.clarke_wright_routing(nodes.savings_list, len(nodes.distance_arr))
326/334: shifts_book = savings.clarke_wright_routing(nodes.savings_list, len(nodes.distance_arr))
326/335: reload(savings)
326/336: reload(savings)
326/337: shifts_book = savings.clarke_wright_routing(nodes.savings_list, len(nodes.distance_arr))
326/338: reload(savings)
326/339: shifts_book = savings.clarke_wright_routing(nodes.savings_list, len(nodes.distance_arr))
326/340: reload(savings)
326/341: shifts_book = savings.clarke_wright_routing(nodes.savings_list, len(nodes.distance_arr))
326/342: reload(savings)
326/343: shifts_book = savings.clarke_wright_routing(nodes.savings_list, len(nodes.distance_arr))
326/344: reload(savings)
326/345: shifts_book = savings.clarke_wright_routing(nodes.savings_list, len(nodes.distance_arr))
326/346: reload(savings)
326/347: shifts_book = savings.clarke_wright_routing(nodes.savings_list, len(nodes.distance_arr))
326/348: reload(savings)
326/349: shifts_book = savings.clarke_wright_routing(nodes.savings_list, len(nodes.distance_arr))
326/350: shifts_book = savings.clarke_wright_routing(nodes.savings_list, len(nodes.distance_arr))
326/351: reload(savings)
326/352: shifts_book = savings.clarke_wright_routing(nodes.savings_list, len(nodes.distance_arr))
326/353: reload(savings)
326/354: shifts_book = savings.clarke_wright_routing(nodes.savings_list, len(nodes.distance_arr))
326/355: reload(savings)
326/356: shifts_book = savings.clarke_wright_routing(nodes.savings_list, len(nodes.distance_arr))
326/357: reload(savings)
326/358: shifts_book = savings.clarke_wright_routing(nodes.savings_list, len(nodes.distance_arr))
326/359: reload(savings)
326/360: shifts_book = savings.clarke_wright_routing(nodes.savings_list, len(nodes.distance_arr))
326/361: shifts_book = savings.clarke_wright_routing(nodes.savings_list, len(nodes.distance_arr))
326/362: reload(savings)
326/363: shifts_book = savings.clarke_wright_routing(nodes.savings_list, len(nodes.distance_arr))
326/364: reload(savings)
326/365: shifts_book = savings.clarke_wright_routing(nodes.savings_list, len(nodes.distance_arr))
326/366: shifts_book = savings.clarke_wright_routing(nodes.savings_list, len(nodes.distance_arr))
326/367: reload(savings)
326/368: shifts_book = savings.clarke_wright_routing(nodes.savings_list, len(nodes.distance_arr))
326/369: reload(savings)
326/370: shifts_book = savings.clarke_wright_routing(nodes.savings_list, len(nodes.distance_arr))
326/371: l
326/372: import list
326/373: mit_example
326/374: l
326/375: l[-1]
326/376: reload(savings)
326/377: shifts_book = savings.clarke_wright_routing(nodes.savings_list, len(nodes.distance_arr))
326/378: reload(savings)
326/379: shifts_book = savings.clarke_wright_routing(nodes.savings_list, len(nodes.distance_arr))
326/380: reload(savings)
326/381: shifts_book = savings.clarke_wright_routing(nodes)
326/382: reload(savings)
326/383: shifts_book = savings.clarke_wright_routing(nodes)
326/384: reload(savings)
326/385: shifts_book = savings.clarke_wright_routing(nodes)
326/386: reload(savings)
326/387: shifts_book = savings.clarke_wright_routing(nodes)
326/388: reload(savings)
326/389: shifts_book = savings.clarke_wright_routing(nodes)
326/390: reload(savings)
326/391: shifts_book = savings.clarke_wright_routing(nodes)
326/392: nodes
326/393: nodes.get_distance(0, 5)
326/394: nodes.distance_arr
326/395: mit_example
326/396: nodes  = NodeData(json_path)
326/397: nodes  = savings.NodeData(json_path)
326/398: nodes.set_distance_arr(mit_example)
326/399: nodes.distance_arr
326/400: shifts_book = savings.clarke_wright_routing(nodes)
326/401: reload(savings)
326/402: shifts_book = savings.clarke_wright_routing(nodes)
326/403: nodes.get_distance(0,5)
326/404: nodes.distance_arr
326/405: nodes.calc_ranked_savings()
326/406: shifts_book = savings.clarke_wright_routing(nodes)
326/407: nodes.calc_ranked_savings()
326/408: reload(savings)
326/409: shifts_book = savings.clarke_wright_routing(nodes)
326/410: json_nodes = savings.NodeData(json_path)
326/411: json_nodes.calc_distance_arr()
326/412: json_nodes.calc_ranked_savings()
326/413: json_nodes.get_distance(0, 305)
326/414: json_nodes.get_distance(305, 695)
326/415: json_nodes.get_distance(695, 0)
325/19: ls
325/20: model_path = "examples/librispeech/models/ctc_models/20200406/20200408"
325/21: from speech import io
325/22: from speech.utils import io
325/23: model, preproc = io.load(model_path)
325/24: model
325/25: model.paramaters()
325/26: model.parameters()
325/27: model.parameters
325/28: [model.parameters()]
325/29: [*model.parameters()]
325/30: [model.named_parameters()]
325/31: [*model.named_parameters()]
325/32: [*model.named_parameters()][0]
325/33: [*model.named_parameters()][10]
325/34: [*model.named_parameters()][10][1]
325/35: [*model.named_parameters()][10][1].grad()
325/36: [*model.named_parameters()][10][1]
325/37: *model.named_parameters()[10][1]
325/38: type([*model.named_parameters()][10][1])
325/39: [*model.named_parameters()][10][1].grad
325/40: print([*model.named_parameters()][10][1].grad)
325/41: [*model.named_parameters()][10][1].grad.view()
325/42: import torchvision.models as models
325/43:
model = models.resnet50()
# Calculate dummy gradients
model(torch.randn(1, 3, 224, 224)).mean().backward()
grads = []
325/44: import torch
325/45:
model = models.resnet50()
# Calculate dummy gradients
model(torch.randn(1, 3, 224, 224)).mean().backward()
grads = []
325/46:
for param in model.parameters():
    grads.append(param.grad.view(-1))
325/47: grads
325/48:     grads = []
325/49: names=[]
325/50: model.named_parameters()
325/51:
for name, param in model.named_parameters():
    grads.append(param.grad.view())
    names.append(name)
325/52: t = torch.rand(4, 4)
325/53: b = t.view(2, 8)
325/54: b.shape
325/55:
grads, names = [], []
for name, param in model.named_parameters():
    grads.append(param.grad.view(-1))
    names.append(name)
325/56: names
325/57: grads[0]
325/58: grads[0].shape
325/59: grads[1].shape
325/60: grads[2].shape
325/61: grads[4].shape
325/62: grads[5].shape
325/63: grads[6].shape
325/64:
grads, names = [], []
for name, param in model.named_parameters():
    grads.append(param.grad)
    names.append(name)
325/65: grads
325/66: grads[0].shape
325/67: np
325/68: np.array([1,2,3,4,5])
325/69: import numpy as np
325/70: l = np.array([1,2,3,4,5])
325/71: l.sum()
325/72: l.mean()
325/73: l = np.array([1,2,3,4,5], [6,7,8,9,10])
325/74: z
325/75: z
325/76: l = np.array([1,2,3,4,5], [6,7,8,9,10])
325/77: l = np.array([[1,2,3,4,5], [6,7,8,9,10]])
325/78: l.sum()
325/79: l.mean()
325/80: audio_path = "30709__unclesigmund__breath.wav"
325/81: from speech.loader import log_specgram_from_file
325/82:
log_spec = log_specgram_from_file(audio_path
)
325/83: audio_path = "/Users/dustin/CS/consulting/firstlayerai/data/background_noise/breath/30709__unclesigmund__breath.wav"
325/84: log_spec = log_specgram_from_file(audio_path)
325/85: log_spec.shape
325/86: log_spec = log_specgram_from_file(audio_path, window_size=32, step_size=16)
325/87: log_spec.shape
325/88: audio_path = "/Users/dustin/CS/consulting/firstlayerai/data/background_noise/demand/DKITCHEN/ch01.wav"
325/89: log_spec = log_specgram_from_file(audio_path, window_size=32, step_size=16)
325/90: log_spec.shape
325/91: np.vstack([log_spec, log_spec])
325/92: np.vstack([log_spec, log_spec]).shape
325/93: x = range(5)
325/94: x.start
325/95: x.endd
325/96: x.end
325/97: doc(x)
325/98: __dir__(x)
325/99: x.__dir__()
325/100: x.start
325/101: x.count
325/102: x.index
325/103: x.stop
325/104: x.step
325/105: target_type
325/106: int(0)
325/107: float(0)
325/108:
 subset_size = 100
        outpath_str = "/home/dzubke/awni_speech/data/subsets/20200603/{name}_{subset_size}"
325/109:
 subset_size = 100
outpath_str = "/home/dzubke/awni_speech/data/subsets/20200603/{name}_{subset_size}"
325/110: outpath
325/111: outpath_str
325/112:
 subset_size = 100
outpath_str = "/home/dzubke/awni_speech/data/subsets/20200603/{name}_{subset_size}".format(subset_size=100)
325/113:
 subset_size = 100
outpath_str = "/home/dzubke/awni_speech/data/subsets/20200603/_{subset_size}".format(subset_size=100)
325/114: outpath_str
325/115: from datetime import date
325/116: data.today()
325/117: date.today()
325/118: str(date.today())
325/119: write_path_str = "/home/dzubke/awni_speech/data/subsets/20200603/{data_name}_{size}_{date}.json"
325/120: write_path_str.format(1,2,3)
325/121: write_path_str = "/home/dzubke/awni_speech/data/subsets/20200603/{name}_{size}_{date}.json"
325/122: write_path_str.format(1,2,3)
325/123: write_path_str.format(name=1,size=2,date=3)
325/124: import librosa
325/125: y, sr = librosa.load(librosa.util.example_audio_file())
325/126: y_third = librosa.effects.pitch_shift(y, sr, n_steps=4)
325/127: from matplotlib import pyplot as plt
325/128: plt.plot(y, format="-")
325/129: plt.plot(y)
325/130: plt.show()
325/131: ax, fig= plt.subplots(3,1)
325/132: ax.plot(y)
325/133: ax1.plot(y)
325/134: ax
325/135: ax1, ax2, ax3 = ax
325/136: fig, ax= plt.subplots(3,1)
325/137: ax1
325/138: ax1, ax2, ax3 = ax
325/139: a1.plot(y)
325/140: ax1.plot(y)
325/141: ax2.plot(y_third)
325/142: ax3.plot(y_third - y)
325/143: plt.show()
325/144: plt.show()
325/145: y
325/146: y.sum()
325/147: y_third.sum()
325/148: (y_third - y ).sum()
325/149: tmp_path = "/tmp/pitch_test.wav"
327/1: history ~1/1-100
327/2: import librosa
327/3:
y, sr = librosa.load(librosa.util.example_audio_file())
>
327/4: y, sr = librosa.load(librosa.util.example_audio_file())
327/5:     y_third = librosa.effects.pitch_shift(y, sr, n_steps=4)
327/6: import wave
327/7: tmp_path = "/tmp/pitch_test.wav"
327/8:
with wave.open(tmp_path, 'wb') as fid:
    fid.writeframes(y_third)
327/9:
with wave.open(tmp_path, 'wb') as fid:
    fid.setnchannels(1)
    fid.writeframes(y_third)
327/10:
with wave.open(tmp_path, 'wb') as fid:
    fid.setnchannels(1)
    fid.setsampwidth(2)
    fid.writeframes(y_third)
327/11:
with wave.open(tmp_path, 'wb') as fid:
    fid.setnchannels(1)
    fid.setsampwidth(2)
    fid.setframerate(1600)
    fid.writeframes(y_third)
327/12: tmp_path
327/13: tmp_org_path = '/tmp/pitch_test_org.wav'
327/14:
with wave.open(tmp_org_path, 'wb') as fid:
    fid.setnchannels(1)
    fid.setsampwidth(2)
    fid.setframerate(1600)
    fid.writeframes(y)
327/15: dustin_path = "/Users/dustin/CS/consulting/firstlayerai/data/dustin_test_data/20191202_clean/5-drz-test-20191202.wv"
327/16: dustin_audio, sr = librosa.load(dustin_path)
327/17: wave
327/18:
with wave.open(dustin_path, 'rb') as fid:
    print(fid.getnchannels())
327/19:
with wave.open(dustin_path, 'rb') as fid:
    print(fid.getnchannels())
    print(fid.getsampwidth())
    print(fid.getframerate())
327/20: import random
327/21: random.uniform(0,5)
327/22: random.randint(0,5)
327/23: random.randint(0,5)
327/24: random.randint(0,5)
327/25: random.randint(0,5)
327/26:     audio_path = "/Users/dustin/CS/consulting/firstlayerai/phoneme_classification/src/awni_speech/speech/tests/pytest/test_audio/Speak-out.wav"
327/27: audio_data, samp_rate = array_from_wave(audio_path)
327/28: from speech.utils.wave import array_from_wave
327/29: audio_data, samp_rate = array_from_wave(audio_path)
327/30: audio_data.shape
327/31: max(audio_data)
327/32: min(audio_data)
327/33: all([1,1,1,1])
327/34: y, sr = librosa.load(audio_path)
327/35: augment_data = librosa.effects.pitch_shift(y, sr, n_steps=0, bins_per_octave=24)
327/36: all(augment_data == y)
327/37: augment_data.sum()
327/38: y.sum()
327/39: y.sum()*32768
327/40: audio_data
327/41: audio_data.sum()
327/42: 3.0517578125e-5f
327/43: 3.0517578125e-5
327/44: 1/32678
327/45: 1/32768
327/46: ad_float = audio_data/32768
327/47: ad_float.sum()
327/48: y.sum()
327/49: y.max()
327/50: y.min()
327/51: ad_float.max()
327/52: ad_float.min()
327/53: y.mean()
327/54: ad_float.mean()
327/55: y.std()
327/56: ad_float.std()
327/57: random.randint(5, 5)
327/58: random.randint(5, 5)
327/59: random.randint(5, 5)
327/60: random.randint(5, 5)
327/61: random.randint(5, 5)
327/62: random.randint(5, 5)
327/63: np.iinfo('int16')
327/64: import numpy as np
327/65: np.iinfo('int16')
327/66: np.iinfo('float32')
327/67: np.iinfo('float64')
327/68: np.iinfo('float')
327/69: np.iinfo('int16')
327/70: np.iinfo(np.float64)
327/71: audio_data.dtype
327/72: audio_data.astype('float64').dtype
327/73: np.iinfo(audio_data.astype('float64').dtype)
327/74: audio_data.dtype.kind
327/75: i = np.iinfo(audio_data.dtype)
327/76: i
327/77: i.bits
327/78: abs_max = 2 ** (i.bits - 1)
327/79: abs_max
327/80: offset = i.min + abs_max
327/81: i.min
327/82: offset
327/83: audio_data.astype('float64').mean()
327/84: audio_data.astype('float64').sum()
327/85: audio_data.astype('float64')/32768.sum()
327/86: np.sum(audio_data.astype('float64')/32768)
327/87: np.mean(audio_data.astype('float64')/32768)
327/88: audio_data.astype('float64').dtype
327/89: audio_data.astype('float64').dtype.kind
327/90: audio_data.shape
327/91: np.empty().shape
327/92: np.empty(1,).shape
327/93: np.array([])
327/94: np.array([]).shape
327/95: np.empty(,).shape
327/96: np.empty(0,).shape
327/97: np.empty(2,).sum()
327/98: pith = {"init": True, "curr": True}
327/99: pitch.init
327/100: from collections import namedtuples
327/101: import collections
327/102: collections.namedtuples
327/103: collections.NamedTuples
327/104: collections.namedtuple
327/105: from collections import namedtuple
327/106: namedtuple("test",["dustin", "best"])
327/107: test.dustin = 5
327/108: t = test(dustin=1, best=5)
327/109: Test = namedtuple("Test",["dustin", "best"])
327/110: t = Test(dustin=1, best=5)
327/111: t.dustin
327/112: Status = namedtuple("Status", ["initial", "current"])
327/113: spec_augment = Status(initial=True, current=True)
327/114: spec_augment.initial
327/115: type(spec_augment)
327/116: type(spec_augment) == Status
327/117: spec_augment.initial = False
327/118: import torch
327/119: torch.normal(2, 3, size=(1, 4))
327/120: torch.normal(2, 3, size=(1, 4)).shape
327/121: torch.normal(mean=2, std=3, size=(1, 4)).shape
327/122: torch.normal(mean=2, std=3, size=(1, 4))
327/123: torch.normal(mean=2, std=3, size=(1, 4))
327/124: torch.normal(mean=2, std=3, size=(1, 4))
327/125: torch.normal(mean=2, std=3, size=(1, 4))
327/126: torch.normal(mean=2, std=3, size=(1, 4))
327/127: torch.normal(mean=1, std=1, size=(1, 4))
327/128: torch.normal(mean=1, std=2, size=(1, 4))
327/129: torch.normal(mean=1, std=3, size=(1, 4))
327/130: torch.normal(mean=1, std=10, size=(1, 4))
327/131: torch.normal(mean=1, std=10, size=(1, 4))
327/132: torch.normal(mean=1, std=10, size=(1, 4))
327/133: torch.normal(mean=1, std=10, size=(1, 4))
327/134: torch.normal(mean=1, std=100, size=(1, 4))
327/135: torch.normal(mean=1, std=100, size=(1, 4))
327/136: torch.normal(mean=1, std=100, size=(1, 4))
327/137: torch.normal(mean=1, std=100, size=(1, 4))
327/138: torch.normal(mean=1, std=100, size=(1, 4))
327/139: torch.normal(mean=1, std=1, size=(1, 4))
327/140: torch.normal(mean=1, std=1, size=(1, 4))
327/141: torch.normal(mean=1, std=1, size=(1, 4))
327/142: torch.normal(mean=1, std=1, size=(1, 4))
327/143: torch.normal(mean=1, std=1, size=(1, 4))
327/144: torch.normal(mean=1, std=1, size=(1, 4))
327/145: torch.normal(mean=1, std=1, size=(1, 4))
327/146: torch.normal(mean=1, std=1, size=(1, 4))
327/147: torch.normal(mean=1, std=1, size=(1, 4))
327/148: torch.normal(mean=1, std=1, size=(1, 4))
327/149: torch.normal(mean=1, std=1, size=(1, 4))
327/150: torch.normal(mean=1, std=1, size=(1, 4))
327/151: torch.normal(mean=1, std=1, size=(1, 4))
327/152: torch.normal(mean=1, std=1, size=(1, 4))
327/153: torch.normal(mean=1, std=1, size=(1, 4))
327/154: torch.normal(mean=1, std=.1, size=(1, 4))
327/155: torch.normal(mean=1, std=.1, size=(1, 4))
327/156: torch.normal(mean=1, std=.1, size=(1, 4))
327/157: torch.normal(mean=1, std=.1, size=(1, 4))
327/158: torch.normal(mean=1, std=.1, size=(1, 4))
327/159: torch.normal(mean=1, std=1, size=(1, 4))
327/160: torch.normal(mean=1, std=1, size=(1, 4)).sum()
327/161: torch.normal(mean=1, std=1, size=(1, 4)).sum()
327/162: torch.normal(mean=1, std=1, size=(1, 4)).sum()
327/163: torch.normal(mean=1, std=1, size=(1, 4)).sum()
327/164: torch.normal(mean=1, std=1, size=(1, 4)).sum()
327/165: torch.normal(mean=1, std=1, size=(1, 4)).sum()
327/166: torch.normal(mean=1, std=1, size=(1, 4)).sum()
327/167: torch.normal(mean=1, std=1, size=(1, 4)).sum()
327/168: torch.normal(mean=1, std=1, size=(1, 4)).sum()
327/169: torch.normal(mean=1, std=1, size=(1, 4)).sum()
327/170: torch.normal(mean=1, std=1, size=(1, 4)).sum()
327/171: torch.normal(mean=1, std=1, size=(1, 4))
327/172: torch.normal(mean=1, std=0.5, size=(1, 4))
327/173: torch.normal(mean=1, std=0.5, size=(1, 4))
327/174: torch.normal(mean=1, std=0.5, size=(1, 4))
327/175: torch.normal(mean=1, std=0.5, size=(1, 4))
327/176: torch.normal(mean=1, std=0.5, size=(1, 4))
327/177: torch.normal(mean=1, std=0.5, size=(1, 4))
327/178: torch.normal(mean=1, std=0.5, size=(1, 4))
327/179: torch.normal(mean=0, std=0.5, size=(1, 4))
327/180: torch.normal(mean=0, std=0.5, size=(1, 4))
327/181: torch.normal(mean=0, std=0.5, size=(1, 4))
327/182: torch.normal(mean=0, std=0.5, size=(1, 4))
327/183: torch.normal(mean=0, std=0.5, size=(1, 4))
327/184: torch.normal(mean=0, std=0.5, size=(1, 4))
327/185: torch.normal(mean=0, std=0.5, size=(1, 4))
327/186: torch.normal(mean=0, std=1, size=(1, 4))
327/187: torch.normal(mean=0, std=1, size=(1, 4))
327/188: torch.normal(mean=0, std=1, size=(1, 4))
327/189: torch.normal(mean=0, std=1, size=(1, 4))
327/190: torch.normal(mean=0, std=1, size=(1, 4))
327/191: torch.normal(mean=0, std=1, size=(1, 4))
327/192: torch.normal(mean=0, std=1, size=(1, 4))
327/193: torch.normal(mean=0, std=1, size=(1, 4))
327/194: audio_path
327/195: from speech.utils.wave import array_from_wave
327/196: audio_data
327/197: audio_data = array_from_wave(audio_path)
327/198: from speech.loader import log_specgram_from_data
327/199: import importlib
327/200: importlib.reload(speech.utils.wave)
328/1: audio_path = '/Users/dustin/CS/consulting/firstlayerai/phoneme_classification/src/awni_speech/speech/tests/pytest/test_audio/Speak-out.wav'
328/2: from speech.loader import log_specgram_from_datat
328/3: from speech.loader import log_specgram_from_data
328/4: from speech.utils.wave import array_from_wave
328/5: audio_data = array_from_wave(audio_path)
328/6: audio_data, sr = array_from_wave(audio_path)
328/7: log_spec = log_specgram_from_data(audio_data, sr)
328/8: log_spec.shape
328/9: norm_log_spec = log_spec - log_spec.mean()
328/10: norm_log_spec_1 = norm_log_spec/norm_log_spec.std()
328/11: norm_log_spec_2 = log_spec/log_spec.std()
328/12: norm_log_spec_1 == norm_log_spec_2
328/13: (norm_log_spec_1 == norm_log_spec_2).sum()
328/14: log_spec.sum()
328/15: norm_log_spec_2.sum()
328/16: norm_log_spec_1.sum()
328/17: norm_log_spec_1.mean()
328/18: norm_log_spec_2.mean()
328/19: norm_log_spec_2.std()
328/20: norm_log_spec_1.std()
328/21: norm_log_spec = norm_log_spec_1
328/22: norm_log_spec.sum()
328/23: norm_log_spec.mean()
328/24: norm_log_spec.std()
328/25: import torch
328/26: torch.normal(2, 3, size=(1, 4))
328/27: noise = torch.normal(2, 3, size=norm_log_spec.shape)
328/28: noise.shape
328/29: import numpy as np
328/30: noise = np.random.normal(loc=0, scale=1.0, size=norm_log_spec.shape)
328/31: noise.shape
328/32: noise.sum()
328/33: norm_log_spec.sum()
328/34: norm_log_spec.sum()
328/35: noise.mean()
328/36: noise.std()
328/37: noise_1 = noise
328/38: noise_05 = np.random.normal(loc=0, scale=0.5, size=norm_log_spec.shape)
328/39: noise_10 = noise_1
328/40: noise_05.sum()
328/41: norm_log_spec
328/42: noise_05
328/43: noise_05
328/44: noise05.sum()
328/45: noise_05.sum()
328/46: noise_05.std()
328/47: noise_05.mean()
328/48: norm_log_spec.max()
328/49: norm_log_spec.min()
328/50: noise_05.min()
328/51: noise_05.max()
328/52: noise_10.min()
328/53: noise_10.max()
328/54: noise_05 = np.random.normal(loc=0, scale=0.25, size=norm_log_spec.shape)
328/55: noise_05 = np.random.normal(loc=0, scale=0.05, size=norm_log_spec.shape)
328/56: noise_05 = np.random.normal(loc=0, scale=0.5, size=norm_log_spec.shape)
328/57: noise_025 = np.random.normal(loc=0, scale=0.25, size=norm_log_spec.shape)
328/58: noise_025.sum()
328/59: noise_025.mean()
328/60: noise_025.std()
328/61: noise_025.min()
328/62: noise_025.max()
328/63: norm_log_spec.sum()
328/64: noise_125 = np.random.normal(loc=1, scale=0.25, size=norm_log_spec.shape)
328/65: noie_125.sum()
328/66: noise_125.sum()
328/67: noise_125.max()
328/68: noise_125.min()
328/69: noise_101 = np.random.normal(loc=1, scale=0.1, size=norm_log_spec.shape)
328/70: noise_101.sum()
328/71: noise_101.mean()
328/72: orm_log_spec.shape
328/73: norm_log_spec.shape
328/74: noise_05.sum()
328/75: noise_10.sum()
328/76: noise_025.sum()
328/77: noise_101.sum()
328/78: noise_101.mean()
328/79: noise_101.std()
328/80: noise_101.max()
328/81: noise_101.min()
328/82: norm_log_spec.mean()
328/83: norm_log_spec.std()
328/84: norm_log_spec.sum()
328/85: norm_log_spec.max()
328/86: norm_log_spec.min()
328/87: ovrs_path = "/Users/dustin/CS/consulting/firstlayerai/phoneme_classification/src/awni_speech/speech/model_convert/audio_files/Validatio-audio_2020-05-21/Speak-OVrsxD1n9Wbh0Hh6thej8FIBIOE2-1574725033.wav"
328/88: ovrs_data = array_from_wave(ovrs_path)
328/89: ls_ovrs_data = log_specgram_from_data(ovrs_data)
328/90: ovrs_data, sr = array_from_wave(ovrs_path)
328/91: ls_ovrs_data = log_specgram_from_data(ovrs_data, sr)
328/92: ls_ovrs_data -= ls_ovrs_data.mean()
328/93: ls_ovrs_data /= ls_ovrs_data.std()
328/94: norm_ovrs = ls_ovrs_data
328/95: norm_ovrs.mean()
328/96: norm_ovrs.std()
328/97: norm_ovrs.sum()
328/98: norm_ovrs.max()
328/99: norm_ovrs.min()
328/100: __dir__()
328/101: __doc__()
328/102: whos
328/103: noise_10.mean()
328/104: noise_10.std()
328/105: noise_10.sum()
328/106: noise_10.max()
328/107: noise_10.min()
328/108: from scipy import stats
328/109: stats.describe(noise_05)
328/110:
def sum_stats(arr):
    print("mean: ", arr.mean())
    print("std: ", arr.std())
    print("sum: ", arr.sum())
    print("max: ", arr.max())
    print("min: ", arr.min())
328/111: sum_stats(noise_05)
328/112: sum_stats(noise_025)
328/113: whos
328/114: noise_01 = np.random.normal(loc=0, scale=0.1, size=norm_log_spec.shape)
328/115: sum_stats(noise_01)
328/116: sum_stats(noise_10)
328/117: sum_stats(noise_101)
328/118: sum_stats(noise_125)
328/119: noise_150 = np.random.normal(loc=1, scale=0.5, size=norm_log_spec.shape)
328/120: sum_stats(noise_150)
328/121: noise_150 = np.random.normal(loc=0, scale=0.2, size=norm_log_spec.shape)
328/122: noise_002 = np.random.normal(loc=0, scale=0.2, size=norm_log_spec.shape)
328/123: sum_stats(noise_002)
328/124: norm_log_spec.shape
328/125: noise_0 = np.random.normal(loc=0, scale=0, size=norm_log_spec.shape)
328/126: sum_stats(noise_0)
328/127: noise_1 = np.random.normal(loc=1, scale=0, size=norm_log_spec.shape)
328/128: sum_stats(noise_1)
328/129: sum_stats(norm_log_spec)
328/130: norm_log_spec == norm_log_spec * noise_1
328/131: (norm_log_spec == norm_log_spec * noise_1).sum()
328/132: norm_log_spec.size
328/133: (norm_log_spec == norm_log_spec + noise_0).sum()
328/134: import torch
328/135: from torch import autograd
328/136:
class MyFunc(autograd.Function):
    @staticmethod
    def forward(ctx, inp):
         return inp.clone()
     @staticmethod
     def backward(ctx, gO):
         # Error during the backward pass
         raise RuntimeError("Some error in backward")
         return gO.clone()
328/138:
class MyFunc(autograd.Function):
    @staticmethod
    def forward(ctx, inp):
         return inp.clone()
    @staticmethod
    def backward(ctx, gO):
         # Error during the backward pass
         raise RuntimeError("Some error in backward")
         return gO.clone()
328/139:
def run_fn(a):
    out = MyFunc.apply(a)
    return out.sum()
328/140: inp = torch.rand(10, 10, requires_grad=True)
328/141: out = run_fn(inp)
328/142: out.backward()
328/143:
with autograd.detect_anomaly():
    inp = torch.rand(10, 10, requires_grad=True)
    out = run_fn(inp)
    out.backward()
328/144: import librosa
328/145: audio_data
328/146: stretch_data = librosa.effects.time_stretch(audio_data, 2)
328/147:
def os_play(play_file:str):
    play_str = f"play {play_file}"
    os.system(play_str)
328/148: from speech.utils.convert import pcm2float
328/149: audio_data_f = pcm2float(audio_data)
328/150: stretch_data = librosa.effects.time_stretch(audio_data_f, 2)
328/151: os_play(stretch_data)
328/152: import os
328/153: os_play(stretch_data)
328/154: import timeit
328/155: timeit.timeit('librosa.effects.time_stretch(audio_data_f, 2)', number=10000)
328/156: librosa
328/157: timeit.timeit('librosa.effects.time_stretch(audio_data_f, 2)', number=10000)
328/158: timeit.timeit(librosa.effects.time_stretch(audio_data_f, 2), number=10000)
328/159: timeit.timeit(librosa.effects.time_stretch(), number=10000)
328/160: timeit.timeit(librosa.effects.time_stretch(audio_data_f, 2), number=10000)
328/161: timeit.timeit('librosa.effects.time_stretch(audio_data_f, 2)', number=10000)
328/162: timeit.timeit('librosa.effects.time_stretch(audio_data_f, 2)', number=10000)
328/163: timeit.timeit('time_stretch(audio_data_f, 2)', number=10000)
328/164: from librosa.effects import time_stretch
328/165: timeit.timeit('time_stretch(audio_data_f, 2)', number=10000)
328/166: timeit.timeit(time_stretch(audio_data_f, 2), number=10000)
328/167: timeit.timeit(time_stretch(), number=10000)
328/168: timeit.timeit(time_stretch(), )
328/169: timeit.timeit(time_stretch(audio_data_f, 2))
328/170: timeit.timeit('time_stretch(audio_data_f, 2)')
328/171: timeit.timeit(lambda: time_stretch(audio_data_f, 2))
329/1: from librosa.effects import time_stretch
329/2: audio_path = "/Users/dustin/CS/consulting/firstlayerai/phoneme_classification/src/awni_speech/speech/model_convert/audio_files/Validatio-audio_2020-05-21/Speak-out.wav"
329/3: from speech.utils.wave import array_from_wave
329/4: audio_data, sr = array_from_wave(audio_path)
329/5: from speech.utils.convert import pcm2float
329/6: audio_data_f = pcm2float(audio_data)
329/7: time_stretch(audio_data_f, 2)
329/8: audio_data_f.shape
329/9: time_stretch(audio_data_f, 2).shape
329/10: timeit.timeit(lambda: time_stretch(audio_data_f, 2), number=5)
329/11: import timeit
329/12: timeit.timeit(lambda: time_stretch(audio_data_f, 2), number=5)
329/13: timeit.timeit(lambda: time_stretch(audio_data_f, 2), number=10)
329/14: timeit.timeit(lambda: time_stretch(audio_data_f, 2), number=100)
329/15: timeit.timeit(lambda: time_stretch(audio_data_f, 2), number=1)
329/16: timeit.timeit(lambda: time_stretch(audio_data_f, 2), number=100)
329/17: from speech.utils.signal_augment import speed_vol_perturb
329/18: timeit.timeit(lambda: speed_vol_perturb(audio_data, tempo_range=(2,2), gain_range=(0,0)), number=100)
329/19: speed_vol_perturb(audio_data, tempo_range=(2,2), gain_range=(0,0))
329/20: speed_vol_perturb(audio_path, tempo_range=(2,2), gain_range=(0,0))
329/21: timeit.timeit(lambda: speed_vol_perturb(audio_path, tempo_range=(2,2), gain_range=(0,0)), number=100)
329/22: timeit.timeit(lambda: time_stretch(audio_data_f, 2), number=100)
329/23:
def read_time_stretch(audio_path):
    audio_data, sr = array_from_wave(audio_path)
    time_stretch(audio_data_f, 2)
329/24:
def read_time_stretch(audio_path):
    audio_data, sr = array_from_wave(audio_path)
    audio_data_f = time_stretch(audio_data_f, 2)
329/25:
def read_time_stretch(audio_path):
    audio_data, sr = array_from_wave(audio_path)
    audio_data_f = pcm2float(audio_data)
    return time_stretch(audio_data_f, 2)
329/26: read_time_stretch(audio_path)
329/27: timeit.timeit(lambda: read_time_stretch(audio_path), number=100)
329/28: timeit.timeit(lambda: speed_vol_perturb(audio_path, tempo_range=(2,2), gain_range=(0,0)), number=1000)
329/29: timeit.timeit(lambda: read_time_stretch(audio_path), number=1000)
329/30: from speech.utils.convert import float2pcm
329/31:
def read_time_stretch(audio_path):
    audio_data, sr = array_from_wave(audio_path)
    audio_data_f = pcm2float(audio_data)
    return float2pcm(time_stretch(audio_data_f, 2))
329/32: timeit.timeit(lambda: read_time_stretch(audio_path), number=1000)
329/33: timeit.timeit(lambda: read_time_stretch(audio_path), number=1000)
329/34: timeit.timeit(lambda: read_time_stretch(audio_path), number=1000)
329/35: timeit.timeit(lambda: read_time_stretch(audio_path), number=1000)
329/36: timeit.timeit(lambda: read_time_stretch(audio_path), number=1000)
329/37: timeit.timeit(lambda: read_time_stretch(audio_path), number=1000)
329/38: timeit.timeit(lambda: speed_vol_perturb(audio_path, tempo_range=(2,2), gain_range=(0,0)), number=1000)
329/39: timeit.timeit(lambda: speed_vol_perturb(audio_path, tempo_range=(2,2), gain_range=(0,0)), number=1000)
329/40: timeit.timeit(lambda: speed_vol_perturb(audio_path, tempo_range=(2,2), gain_range=(0,0)), number=1000)
329/41: timeit.timeit(lambda: speed_vol_perturb(audio_path, tempo_range=(2,2), gain_range=(-6,8)), number=1000)
329/42: timeit.timeit(lambda: array_from_wave(audio_path), number=1000)
329/43: timeit.timeit(lambda: speed_vol_perturb(audio_path, tempo_range=(2,2), gain_range=(-6,8)), number=1000)
329/44: timeit.timeit(lambda: speed_vol_perturb(audio_path, tempo_range=(2,2), gain_range=(-6,8)), number=1000)
329/45: audio_data
329/46: timeit.timeit(lambda: audio_data_f = pcm2float(audio_data), number=1000)
329/47: timeit.timeit(lambda: pcm2float(audio_data), number=1000)
329/48: audio_data_f = pcm2float(audio_data)
329/49: timeit.timeit(lambda: time_stretch(audio_data_f, 2), number=1000)
329/50: timeit.timeit(lambda: time_stretch(audio_data_f, 2), number=1000)
329/51: import numpy as np
329/52: np.random.normal(loc=0, scale=1, size=audio_data.shape)
329/53: std_norm = np.random.normal(loc=0, scale=1, size=audio_data.shape)
329/54: std_norm.mean()
329/55: std_norm.std()
329/56: std_norm = np.random.normal(loc=0, scale=0.5, size=audio_data.shape)
329/57: dist = np.random.normal(loc=0, scale=0.5, size=audio_data.shape)
329/58: dists.std()
329/59: dist.std()
329/60: std_norm = np.random.normal(loc=0, scale=1, size=audio_data.shape)
329/61: (std_norm* 0.5).std()
329/62: (std_norm).std()
329/63: (std_norm* 0.5).std()
329/64: from matplotlib import plyplot as plt
329/65: from matplotlib import pyplot as plt
329/66: plt.plot(std_norm)
329/67: plt.show()
329/68: (std_norm* 0.5).meaan()
329/69: (std_norm* 0.5).mean()
329/70: (std_norm).mean()
329/71: dist.mean()
329/72: std_norm = np.random.normal(loc=0, scale=1, size=100)
329/73: plt.plot(std_norm)
329/74: plt.show()
329/75: plt.plot(std_norm, kind='hist')
329/76: plt.hist(std_norm)
329/77: plt.show()
329/78: std_norm = np.random.normal(loc=0, scale=1, size=1000)
329/79: plt.hist(std_norm)
329/80: plt.show()
329/81: fig, (ax1, ax2)=plt.subplots(2)
329/82: ax1.plot(std_norm*0.5, kind='hist')
329/83: fig, (ax1, ax2)=plt.subplots(1,2)
329/84: ax1.hist(std_norm*0.5)
329/85: dist = np.random.normal(loc=0, scale=0.5, size=1000)
329/86: ax2.hist(dist)
329/87: plt.show()
329/88: std_norm = np.random.normal(loc=0, scale=1, size=audio_data.shape)
329/89: std_norm.shape
329/90: std_norm = np.random.normal(loc=0, scale=1, size=audio_data.size)
329/91: std_norm.shape
329/92: noise_power = std_norm.dot(std_norm) / std_norm.size
329/93: noise_power
329/94: std_norm = np.random.normal(loc=0, scale=0.5, size=audio_data.size)
329/95: noise_power = std_norm.dot(std_norm) / std_norm.size
329/96: noise_power
329/97: import math
329/98: math.sqrt(noise_power)
329/99: np.sqrt(noise_power)
329/100: import random
329/101: random.randint(-8, 8)
329/102: random.randint(*(-8, 8))
329/103: random.randint(*(8, 8))
329/104: math.sqrt(2763217.33017)
329/105: min((5,4))
329/106: import addioiopa
329/107: import audioop
329/108: audio_data
329/109: aop_rms = audioop.rms(audio_data)
329/110: aop_rms = audioop.rms(audio_data, 2)
329/111: aop_rms
329/112: audio_data_f.shape
329/113: (audio_data_f==audio_data).sum()
329/114: audio_data_f = pcm2float(audio_data)
329/115: (audio_data_f==audio_data).sum()
329/116: audio_data_f==audio_data
329/117: audio_power = audio_data_f.dot(audio_data_f) / audio_data_f.size
329/118: audio_power
329/119: audio_data_asf = audio_data.astype(np.float64)
329/120: audio_power = audio_data_asf.dot(audio_data_asf) / audio_data_asf.size
329/121: audio_power
329/122: np.sqrt(audio_power)
329/123: audio_data.dtype
329/124: np.random.normal(loc=0, scale=1, size=audio_data.size).dtype
329/125: from speech.utils.signal_augment import signal_noise_inject, signal_noise_inject_v2
329/126: import speech.utils.signal_augment
329/127: from speech.utils import signal_augment
329/128: audio_path
329/129: audio_data = array_from_wave(audio_path)
329/130: noise_aug_v1 = signal_augment.signal_noise_inject(audio_data, (10,10))
329/131: ls
329/132: import importlib
329/133: importlib.reload(signal_augment)
329/134: noise_aug_v1 = signal_augment.signal_noise_inject(audio_data, (10,10))
329/135: audio_data, sr= array_from_wave(audio_path)
329/136: noise_aug_v1 = signal_augment.signal_noise_inject(audio_data, (10,10))
329/137: noise_aug_v2 = signal_augment.signal_noise_inject_v2(audio_data, (10,10))
329/138: importlib.reload(signal_augment)
329/139: noise_aug_v1 = signal_augment.signal_noise_inject(audio_data, (10,10))
329/140: noise_aug_v2 = signal_augment.signal_noise_inject_v2(audio_data, (10,10))
329/141: auio_data.dtype
329/142: audio_data.dtype
329/143: audio_data.dtype.kind
329/144: audio_data.dtype.kind == 'i'
329/145: audio_data.dtype == 'int16'
329/146: importlib.reload(signal_augment)
329/147: noise_aug_v1 = signal_augment.signal_noise_inject(audio_data, (10,10))
329/148: noise_aug_v2 = signal_augment.signal_noise_inject_v2(audio_data, (10,10))
329/149: importlib.reload(signal_augment)
329/150: noise_aug_v1 = signal_augment.signal_noise_inject(audio_data, (10,10))
329/151: noise_aug_v2 = signal_augment.signal_noise_inject_v2(audio_data, (10,10))
329/152: noise_aug_v1== noise_aug_v2
329/153: (noise_aug_v1== noise_aug_v2).sum()
329/154: np.testing.assert_allclose(noise_aug_v1, noise_aug_v2, rtol=1e-03, atol=1e-05)
329/155: importlib.reload(signal_augment)
329/156: noise_aug_v1 = signal_augment.signal_noise_inject(audio_data, (10,10), use_seed=True)
329/157: noise_aug_v2 = signal_augment.signal_noise_inject_v2(audio_data, (10,10), use_seed=True)
329/158: np.testing.assert_allclose(noise_aug_v1, noise_aug_v2, rtol=1e-03, atol=1e-05)
329/159: importlib.reload(signal_augment)
329/160: noise_aug_v1 = signal_augment.signal_noise_inject(audio_data, (10,10), use_seed=True)
329/161: importlib.reload(signal_augment)
329/162: noise_aug_v1 = signal_augment.signal_noise_inject(audio_data, (10,10), use_seed=True)
329/163: noise_aug_v2 = signal_augment.signal_noise_inject_v2(audio_data, (10,10), use_seed=True)
329/164: importlib.reload(signal_augment)
329/165: noise_aug_v1 = signal_augment.signal_noise_inject(audio_data, (10,10), use_seed=True)
329/166: importlib.reload(signal_augment)
329/167: noise_aug_v1 = signal_augment.signal_noise_inject(audio_data, (10,10), use_seed=True)
329/168: noise_aug_v2 = signal_augment.signal_noise_inject_v2(audio_data, (10,10), use_seed=True)
329/169: importlib.reload(signal_augment)
329/170: noise_aug_v1 = signal_augment.signal_noise_inject(audio_data, (10,10), use_seed=True)
329/171: noise_aug_v2 = signal_augment.signal_noise_inject_v2(audio_data, (10,10), use_seed=True)
329/172: np.testing.assert_allclose(noise_aug_v1, noise_aug_v2, rtol=1e-03, atol=1e-05)
329/173: np.testing.assert_allclose(noise_aug_v1, noise_aug_v2, rtol=1e-02, atol=1e-03)
329/174: np.testing.assert_allclose(noise_aug_v1, noise_aug_v2, rtol=1e-02, atol=320)
329/175: np.testing.assert_allclose(noise_aug_v1, noise_aug_v2, rtol=1e-01, atol=320)
329/176: import timeit
329/177: timeit.timeit(lambda: signal_augment.signal_noise_inject(audio_data, (10,10), use_seed=True), number=1000)
329/178: importlib.reload(signal_augment)
329/179: importlib.reload(signal_augment)
329/180: timeit.timeit(lambda: signal_augment.signal_noise_inject(audio_data, (10,10)), number=1000)
329/181: timeit.timeit(lambda: signal_augment.signal_noise_inject_v2(audio_data, (10,10)), number=1000)
329/182: timeit.timeit(lambda: signal_augment.signal_noise_inject(audio_data, (10,10)), number=1000)
329/183: timeit.timeit(lambda: signal_augment.signal_noise_inject(audio_data, (10,10)), number=1000)
329/184: timeit.timeit(lambda: signal_augment.signal_noise_inject_v2(audio_data, (10,10)), number=1000)
329/185: timeit.timeit(lambda: signal_augment.signal_noise_inject_v2(audio_data, (10,10)), number=1000)
329/186: timeit.timeit(lambda: signal_augment.signal_noise_inject_v2(audio_data, (10,10)), number=1000)
329/187: timeit.timeit(lambda: signal_augment.signal_noise_inject(audio_data, (10,10)), number=1000)
329/188: timeit.timeit(lambda: signal_augment.signal_noise_inject(audio_data, (10,10)), number=1000)
329/189: timeit.timeit(lambda: signal_augment.signal_noise_inject(audio_data, (10,10)), number=1000)
329/190: timeit.timeit(lambda: signal_augment.signal_noise_inject_v2(audio_data, (10,10)), number=1000)
329/191: timeit.timeit(lambda: signal_augment.signal_noise_inject_v2(audio_data, (10,10)), number=1000)
329/192: timeit.timeit(lambda: signal_augment.signal_noise_inject_v2(audio_data, (10,10)), number=1000)
329/193:
def time_v1(audio_data, snr_level):
    time_1 = timeit.timeit(lambda: audio_data = audio_data.astype('float64'), number=1000)
329/194:
def time_v1(audio_data, snr_level):
    time_1 = timeit.timeit(lambda: audio_data.astype('float64'), number=1000)
    print("time_1", time_1)
    time_2 = timeit.timeit(lambda: np.random.normal(loc=0, scale=1, size=audio_data.size).astype('float64'), number=1000)
    print("time_2", time_2)
    time_3 = timeit.timeit(lambda: np.random.uniform(snr_level, snr_level))
    print("time_3", time_3)
    time_4 = timeit.timeit(lambda:audio_data.dot(audio_data) / audio_data.size, number=1000)
    print("time_4", time_4)
329/195: time_v1(audio_data, 10)
329/196:
def time_v1(audio_data, snr_level):
    time_1 = timeit.timeit(lambda: audio_data.astype('float64'), number=1000)
    print("time_1", time_1)
    time_2 = timeit.timeit(lambda: np.random.normal(loc=0, scale=1, size=audio_data.size), number=1000)
    print("time_2", time_2)
    time_3 = timeit.timeit(lambda: np.random.uniform(snr_level, snr_level))
    print("time_3", time_3)
    time_4 = timeit.timeit(lambda:audio_data.dot(audio_data) / audio_data.size, number=1000)
    print("time_4", time_4)
329/197: time_v1(audio_data, 10)
329/198: time_v1(audio_data, 10)
329/199: dirname = "/Users/dustin/CS/consulting/firstlayerai/phoneme_classification/src/awni_speech/speech/tests/pytest/test_audio"
329/200: import glob
329/201: audio_files = glob.glob(dirname)
329/202: audio_files
329/203: audio_files = glob.glob(dirname+"/*")
329/204: audio_files
329/205: importlib.reload(signal_augment)
329/206: audio_path
329/207: audio_data, sr = array_from_wave(audio_path)
329/208: augmented_data =
329/209: snr_level = 30
329/210: np.random_seed(0)
329/211: np.random.seed(0)
329/212: std_norm_1 = np.random.normal(loc=0, scale=1, size=10)
329/213: std_norm_2 = np.random.normal(loc=0, scale=1, size=10)
329/214: (std_norm_1==std_norm_2).sum()
329/215: std_norm_1
329/216: std_norm_2
329/217: np.random.seed(0); std_norm_1 = np.random.normal(loc=0, scale=1, size=10)
329/218: np.random.seed(0); std_norm_2 = np.random.normal(loc=0, scale=1, size=10)
329/219: std_norm_1
329/220: std_norm_2
329/221: (std_norm_1==std_norm_2).sum()
329/222: np.random.seed(0)std_norm_1 = np.random.normal(loc=0, scale=1, size=10)
329/223: np.random.seed(0) std_norm_1 = np.random.normal(loc=0, scale=1, size=10)
329/224: np.random.seed(0)
329/225: std_norm_1 = np.random.normal(loc=0, scale=1, size=10)
329/226: np.random.seed(0)
329/227: std_norm_2 = np.random.normal(loc=0, scale=1, size=10)
329/228: (std_norm_1==std_norm_2).sum()
329/229: std_norm_1
329/230: std_norm_2
329/231: aug_data_1 = signal_augment.synthetic_gaussian_noise_inject(audio_dataa, snr_range(30,30))
329/232: aug_data_1 = signal_augment.synthetic_gaussian_noise_inject(audio_data, snr_range(30,30))
329/233: aug_data_1 = signal_augment.synthetic_gaussian_noise_inject(audio_data, snr_range=(30,30))
329/234: np.random.seed(0)
329/235: aug_data_1 = signal_augment.synthetic_gaussian_noise_inject(audio_data, snr_range=(30,30))
329/236: np.random.seed(0)
329/237: aug_data_2 = signal_augment.synthetic_gaussian_noise_inject(audio_data, snr_range=(30,30))
329/238: aug_data_1
329/239: aug_data_2
329/240: (aug_data_2==aug_data_1).sum()
329/241: aug_data_path ="/Users/dustin/CS/consulting/firstlayerai/phoneme_classification/src/awni_speech/speech/tests/pytest/test_pickles/sythentic-gaussian-noise-inject_Speak-out_snr-30.pickle"
329/242: import pickle'
329/243: import pickle
329/244:
with open(aug_data_path, 'wb') as fid:
        pickle.dump(aug_data_1, fid)
329/245:
with open(aug_data_path, 'rb') as fid:
        pckle_data = pickle.load(aug_data_1, fid)
329/246:
with open(aug_data_path, 'rb') as fid:
        pckle_data = pickle.load(fid)
329/247: np.random.seed(0)
329/248: aug_data_2 = signal_augment.synthetic_gaussian_noise_inject(audio_data, snr_range=(30,30))
329/249: (aug_data_2==pckle_data).sum()
329/250: np.random.binomial(1, 0.9)
329/251: np.random.binomial(1, 0.9)
329/252: np.random.binomial(1, 0.9)
329/253: np.random.binomial(1, 0.9)
329/254: np.random.binomial(1, 0.9)
329/255: np.random.binomial(1, 0.9)
329/256: (aug_data_2==pckle_data).sum()
329/257: np.random.binomial(1, 0.9)
329/258: np.random.binomial(1, 0.9)
329/259: np.random.binomial(1, 0.9)
329/260: np.random.binomial(1, 0.9)
329/261: np.random.binomial(1, 0.9)
329/262: np.random.binomial(1, 0.9)
329/263: np.random.binomial(1, 0.9)
329/264: np.random.binomial(1, 0.9)
329/265: np.random.binomial(1, 0.9)
329/266: np.random.binomial(1, 0.9)
329/267: np.random.binomial(1, 0.9)
329/268: np.random.binomial(1, 0.9)
329/269: np.random.binomial(1, 0.9)
329/270: np.random.binomial(1, 0.9)
329/271: np.random.binomial(1, 0.9)
329/272: np.random.binomial(1, 0.9)
329/273: np.random.binomial(1, 0.9)
329/274: np.random.binomial(1, 0.9)
329/275: np.random.binomial(1, 0.9)
329/276: np.random.binomial(1, 0.9)
329/277: np.random.binomial(1, 0.9)
329/278: np.random.binomial(1, 0.9)
329/279: np.random.binomial(1, 0.9)
329/280: np.random.binomial(1, 0.9)
329/281: np.random.binomial(1, 0.9)
329/282: np.random.binomial(1, 0.1)
329/283: np.random.binomial(1, 0.1)
329/284: np.random.binomial(1, 0.1)
329/285: np.random.binomial(1, 0.1)
329/286: np.random.binomial(1, 0.1)
329/287: np.random.binomial(1, 0.1)
329/288: np.random.binomial(1, 0.1)
329/289: np.random.binomial(1, 0.1)
329/290: np.random.binomial(1, 0.1)
329/291: np.random.binomial(1, 0.1)
329/292: np.random.binomial(1, 0.1)
329/293: np.random.binomial(1, 0.1)
329/294: np.random.binomial(1, 0.1)
329/295: np.random.binomial(1, 0.1)
329/296: np.random.binomial(1, 0.1)
329/297: np.random.binomial(1, 0.1)
329/298: np.random.binomial(1, 0.1)
329/299: np.random.binomial(1, 0.1)
329/300: np.random.binomial(1, 0.1)
329/301: np.random.binomial(1, 0.1)
329/302: np.random.binomial(1, 0.1)
329/303: np.random.binomial(1, 0.1)
329/304: from loader import log_specgram_from_data
329/305: from speech.loader import log_specgram_from_data
329/306: from speech.loader import log_specgram_from_data
329/307: from speech.loader import log_specgram_from_data
329/308: preprocessor = "log_specgram"
329/309: x = eval(preprocessor+"_from_data)
329/310: x = eval(preprocessor+"_from_data")
329/311: x
329/312:
class Test():
    def __init__(self):
        self.attr = 1
329/313: test=Test()
329/314: test.atatr
329/315: test.attr
329/316:
def set_attr(self, add):
    self.attr += add
329/317: test.set_attry(4)
329/318: test.set_attr(4)
329/319: __dir__(test)
329/320: test.__dir__()
329/321: l = [5]
329/322: l[:100]
329/323: from datetime import datetime
329/324: datetime.now().strftime("%H:%M:%S")
329/325: datetime.now().strftime("%YY-%mm-%dd,%H:%M:%S")
329/326: datetime.now().strftime("%Y-%m-%d,%H:%M:%S")
329/327: datetime.now().strftime("%Y-%m-%d|%H:%M")
329/328: datetime.now().strftime("%Y-%m-%d|%H:%M:%S")
329/329: datetime.now().strftime("%Y-%m-%d_%H-%M-%S")
329/330: datetime.now().strftime("%Y-%m-%d|%Hhr")
329/331: model
329/332: curr_batch_path = "/Users/dustin/CS/consulting/firstlayerai/phoneme_classification/src/awni_speech/speech/tests/debug_files/2020-06-16_09-15-14/current-batch_2020-06-16_09-15-14.pickle"
329/333: import pickle
329/334: class Test()
329/335:
class Test():
    def __init__(self, value):
        self.value = value
    def
329/336:
def sub(num1, num2):
    return num1 - num2
class Test():
    def __init__(self, value):
        self.value = value
    def add(self, num_to_add):
        self.value += num_to_add
    def use_add(self, num_to_add):
        self.add(num_to_add)
    def use_sub(self, num_to_sub):
        self.value = sub(self.value, num_to_sub)
329/337: test = Test(5)
329/338: test.value
329/339: test.add(1)
329/340: test.value
329/341: test.use_add(2)
329/342: test.value
329/343: test.use_sub(2)
329/344: test.value
329/345: test.use_sub(5)
329/346: test.use_sub(5)
329/347: test.use_sub(5)
329/348: test.value
329/349: from scipy import signal
329/350:
>>> fs = 10e3
>>> N = 1e5
>>> amp = 2 * np.sqrt(2)
>>> noise_power = 0.01 * fs / 2
>>> time = np.arange(N) / float(fs)
>>> mod = 500*np.cos(2*np.pi*0.25*time)
>>> carrier = amp * np.sin(2*np.pi*3e3*time + mod)
>>> noise = np.random.normal(scale=np.sqrt(noise_power), size=time.shape)
>>> noise *= np.exp(-time/5)
>>> x = carrier + noise
329/351: >>> f, t, Sxx = signal.spectrogram(x, fs)
329/352: Sxx.dtype
329/353: t.shape
329/354: Sxx.shape
329/355: Sxx = Sxx.T
329/356: samples = []
329/357: samples.append(Sxx, Sxx, Sxx)
329/358: samples.append([Sxx, Sxx, Sxx])
329/359: len(samples)
329/360: samples = samples[0]
329/361: len(samples)
329/362: samples = np.vstack(samples)
329/363: samples.shape
329/364: Sxx.shape
329/365: mean = np.mean(samples, axis=0)
329/366: mean.shape
329/367: mean
329/368: import numpy as np
329/369: np.empty()
329/370: np.empty(1,)
329/371: np.empty(1,)
329/372: np.zeros(1,)
329/373: m = list()
329/374: m
329/375: np.zeros(1,1)
329/376: np.empty(1,1)
329/377: np.zeros((1,1), dtype=np.float32).std()
329/378: np.empty((1,1), dtype=np.float32).std()
329/379: np.random.randn(1,1)
329/380: np.random.randn(1,1).dtype
329/381: np.random.randn(1,1).astype('float')
329/382: np.random.randn(1,1).astype('float').dtype
329/383: np.random.randn(1,1).astype('float64').dtype
329/384: np.random.randn(1,1).astype(np.float64).dtype
329/385: np.random.randn(1,1).std()
329/386: np.random.randn(1,257).std()
329/387: np.random.randn(1,257).std()
329/388: np.random.randn(1,1).std()
329/389: rand = np.random.randn(100,257)
329/390: rand.mean()
329/391: rand.std()
329/392: rand = np.random.rand(100,257)
329/393: rand.mean()
329/394: rand.std()
329/395: rand -= rand.mean()
329/396: rand.mean()
329/397: rand = np.random.randn(100,257)
329/398: rand.std()
329/399: zeros = np.zeros((100, 257))
329/400: zeros.std()
329/401: zeros /= zeros.std()
329/402: zeros
329/403: zeros = np.zeros((100, 257))
329/404: zeros /= (zeros.std()+1e-7)
329/405: zeros
329/406: test_arr = np.array([[2,2,2],[2,2,2]])
329/407: test_arr.std()
329/408: test_arr /= test_arr.std()
329/409: test_arr
329/410: test_arr /= 2
329/411: test_arr /= 2.0
329/412: test_arr
329/413: test_arr = test_arr.astype('float')
329/414: test_arr
329/415: test_arr /= 2.0
329/416: test_arr
329/417: test_arr /= test_arr.std()
329/418: test_arr
329/419: test_arr = np.array([[2,2,2],[2,2,2]]).astype('float')
329/420: test_arr /= (test_arr.std()+1e-7)
329/421: tests_arr
329/422: test_arr
329/423: zeros
329/424: import platform
329/425: platform.system()
329/426: pwd
329/427: if true: print("hello")
329/428: if True: print("hello")
329/429: if True: if True: print("hello")
329/430: if True: if True: print("hello")
329/431: if True: if True: print("hello")
329/432: if True: print("hello")
329/433: if True: print("hello")
329/434:
if True:
    if True: print("hello")
329/435: f'{400.00:0f}'
329/436: f'{400.00:.3f}'
329/437: f'{400.00:.2f}'
329/438: f'{400.00:.1f}'
329/439: f'{400.00:.0f}'
329/440: f'{400.00:.0f}'
329/441: whos
329/442: preproc
329/443: np
329/444: np.array([1,2,3,4])
329/445: arr = np.array([1,2,3,4])
329/446: arr.mean().dtype
329/447: arr.dtype
329/448: arr.mean(dtype='float32')
329/449: arr.mean(dtype='float32').dtype
329/450: filename_str = "compute-mean-std{}_log-spec_{}_2020-06-22.pickle"
329/451: filename_str.format("-with-feature-normalize", "mean")
331/1: import subprocess
331/2: ps = subprocess.Popen(['ps', 'aux'], stdout=subprocess.PIPE).communicate()[0]
331/3: ps = subprocess.Popen(['cat', '/test'], stdout=subprocess.PIPE).communicate()[0]
331/4:
try:
    ps = subprocess.Popen(['cat', '/test'], stdout=subprocess.PIPE).communicate()[0]
except Exception as err:
    print(err)
331/5:
try:
    ps = subprocess.Popen(['cat', '/test'], stdout=subprocess.PIPE).communicate()[0]
except Exception as err:
    print(err)
    print('hello')
331/6: from tests.pytest.utils import get_all_test_audio
331/7: test_audio_list = get_all_test_audio()
331/8: test_audio_list
331/9: test_audio_list[0]
331/10: from speech.utils import signal_augment
331/11: aug_data, sr = signal_augment.tempo_gain_pitch_perturb(test_audio_list[0], 16000, (1,1), (0,0), (400,400))
331/12: from speech.utils.wave import array_from_wave
331/13: audio_data, sr = array_from_wave(test_audio_list[0])
331/14: audio_data.mean()
331/15: aug_data.mean()
331/16: audio_data.sum()
331/17: aug_data.sum()
331/18: import numpy as np
331/19: np.sum(abs(aug_data))
331/20: import sox
331/21:
# create trasnformer
tfm = sox.Transformer()
# trim the audio between 5 and 10.5 seconds.
tfm.trim(5, 10.5)
# apply compression
tfm.compand()
# apply a fade in and fade out
tfm.fade(fade_in_len=1.0, fade_out_len=0.5)
# create the output file.
sox_array = tfm.build_array(test_audio_list[0])
# see the applied effects
tfm.effects_log
331/22: import sox
331/23:
# create trasnformer
tfm = sox.Transformer()
# trim the audio between 5 and 10.5 seconds.
tfm.trim(5, 10.5)
# apply compression
tfm.compand()
# apply a fade in and fade out
tfm.fade(fade_in_len=1.0, fade_out_len=0.5)
# create the output file.
sox_array = tfm.build_array(test_audio_list[0])
# see the applied effects
tfm.effects_log
331/24: tfm
331/25: import importlib
331/26: importlib.reload(sox)
331/27:
# create trasnformer
tfm = sox.Transformer()
# trim the audio between 5 and 10.5 seconds.
tfm.trim(5, 10.5)
# apply compression
tfm.compand()
# apply a fade in and fade out
tfm.fade(fade_in_len=1.0, fade_out_len=0.5)
# create the output file.
sox_array = tfm.build_array(test_audio_list[0])
# see the applied effects
tfm.effects_log
331/28: tfm.__dir__()
331/29: importlib.reload(sox)
331/30:
# create trasnformer
tfm = sox.Transformer()
# trim the audio between 5 and 10.5 seconds.
tfm.trim(5, 10.5)
# apply compression
tfm.compand()
# apply a fade in and fade out
tfm.fade(fade_in_len=1.0, fade_out_len=0.5)
# create the output file.
sox_array = tfm.build_array(test_audio_list[0])
# see the applied effects
tfm.effects_log
332/1: import sox
332/2: tfm = sox.Transformer()
332/3: tfm.build_array
332/4: from tests.pytest.utils import get_all_test_audio
332/5: test_audio = get_all_test_audio()
332/6:
# create transformer
tfm = sox.Transformer()
# trim the audio between 5 and 10.5 seconds.
tfm.trim(5, 10.5)
# apply compression
tfm.compand()
# apply a fade in and fade out
tfm.fade(fade_in_len=1.0, fade_out_len=0.5)
# get the output in-memory as a numpy array
# by default the sample rate will be the same as the input file
array_out = tfm.build_array(input_filepath=test_audio[0])
# see the applied effects
tfm.effects_log
332/7: array_out
332/8: type(array_out)
332/9:
def py_sox_aug(audio_path):
    tempo = 1.15
    gain = 3
    pitch = 200
    tfm = sox.Transformer()
    tfm.pitch(pitch, quick=True)
    tfm.tempo(tempo, audio_type='s', quick=True)
    tfm.gain(gain_db=gain, normalize=False, limiter=False, balance=None)
    return tfm.build_array(input_filepath=audio_path)
332/10: pysox_aug = py_sox_aug(test_audio[0])
332/11:
def py_sox_aug(audio_path):
    tempo = 1.15
    gain = 3.0
    pitch = 200
    tfm = sox.Transformer()
    tfm.pitch(pitch, quick=True)
    tfm.tempo(tempo, audio_type='s', quick=True)
    tfm.gain(gain_db=gain, normalize=False, limiter=False, balance=None)
    return tfm.build_array(input_filepath=audio_path)
332/12: pysox_aug = py_sox_aug(test_audio[0])
332/13:
def py_sox_aug(audio_path):
    tempo = 1.1
    gain = 3.0
    pitch = 200
    tfm = sox.Transformer()
    tfm.pitch(pitch, quick=True)
    tfm.tempo(tempo, audio_type='s', quick=True)
    tfm.gain(gain_db=gain, normalize=False, limiter=False, balance=None)
    return tfm.build_array(input_filepath=audio_path)
332/14: pysox_aug = py_sox_aug(test_audio[0])
332/15:
def py_sox_aug(audio_path):
    tempo = 1.1
    gain = 3.0
    pitch = 200.0
    tfm = sox.Transformer()
    tfm.pitch(pitch, quick=True)
    tfm.tempo(tempo, audio_type='s', quick=True)
    tfm.gain(gain_db=gain, normalize=False, limiter=False, balance=None)
    return tfm.build_array(input_filepath=audio_path)
332/16: pysox_aug = py_sox_aug(test_audio[0])
332/17: test_audio[0]
332/18: tfm.build_array(test_audio[0])
332/19: tfm.build_array('')
332/20: pysox_aug = py_sox_aug(test_audio[0])
332/21:
def py_sox_aug(audio_path):
    tempo = 1.1
    gain = 3.0
    pitch = 200.0
    tfm = sox.Transformer()
    tfm.pitch(pitch, quick=True)
    tfm.tempo(tempo, audio_type='s', quick=True)
    tfm.gain(gain, normalize=False, limiter=False, balance=None)
    return tfm.build_array(input_filepath=audio_path)
332/22: pysox_aug = py_sox_aug(test_audio[0])
332/23:
def py_sox_aug(audio_path):
    tempo = 1.1
    gain = 3.0
    pitch = 200.0
    tfm = sox.Transformer()
    tfm.pitch(pitch, quick=True)
    tfm.tempo(tempo, audio_type='s', quick=True)
    tfm.gain(gain, normalize=False, limiter=False, balance=None)
    return tfm
332/24: pysox_aug = py_sox_aug(test_audio[0])
332/25:
def py_sox_aug(audio_path):
    tempo = 1.1
    gain = 3.0
    pitch = 4
    tfm = sox.Transformer()
    tfm.pitch(pitch, quick=True)
    tfm.tempo(tempo, audio_type='s', quick=True)
    tfm.gain(gain, normalize=False, limiter=False, balance=None)
    return tfm
332/26:
def py_sox_aug(audio_path):
    tempo = 1.1
    gain = 3.0
    pitch = 2.0
    tfm = sox.Transformer()
    tfm.pitch(pitch, quick=True)
    tfm.tempo(tempo, audio_type='s', quick=True)
    tfm.gain(gain, normalize=False, limiter=False, balance=None)
    return tfm.build_array(input_filepath=audio_path)
332/27: pysox_aug = py_sox_aug(test_audio[0])
332/28: pysox_aug
332/29: pysox_aug.mean()
332/30: pysox_aug.max()
332/31: pysox_aug.min()
332/32: tempo_gain_pitch_perturb
332/33: from speech.utils.signal_augment import tempo_gain_pitch_perturb
332/34: tempo_gain_pitch_perturb
332/35: import timeit.timeit
332/36: import timeit
332/37: timeit.timeit('tempo_gain_pitch_perturb(test_audio_list[0], 16000, (1.1,1.1), (3,3), (200,200))', number=100)
332/38: tempo_gain_pitch_perturb
332/39: timeit.timeit('tempo_gain_pitch_perturb(test_audio[0], 16000, (1.1,1.1), (3,3), (200,200))', number=100)
332/40: timeit.timeit('tempo_gain_pitch_perturb(test_audio[0], 16000, (1.1,1.1), (3,3), (200,200))', setup='from speech.utils.signal_augment import tempo_gain_pitch_perturb',  number=100)
332/41: test_audi
332/42: test_audio
332/43: test_audio[0]
332/44: timeit.timeit('tempo_gain_pitch_perturb("/Users/dustin/CS/consulting/firstlayerai/phoneme_classification/src/awni_speech/speech/tests/pytest/test_audio/Speak-out.wav", 16000, (1.1,1.1), (3,3), (200,200))', setup='from speech.utils.signal_augment import tempo_gain_pitch_perturb',  number=100)
332/45: py_sox_aug
332/46: timeit.timeit('py_sox_aug("/Users/dustin/CS/consulting/firstlayerai/phoneme_classification/src/awni_speech/speech/tests/pytest/test_audio/Speak-out.wav")', number=100)
332/47: timeit.timeit('py_sox_aug("/Users/dustin/CS/consulting/firstlayerai/phoneme_classification/src/awni_speech/speech/tests/pytest/test_audio/Speak-out.wav")', setup='from __main__ import py_sox_aug', number=100)
332/48: timeit.timeit('tempo_gain_pitch_perturb("/Users/dustin/CS/consulting/firstlayerai/phoneme_classification/src/awni_speech/speech/tests/pytest/test_audio/Speak-out.wav", 16000, (1.1,1.1), (3,3), (200,200))', setup='from speech.utils.signal_augment import tempo_gain_pitch_perturb',  number=100)
332/49: timeit.timeit('py_sox_aug("/Users/dustin/CS/consulting/firstlayerai/phoneme_classification/src/awni_speech/speech/tests/pytest/test_audio/Speak-out.wav")', setup='from __main__ import py_sox_aug', number=100)
332/50: timeit.timeit('py_sox_aug("/Users/dustin/CS/consulting/firstlayerai/phoneme_classification/src/awni_speech/speech/tests/pytest/test_audio/Speak-out.wav")', setup='from __main__ import py_sox_aug', number=1000)
332/51: timeit.timeit('tempo_gain_pitch_perturb("/Users/dustin/CS/consulting/firstlayerai/phoneme_classification/src/awni_speech/speech/tests/pytest/test_audio/Speak-out.wav", 16000, (1.1,1.1), (3,3), (200,200))', setup='from speech.utils.signal_augment import tempo_gain_pitch_perturb',  number=1000)
332/52:
def py_sox_aug(audio_path):
    tempo = 1.1
    gain = 3.0
    pitch = 2.0
    tfm = sox.Transformer()
    tfm.pitch(pitch, quick=False)
    tfm.tempo(tempo, audio_type='s', quick=False)
    tfm.gain(gain, normalize=False, limiter=False, balance=None)
    return tfm.build_array(input_filepath=audio_path)
332/53: timeit.timeit('py_sox_aug("/Users/dustin/CS/consulting/firstlayerai/phoneme_classification/src/awni_speech/speech/tests/pytest/test_audio/Speak-out.wav")', setup='from __main__ import py_sox_aug', number=1000)
332/54: from speech.utils.io import load
332/55: model, preproc = load('examples/librispeech/models/ctc_models/20200625/ph2')
333/1: from speech.utils.io import load
333/2: model, preproc = load('examples/librispeech/models/ctc_models/20200625/ph2')
333/3: vars(preproc)
333/4: m = str()
333/5: preproc
333/6: print(preproc)
333/7: model, preproc = load('examples/librispeech/models/ctc_models/20200625/ph2')
333/8: print(preproc)
333/9: import importlib
333/10: import speech.loader
333/11: model, preproc = load('examples/librispeech/models/ctc_models/20200625/ph2')
333/12: print(preproc)
333/13: importlib.reload(loader)
334/1: from speech.utils.io import load
334/2: model, preproc = load('examples/librispeech/models/ctc_models/20200625/ph2')
334/3: print(preproc)
334/4: type(vars(preproc))
335/1: from speech.utils.io import load
335/2: model, preproc = load('examples/librispeech/models/ctc_models/20200625/ph2')
335/3: print(preproc)
335/4: type(model.parameters())
335/5: type(list(model.parameters()))
335/6: list(model.parameters())
335/7: type(list(model.parameters())[0])
335/8: type(list(model.parameters())[0].data)
335/9: import numpy as np
335/10: zeros = np.arange(25700).reshape(257, 100)
335/11: arr = np.arange(25700).reshape(257, 100)
335/12: arr.shape
335/13: arr[0][0]
335/14: arr[15:75][:]=0
335/15: np.sum(arr[15])
335/16: len(arr)
335/17: arr[:][0].shape
335/18: np.sum(arr[:][0])
335/19: arr.shape
335/20: arr.shape[0].shape
335/21: arr.shape[0]
335/22: arr
335/23: arr[0].shape
335/24: arr.shape
335/25: arr.shape[0]
335/26: arr.shape[1]
335/27: arr[:][0].shape
335/28: arr[0][:].shape
335/29: arr[:,0].shape
335/30: np.zeros((10,10)).std()
335/31: arr_nan = arr
335/32: arr_nan [0,0] = np.nan
335/33: arr_nan = arr_nan.astype('float')
335/34: arr_nan [0,0] = np.nan
335/35: arr_nan[0,0]
335/36: arr_nan.mean()
335/37: import math
335/38: math.isclose(0, 1e-6, rel_tol=1e-5)
335/39: math.isclose(0, 1e-8, rel_tol=1e-5)
335/40: math.isclose(0, 0, rel_tol=1e-5)
335/41: math.isclose(0, 1e-10, rel_tol=1e-5)
335/42: math.isclose(1e-8, 1e-10, rel_tol=1e-5)
335/43: math.isclose(0, 1e-10, abs_tol=1e-5)
335/44: np.isnan([5, 5, 5, 5])
335/45: any(np.isnan([5, 5, 5, 5]))
335/46: any(np.isnan([5, 5, np.nan, 5]))
335/47: l = [1,2,3,4,5]
335/48: l[1:None]
336/1: import pickle
336/2:
with open("landmarks-5.npy") as landmark_file:
    landmarks = pickle.load(landmark_file)
336/3:
with open("landmarks-5.npy", 'rb') as landmark_file:
    landmarks = pickle.load(landmark_file)
336/4: import numpy as np
336/5: landmarks = np.load("landmarks-5.npy")
336/6: landmarks
336/7: landmarks.shape
336/8: landmarks[0].shape
336/9:
import maskUtils
import numpy as np
import utils
import trimesh
from itertools import chain
336/10: from datetime import date
336/11: print(date.today())
336/12: str(date.today())
336/13: training_data_path = "/Users/dustin/CS/consulting/voodoo/mask/repo/data/training_data/training-data_5-examples_2020-06-28.pickle"
336/14: from utils import read_pickle
336/15: import utils
336/16: training_data = utils.read_pickle(training_data_path)
336/17: ls
336/18: import importlib
336/19: importlib.reload(utils)
336/20: training_data = utils.read_pickle(training_data_path)
336/21: type(training_data)
336/22: training_data
336/23: training_data.get("face_landmarks_train") == training_data.get("face_landmarks_train")
336/24: all(training_data.get("face_landmarks_train") == training_data.get("face_landmarks_train"))
336/25:
for i in range(len(training_data.get("face_landmarks_train")):
 all(training_data.get("face_landmarks_train")[i]==training_data.get("face_landmarks_train")[i]))
336/26:
for i in range(len(training_data.get("face_landmarks_train"))):
 all(training_data.get("face_landmarks_train")[i]==training_data.get("face_landmarks_train")[i])
336/27:
for i in range(len(training_data.get("face_landmarks_train"))):
 assert all(training_data.get("face_landmarks_train")[i]==training_data.get("face_landmarks_train")[i])
336/28:
for i in range(len(training_data.get("face_landmarks_train"))):
 assert all(training_data.get("mask_landmarks_train")[i]==training_data.get("face_landmarks_train")[i])
336/29:
for i in range(len(training_data.get("face_landmarks_train"))):
 assert all(training_data.get("mask_landmarks_train")[i]==training_data.get("face_landmarks_train")[i])
336/30: all(training_data.get("mask_landmarks_train")[0]== training_data.get("mask_landmarks_train")[1])
336/31: all(training_data.get("mask_landmarks_train")[0]== training_data.get("mask_landmarks_train")[0])
336/32:
for i in range(len(training_data.get("face_landmarks_train"))):
    assert all(training_data.get("mask_landmarks_train")[i]==training_data.get("face_landmarks_train")[i])
336/33:
for i in range(len(training_data.get("face_landmarks_train"))):
    np.all(training_data.get("mask_landmarks_train")[i]==training_data.get("face_landmarks_train")[i])
336/34:
for i in range(len(training_data.get("face_landmarks_train"))):
    print(training_data.get("mask_landmarks_train")[i]==training_data.get("face_landmarks_train")[i])
336/35:
for i in range(len(training_data.get("face_landmarks_train"))):
    assert training_data.get("mask_landmarks_train")[i]==training_data.get("face_landmarks_train")[i]
336/36:
for i in range(len(training_data.get("face_landmarks_train"))):
    assert training_data.get("face_landmarks_train")[i]==training_data.get("face_landmarks_train")[i]
336/37:
for i in range(len(training_data.get("face_landmarks_train"))):
    print(training_data.get("face_landmarks_train")[i]==training_data.get("face_landmarks_train")[i])
336/38:
for i in range(len(training_data.get("face_landmarks_train"))):
    print(all(training_data.get("face_landmarks_train")[i]==training_data.get("face_landmarks_train")[i]))
336/39:
for i in range(len(training_data.get("face_landmarks_train"))):
    assert all(training_data.get("face_landmarks_train")[i]==training_data.get("face_landmarks_train")[i])
336/40:
for i in range(len(training_data.get("face_landmarks_train"))):
    assert all(training_data.get("face_landmarks_train")[i]==training_data.get("mask_landmarks_train")[i])
336/41:
for i in range(len(training_data.get("face_landmarks_train"))):
    assert (training_data.get("face_landmarks_train")[i]==training_data.get("mask_landmarks_train")[i]).all()
336/42:
for i in range(len(training_data.get("face_landmarks_train"))):
    print(type((training_data.get("face_landmarks_train")[i]==training_data.get("mask_landmarks_train")[i])))
336/43:
for i in range(len(training_data.get("face_landmarks_train"))):
    print(type((training_data.get("mask_landmarks_train")[i]==training_data.get("mask_landmarks_train")[i])))
336/44:
for i in range(len(training_data.get("face_landmarks_train"))):
    np.array_equal(training_data.get("mask_landmarks_train")[i], training_data.get("mask_landmarks_train")[i])
336/45:
for i in range(len(training_data.get("face_landmarks_train"))):
    assert np.array_equal(training_data.get("mask_landmarks_train")[i], training_data.get("mask_landmarks_train")[i])
336/46:
for i in range(len(training_data.get("face_landmarks_train"))):
    assert np.array_equal(training_data.get("face_landmarks_train")[i], training_data.get("mask_landmarks_train")[i])
336/47: x = 5
336/48: x-=3
336/49: x
337/1: link = https://downloads.tatoeba.org/exports/sentences_with_audio.tar.bz2
337/2: link = "https://downloads.tatoeba.org/exports/sentences_with_audio.tar.bz2"
337/3: import bz2
337/4: import tarfile
337/5:
with tarfile.open(link) as tf:
    tf.extractall(path="/tmp/tatoeba.csv")
337/6: import urlliib
337/7: import urllib
337/8: urllib.request.urlretrieve(link, filename="/tmp/tatoeba.tar.bz2")
337/9:
with tarfile.open("/tmp/tatoeba.tar.bz2") as tf:
    tf.extractall(path="/tmp/tatoeba.csv")
337/10:
with tarfile.open("/tmp/tatoeba.tar.bz2") as tf:
    tf.extractall(path="~/tatoeba.csv")
338/1: import math
338/2: math.sqrt(0)
338/3: math.sqrt(-1)
338/4: 1/0
338/5: xTotal, yTotal, zTotal = 0
339/1: f = float()
339/2: f
338/6: data_path = "/Users/dustin/CS/consulting/voodoo/mask/src/data/training_data/2020-06-28/training-data_5-examples_2020-06-28.pickle"
338/7: import pickle
338/8:
    with open(data_path, 'rb') as data_id:
        data = pickle.load(data_id)
338/9: data.keys()
338/10: data.get('face_landmarks_train').shape
338/11: from maskUtils import dump_array_to_xyz
338/12: ls
338/13: import maskUtils
338/14: import mask_utils
338/15: ls
338/16: cd src/
338/17: from maskUtils import dump_array_to_xyz
338/18: from mask_utils import dump_array_to_xyz
338/19: dump_array_to_xyz("./test.xyz", data.get('face_landmarks_train')[0])
338/20: cd utils/
338/21: from mask_utils import dump_array_to_xyz
338/22: from mask_utils import load_xyz_to_array
338/23: load_xyz_to_array
338/24: ziip
338/25: zip
338/26: ave_path_str = "{}_landmarks_{}.xyz"
338/27: save_path_str = "{}_landmarks_{}.xyz"
338/28: save_path_str.format("face", 4)
338/29: os
338/30: import os
338/31: save_path_str = os.path.join("./test/tests", save_path_str)
338/32: save_path_str.format("face", 4)
338/33: save_path_str = os.path.join("./test/tests", "{}_landmarks_{}.xyz")
338/34: save_path_str.format("face", 4)
338/35: save_path_str.format("face", 5)
338/36: data
338/37:
    face_landmarks = data.get('face_landmarks_train')
    mask_landmarks = data.get('mask_landmarks_train')
340/1:
# change this to your actual data path!
FAUST_DATA_PATH = '../../data/MPI-FAUST/'
CKPT_DATA_PATH = '../mesh_regressor.h5'

# all the results will be saved here
LOG_DIR = '../logs'
340/2: !pip install pyntcloud trimesh pillow
340/3:
import faust

train_scans, train_meshes = faust.get_faust_train(FAUST_DATA_PATH)
test_scans = faust.get_faust_test(FAUST_DATA_PATH)
340/4:
from mesh_regressor_model import MeshRegressorMLP
import torch

model = MeshRegressorMLP(n_features=N_BPS_POINTS)

model.load_state_dict(torch.load(CKPT_DATA_PATH, map_location='cpu'))

model.eval()
340/5:
from bps import bps

BPS_RADIUS = 1.7
N_BPS_POINTS = 1024
MESH_SCALER = 1000

xtr, xtr_mean, xtr_max = bps.normalize(train_scans, max_rescale=False, return_scalers=True)
ytr = bps.normalize(train_meshes, x_mean=xtr_mean, x_max=xtr_max, known_scalers=True, max_rescale=False)
xtr_bps = bps.encode(xtr, radius=BPS_RADIUS, n_bps_points=N_BPS_POINTS, bps_cell_type='dists')

xte, xte_mean, xte_max = bps.normalize(test_scans, max_rescale=False, return_scalers=True)
xte_bps = bps.encode(xte, radius=BPS_RADIUS, n_bps_points=N_BPS_POINTS, bps_cell_type='dists')
340/6:
from mesh_regressor_model import MeshRegressorMLP
import torch

model = MeshRegressorMLP(n_features=N_BPS_POINTS)

model.load_state_dict(torch.load(CKPT_DATA_PATH, map_location='cpu'))

model.eval()
340/7:
import os
import numpy as np
from chamfer_distance import chamfer_distance

if not os.path.exists(LOG_DIR):
    os.makedirs(LOG_DIR)

yte_preds = model(torch.Tensor(xte_bps)).detach().numpy() 
yte_preds /= MESH_SCALER

scan2mesh_losses = []
for fid in range(0, len(yte_preds)):
    yte_preds[fid] += xte_mean[fid] #bring back center of the mass after prediction
    scan2mesh_losses.append(MESH_SCALER*chamfer_distance(yte_preds[fid], xte[fid]+xte_mean[fid], direction='y_to_x'))

alignments_path = os.path.join(LOG_DIR, 'alignments.npy')
np.save(alignments_path, yte_preds)

print("FAUST test set scan-to-mesh distance (avg): %4.2f mms" % np.mean(scan2mesh_losses))
340/8:
from tqdm import tqdm

N_TEST_SCANS = 200

smpl_faces = np.loadtxt('../bps_demos/smpl_mesh_faces.txt')

for sid in tqdm(range(0, N_TEST_SCANS)):
    
    mesh_verts = yte_preds[sid]
    scan2mesh_loss = scan2mesh_losses[sid]
    mesh_scan = faust.get_faust_scan_by_id(FAUST_DATA_PATH, sid, 'test')
    
    faust.visualise_predictions(scan=mesh_scan, align_verts=mesh_verts, align_faces=smpl_faces, 
                                scan_id=sid, scan2mesh_loss=scan2mesh_loss, save_dir=LOG_DIR)
340/9:
import os
import numpy as np
from chamfer_distance import chamfer_distance

if not os.path.exists(LOG_DIR):
    os.makedirs(LOG_DIR)

yte_preds = model(torch.Tensor(xte_bps)).detach().numpy() 
yte_preds /= MESH_SCALER

scan2mesh_losses = []
for fid in range(0, len(yte_preds)):
    yte_preds[fid] += xte_mean[fid] #bring back center of the mass after prediction
    scan2mesh_losses.append(MESH_SCALER*chamfer_distance(yte_preds[fid], xte[fid]+xte_mean[fid], direction='y_to_x'))

alignments_path = os.path.join(LOG_DIR, 'alignments.npy')
np.save(alignments_path, yte_preds)

print("FAUST test set scan-to-mesh distance (avg): %4.2f mms" % np.mean(scan2mesh_losses))
340/10:
from tqdm import tqdm
import matplotlib.pyplot as plt

N_TEST_SCANS = 200

smpl_faces = np.loadtxt('../bps_demos/smpl_mesh_faces.txt')

for sid in tqdm(range(0, N_TEST_SCANS)):
    
    mesh_verts = yte_preds[sid]
    scan2mesh_loss = scan2mesh_losses[sid]
    mesh_scan = faust.get_faust_scan_by_id(FAUST_DATA_PATH, sid, 'test')
    
    faust.visualise_predictions(scan=mesh_scan, align_verts=mesh_verts, align_faces=smpl_faces, 
                                scan_id=sid, scan2mesh_loss=scan2mesh_loss, save_dir=LOG_DIR)
    
    plt.close()
341/1:
from tqdm import tqdm
import matplotlib.pyplot as plt

N_TEST_SCANS = 200

smpl_faces = np.loadtxt('../bps_demos/smpl_mesh_faces.txt')

for sid in tqdm(range(0, N_TEST_SCANS)):
    
    mesh_verts = yte_preds[sid]
    scan2mesh_loss = scan2mesh_losses[sid]
    mesh_scan = faust.get_faust_scan_by_id(FAUST_DATA_PATH, sid, 'test')
    
    faust.visualise_predictions(scan=mesh_scan, align_verts=mesh_verts, align_faces=smpl_faces, 
                                scan_id=sid, scan2mesh_loss=scan2mesh_loss, save_dir=LOG_DIR)
    
    plt.close('all')
341/2:
# change this to your actual data path!
FAUST_DATA_PATH = '../../data/MPI-FAUST/'
CKPT_DATA_PATH = '../mesh_regressor.h5'

# all the results will be saved here
LOG_DIR = '../logs'
341/3: !pip install pyntcloud trimesh pillow
341/4:
import faust

train_scans, train_meshes = faust.get_faust_train(FAUST_DATA_PATH)
test_scans = faust.get_faust_test(FAUST_DATA_PATH)
341/5:
from bps import bps

BPS_RADIUS = 1.7
N_BPS_POINTS = 1024
MESH_SCALER = 1000

xtr, xtr_mean, xtr_max = bps.normalize(train_scans, max_rescale=False, return_scalers=True)
ytr = bps.normalize(train_meshes, x_mean=xtr_mean, x_max=xtr_max, known_scalers=True, max_rescale=False)
xtr_bps = bps.encode(xtr, radius=BPS_RADIUS, n_bps_points=N_BPS_POINTS, bps_cell_type='dists')

xte, xte_mean, xte_max = bps.normalize(test_scans, max_rescale=False, return_scalers=True)
xte_bps = bps.encode(xte, radius=BPS_RADIUS, n_bps_points=N_BPS_POINTS, bps_cell_type='dists')
341/6:
from mesh_regressor_model import MeshRegressorMLP
import torch

model = MeshRegressorMLP(n_features=N_BPS_POINTS)

model.load_state_dict(torch.load(CKPT_DATA_PATH, map_location='cpu'))

model.eval()
341/7:
import os
import numpy as np
from chamfer_distance import chamfer_distance

if not os.path.exists(LOG_DIR):
    os.makedirs(LOG_DIR)

yte_preds = model(torch.Tensor(xte_bps)).detach().numpy() 
yte_preds /= MESH_SCALER

scan2mesh_losses = []
for fid in range(0, len(yte_preds)):
    yte_preds[fid] += xte_mean[fid] #bring back center of the mass after prediction
    scan2mesh_losses.append(MESH_SCALER*chamfer_distance(yte_preds[fid], xte[fid]+xte_mean[fid], direction='y_to_x'))

alignments_path = os.path.join(LOG_DIR, 'alignments.npy')
np.save(alignments_path, yte_preds)

print("FAUST test set scan-to-mesh distance (avg): %4.2f mms" % np.mean(scan2mesh_losses))
341/8:
from tqdm import tqdm
import matplotlib.pyplot as plt

N_TEST_SCANS = 200

smpl_faces = np.loadtxt('../bps_demos/smpl_mesh_faces.txt')

for sid in tqdm(range(0, N_TEST_SCANS)):
    
    mesh_verts = yte_preds[sid]
    scan2mesh_loss = scan2mesh_losses[sid]
    mesh_scan = faust.get_faust_scan_by_id(FAUST_DATA_PATH, sid, 'test')
    
    faust.visualise_predictions(scan=mesh_scan, align_verts=mesh_verts, align_faces=smpl_faces, 
                                scan_id=sid, scan2mesh_loss=scan2mesh_loss, save_dir=LOG_DIR)
    
    plt.close('all')
342/1:
import PIL

images_dir = os.path.join(LOG_DIR, 'faust_test_images')
aligns_dir =  os.path.join(LOG_DIR, 'faust_test_alignments')
print("Predicted alignments meshes saved in: %s" % images_dir)
print("Predicted alignments images saved in: %s" % aligns_dir)

fid = 0
merged_img_path =  os.path.join(images_dir, '%03d_scan_align_pair.png'%fid)
PIL.Image.open(merged_img_path)
343/1: np.array([1,2,3,4]).shape
343/2: import numpy as np
343/3: np.array([1,2,3,4]).shape
343/4: y = np.array([1,2,3,4])
343/5: y1 = y.reshape(4,1)
343/6: y1.shape
343/7: np.squeeze(y1).shape
343/8: y1=np.squeeze(y1)
343/9: np.sum(y==y1)
345/1: import numpy as np
345/2: arr = np.arange(120)
345/3: arr.reshape(12, 10)
345/4: arr = arr.reshape(12, 10)
345/5: arr.shape
345/6: len(arr)
345/7: import matplotlib.pyplot as plt
345/8: plt.plot(arr)
345/9: plt.plot(arr)
345/10: arr[0]
345/11: arr[0][0]
345/12: arr[0][1]
345/13: arr.shape
345/14: arr[0,1]
345/15: arr[0,2]
345/16: arr[0][2]
338/38: import torch
338/39: ls
338/40: ..
338/41: ls
338/42: cd ..
338/43: ls
338/44: cd ../bps/
338/45: weight_path = "./mesh_regressor.h5"
338/46: state_dict = torch.load(weight_path, map_location='cpu')
338/47: [keys in state_dict.keys()]
338/48: [keys for keys in state_dict.keys()]
338/49: [keys for keys in state_dict.keys()][-1]
338/50: [keys for keys in state_dict.keys()][-1].split('.')
338/51: ls
338/52: cd bps_demos/
338/53: from mesh_regressor_model import FaceMeshRegressorMLP
338/54: model = FaceMeshRegressorMLP()
338/55: model = FaceMeshRegressorMLP(100)
338/56: model = FaceMeshRegressorMLP(n_features=100)
338/57: import importlib
338/58: importlib.reload(FaceMeshRegressorMLP  )
338/59: import mesh_regressor_model
338/60: model = mesh_regressor_model.FaceMeshRegressorMLP(n_features=100)
338/61: importlib.reload(mesh_regressor_model)
338/62: model = mesh_regressor_model.FaceMeshRegressorMLP(n_features=100)
338/63: model.load_pretrained_weights("../mesh_regressor.h5")
338/64: model = mesh_regressor_model.FaceMeshRegressorMLP(n_features=1024)
338/65: model.load_pretrained_weights("../mesh_regressor.h5")
338/66:
for name, param in model.named_parameters():
    print(f"{name}, sum: {torch.sum(param)}")
338/67: importlib.reload(mesh_regressor_model)
338/68: model = mesh_regressor_model.FaceMeshRegressorMLP(n_features=1024)
338/69: model.load_pretrained_weights("../mesh_regressor.h5")
338/70: importlib.reload(mesh_regressor_model)
338/71: model = mesh_regressor_model.FaceMeshRegressorMLP(n_features=1024)
338/72: model.load_pretrained_weights("../mesh_regressor.h5")
346/1: from datetime import datetime
346/2: datetime.now()
346/3: str(datetime.now())
346/4: datetime.now().strftime(%YYYY-%m)
346/5: datetime.now().strftime(%Y-%m)
346/6: datetime.now().strftime('%Y-%m')
346/7: datetime.now().strftime('%Y-%m-%d')
346/8: datetime.now().strftime('%Y-%m-%d_%H')
346/9: datetime.now().strftime('%Y-%m-%d_%H-%M)
346/10: datetime.now().strftime('%Y-%m-%d_%H-%M')
346/11: datetime.now().strftime('%Y-%m-%d_%H-%M-%S')
346/12: datetime.now().strftime('%Y-%m-%d_%H-%M-%S')
346/13: datetime.now().strftime('%Y-%m-%d_%H-%M-%s')
346/14: datetime.now().strftime('%Y-%m-%d_%H-%M-%S')
346/15: import os
346/16: os.mkdir('~/CS/consulting/voodoo/mask/src/linear_reg')
346/17: os.mkdir('~/CS/consulting/voodoo/mask/src/linear_reg/test')
346/18: os.mkdir('~/CS/consulting/voodoo/mask/src/linear_reg/test')
346/19: pwd
346/20: os.mkdir('/Users/dustin/CS/consulting/voodoo/mask/src/linear_reg/')
346/21: os.makedirs('/Users/dustin/CS/consulting/voodoo/mask/src/linear_reg/', exists_ok=True)
346/22: os.makedirs('/Users/dustin/CS/consulting/voodoo/mask/src/linear_reg/', exist_ok=True)
346/23: os.makedirs('/Users/dustin/CS/consulting/voodoo/mask/src/linear_reg/test', exist_ok=True)
346/24: os.makedirs('/Users/dustin/CS/consulting/voodoo/mask/src/linear_reg/test', exist_ok=True)
346/25: os.makedirs('/Users/dustin/CS/consulting/voodoo/mask/src/linear_reg/test', exist_ok=True)
346/26: np
346/27: import numpy as n
346/28: import numpy as np
347/1: import numpy as np
347/2: arr = np.arange(9, dtype='float').reshape(3,3)
347/3:
for row in arr:
    print(row)
347/4: float()
347/5: float(0.0)
347/6: arr.reverse()
347/7:
for row1, row2 in zip(arr, arr):
    print(row1)
    print(row2)
347/8: arr = np.arange(9, dtype='float').reshape(0,3,3)
347/9: arr = np.arange(100, dtype='float').reshape(0,3,3)
347/10: arr = np.arange(10, dtype='float').reshape(0,3,3)
347/11: arr = np.arange(1, dtype='float').reshape(0,1,1)
347/12: arr = np.arange(27, dtype='float').reshape(3,3,3)
347/13: arr.shape
347/14: len(arr)
347/15: arr
347/16: arr1 = arr + 2
347/17: arr1
347/18: np.linalg.norm(arr[0] - arr1[0], ord=2)
347/19: arr[0]
347/20: np.linalg.norm(arr[0,0] - arr1[0,0], ord=2)
347/21: np.linalg.norm(np.zeros((3,)), ord=2)
347/22: np.linalg.norm(np.zeros((3,)), ord=2)
347/23: np.linalg.norm(np.zeros((3,)) - np.zeros((3,)), ord=2)
347/24: arr = np.arange(10)
347/25: arr
347/26: arr = np.arange(180).reshape(5,36)
347/27: arr.shape
347/28: arr1 = arr.reshape(5,12,3)
347/29: arr
347/30: arr1
347/31: arr1.shape
347/32: arr1.reshape(5, -1)
347/33: arr1.reshape(5, -1).shape
347/34: ls
347/35: cd ../bps/
347/36: cd bps_demos
347/37: from modelnet40 import load_modelnet40
347/38: modelnet40_dir = ""
347/39: sample_mesh_path ="/Users/dustin/CS/consulting/voodoo/mask/src/data/linear_reg/training_data/2020-06-28/raw_data/FLAME_sample_01.ply"
347/40: import trimesh
347/41: sampe_mesh = trimesh.load(sample_mesh_path, process=False)
347/42: sample_mesh = trimesh.load(sample_mesh_path, process=False)
347/43: del sampe_mesh
347/44: sampe_mesh
347/45: modelnet40_dir = "/Users/dustin/CS/consulting/voodoo/mask/src/data/modelnet40"
347/46: xtr, ytr, xte, yte = load_modelnet40(root_data_dir=modelnet40_dir)
347/47: from sys import getsizeof
347/48: getsizeof(xtr)
347/49: getsizeof(ytr)
347/50:
from collections import Mapping, Container
from sys import getsizeof
 
def deep_getsizeof(o, ids):
    """Find the memory footprint of a Python object
 
    This is a recursive function that drills down a Python object graph
    like a dictionary holding nested dictionaries with lists of lists
    and tuples and sets.
 
    The sys.getsizeof function does a shallow size of only. It counts each
    object inside a container as pointer only regardless of how big it
    really is.
 
    :param o: the object
    :param ids:
    :return:
    """
    d = deep_getsizeof
    if id(o) in ids:
        return 0
 
    r = getsizeof(o)
    ids.add(id(o))
 
    if isinstance(o, str) or isinstance(0, unicode):
        return r
 
    if isinstance(o, Mapping):
        return r + sum(d(k, ids) + d(v, ids) for k, v in o.iteritems())
 
    if isinstance(o, Container):
        return r + sum(d(x, ids) for x in o)
 
    return r
347/51: deep_getsizeof
347/52: deep_getsizeof(xtr)
347/53: id(xtr)
347/54: deep_getsizeof(xtr, 140703175156480)
347/55: deep_getsizeof(xtr, [140703175156480])
347/56: deep_getsizeof(xtr, [])
347/57: deep_getsizeof(xtr, {})
347/58: deep_getsizeof(xtr, set([]))
347/59: sample_mesh
347/60: len(xte)
347/61: type(xte)
347/62: len(xte[0])
347/63: xte[0]
347/64: xte.shape
347/65: sample_mesh.shape
347/66: sample_mesh.show()
348/1: %history ~1/1-100
348/2: sample_mesh_path ="/Users/dustin/CS/consulting/voodoo/mask/src/data/linear_reg/training_data/2020-06-28/raw_data/FLAME_sample_01.ply"
348/3: import trimesh
348/4: sample_mesh = trimesh.load(sample_mesh_path, process=False)
348/5:
modelnet40_dir = "/Users/dustin/CS/consulting/voodoo/mask/src/data/modelnet40"
xtr, ytr, xte, yte = load_modelnet40(root_data_dir=modelnet40_dir)
348/6: from modelnet40 import load_modelnet40
348/7: ls
348/8: cd ../bps/bps_demos
348/9: from modelnet40 import load_modelnet40
348/10:
modelnet40_dir = "/Users/dustin/CS/consulting/voodoo/mask/src/data/modelnet40"
xtr, ytr, xte, yte = load_modelnet40(root_data_dir=modelnet40_dir)
348/11: sample_mesh
348/12: sample_mesh.points
348/13: sample_mesh.__dir__()
348/14: sample_mesh.vertices
348/15: len(sample_mesh.vertices)
348/16: sample_mesh.vertices[0]
348/17: sample_mesh.vertices.__dir__()
348/18: verticies = sample_mesh.vertices.view(np.ndarray)
348/19: import numpy as np
348/20: verticies = sample_mesh.vertices.view(np.ndarray)
348/21: verticies.shape
348/22: xte[0].shape
348/23: 5 in [5,6]
348/24: 5 in [2,6]
348/25: type(sample_mesh)
348/26: from trimesh import Trimesh
348/27: Trimesh
348/28: import trimesh
348/29: trimesh
348/30: Trimesh
348/31: trimesh.Trimesh
348/32: np_mesh = np.array(sample_mesh)
348/33: np_mesh.shape
348/34: np_mesh
348/35: verticies = sample_mesh.vertices.view(np.ndarray)
348/36: verticies.shape
348/37: type(verticies)
348/38: ls
348/39: bps
348/40: ls
348/41: from bps import bps
348/42: xte_normalized = bps.normalize(xte)
348/43: xte_normalized.shape
348/44: xte.shape
348/45: xte_normalized_1 = bps.normalize(xte[0])
348/46: verticies.shape
348/47: np_verticies = np.expand_dims(verticies, 0)
348/48: np.verticies.shape
348/49: np_verticies.shape
348/50: vert_normalized = bps.normalize(np_verticies)
348/51: vert_normalized.shape
348/52: num_bps_points=512
348/53: bps_radius=1.7
348/54:
    bps_features = bps.encode(vert_normalized, 
                              n_bps_points=num_bps_points,
                              bps_cell_type='dists', 
                              radius=bps_radius)
348/55: bps_features.shape
348/56: np.squeeze(bps_features).shape
348/57: ls
348/58: cd ../../linear_reg/
348/59: import preprocess
348/60: import preprocess
348/61: face_lmks = preprocess.get_face_landmarks(sample_mesh)
348/62: len(face_lmks)
348/63: len(face_lmks[0])
348/64: type(face_lmks[0])
348/65: sample_mesh.vertices[3121]
348/66: from itertools import chain
348/67: np_lmks = np.array(list(chain.from_iterable(face_lmks)))
348/68: np_lmks.shape
348/69: bps
348/70: bps_train_path ="/Users/dustin/CS/consulting/voodoo/mask/src/data/linear_reg/training_data/2020-07-08/bps-train-data_200-examples_2020-07-12.pickle"
348/71: read_pikcle
348/72: read_pickle
348/73: from utils.save_load import read_pickle
348/74: from utils.save_load import load_pickle
348/75: bps_train = load_pickle(bps_train_path)
348/76: bps_train['input_features'].shape
348/77: bps_train['labels'].shape
348/78: job_obj_path = "/Users/dustin/CS/consulting/voodoo/mask/src/data/linear_reg/new_scans/jon.obj"
348/79: jon_mesh = trimesh.load(job_obj_path, process=False)
348/80: jon_mesh = trimesh.load_mesh(job_obj_path)
348/81: jon_mesh = trimesh.load_obj(job_obj_path)
348/82: jon_mesh = trimesh.exchange.load_obj(job_obj_path)
348/83: jon_mesh = trimesh.load(job_obj_path)
348/84: jon_mesh
350/1: preproc_path = "examples/librispeech/models/ctc_models/20200630/ph2/best_preproc.pyc"
350/2: from speech.utils.io import read_pickle
350/3: preproc = read_pickle(preproc_path)
350/4: preproc
350/5: print(preproc)
351/1: preproc_path = "examples/librispeech/models/ctc_models/20200630/ph2/best_preproc.pyc"
351/2: from speech.utils.io import read_pickle
351/3: preproc = read_pickle(preproc_path)
351/4: test_arr =
351/5: import numpy as np
351/6: test_arr = np.arange(25700, dtype='float32').reshape(100,2570)
351/7: test_arr.shape
351/8: test_arr = np.arange(25700, dtype='float32').reshape(100,257)
351/9: test_arr.shape
351/10: preproc
351/11:
print(preproc
)
351/12: test_arr.mean()
351/13: test_arr.std()
351/14: from loader import feature_normalize
351/15: from speech.loader import feature_normalize
351/16: norm_arr = feature_normalize(test_arr)
351/17: norm_arr.mean()
351/18: norm_arr.std()
351/19: norm_arr = preproc.normalize(test_arr)
351/20: norm_arr = preproc.normalize(test_arr, True)
351/21: norm_arr.mean()
351/22: norm_arr.std()
351/23: arr = np.array([[1,2,3],[4,5,6],[7,8,9]], dtype='float32')
351/24: preproc.mean.shape
351/25: div_arr = np.array([10,20,30])
351/26: arr / div_arr
351/27: arr - div_arr
351/28: arr
351/29: arr = np.array([[1,2,3],[4,5,6],[7,8,9], [10,11,12]], dtype='float32')
351/30: arr.shape
351/31: arr / div_arr
351/32: arr - div_arr
351/33: div_arr
351/34: arr / arr
351/35: arr / arr.T
351/36: div_arr_1 = np.array([10,20,30,40], dtype='float32')
351/37: arr / div_arr
351/38: arr / div_arr_1
351/39: arr.T / div_arr_1
351/40: arr.T
351/41: np.random.randn(10,10).astype('float64').dtype == np.float32
351/42: np.empty((0,0)).std()
351/43: np.empty((1,1)).std()
351/44: np.empty((1,1), dtype=np.float32).std()
351/45: np.empty((0,0), dtype=np.float32).std()
351/46: np.empty((0,0), dtype=np.float32).std() == np.empty((0,0), dtype=np.float32).std()
351/47: np.empty((1,1), dtype=np.float32).std() == np.empty((1,1), dtype=np.float32).std()
351/48: np.empty((1,1), dtype=np.float32).std() == np.empty((1,1), dtype=np.float32).std()
351/49: np.empty((1,1), dtype=np.float32).std()
351/50: np.empty((1,1), dtype=np.float32).std()
351/51: np.empty((1,1), dtype=np.float32).std()
351/52: np.empty((1,1), dtype='float32').std()
351/53: np.random.randn(1,1).std()
351/54: np.empty((1,1), dtype='float32').std()
351/55: np.empty((1,1), dtype=np.float32).std()
351/56: whos
351/57: test_arr.mean()
351/58: test_arr.shape
351/59: test_arr.mean(axis=0).shape
351/60: test_arr.mean(axis=0).mean()
351/61: test_arr.std(axis=0).std()
351/62: test_arr.std(axis=0).shape
351/63: test_arr.std(axis=0)
351/64: test_arr.mean(axis=0)
351/65: test_arr.sum()
351/66: norm_arr.mean()
351/67: test_arr.mean()
351/68: test_arr = np.arange(25700, dtype='float32').reshape(100,257)
351/69: test_arr.mean()
351/70: norm_arr = preproc.normalize(test_arr, True)
351/71: norm_arr.mean()
351/72: norm_arr.std()
351/73: test_arr.mean()
351/74: test_arr = np.arange(25700, dtype='float32').reshape(100,257)
351/75: norm_arr = preproc.normalize(test_arr, False)
351/76: norm_arr.std()
351/77: norm_arr.mean()
351/78: test_arr.mean()
351/79: feat_norm_arr = feature_normalize(test_arr)
351/80: feat_norm_arr.mean()
351/81: test_arr.mean()
351/82: test_arr = np.arange(25700, dtype='float32').reshape(100,257)
351/83: test_arr = preproc.normalize(test_arr, True)
351/84: test_arr.mean()
351/85: test_arr.std()
351/86: from loader import compute_mean_std
351/87: from speech.loader import compute_mean_std
351/88: mean, std = comput_mean_std(["~/CS/consulting/firstlayerai/phoneme_classification/src/awni_speech/speech/tests/pytest/test_audio/Speak-out.wav"], "log_spectrogram", 32, 16, True)
351/89: mean, std = compute_mean_std(["~/CS/consulting/firstlayerai/phoneme_classification/src/awni_speech/speech/tests/pytest/test_audio/Speak-out.wav"], "log_spectrogram", 32, 16, True)
351/90: mean, std = compute_mean_std(["/User/dustin/CS/consulting/firstlayerai/phoneme_classification/src/awni_speech/speech/tests/pytest/test_audio/Speak-out.wav"], "log_spectrogram", 32, 16, True)
351/91: mean, std = compute_mean_std(["/Users/dustin/CS/consulting/firstlayerai/phoneme_classification/src/awni_speech/speech/tests/pytest/test_audio/Speak-out.wav"], "log_spectrogram", 32, 16, True)
351/92: mean
351/93: mean.mean()
351/94: std.std()
351/95: std.mean()
352/1: import yaml
352/2:
with open('config.yaml', 'r') as file:
    config = yaml.load(file)
352/3:
with open('config.yaml', 'r') as file:
    config = yaml.load(file, Loader=yaml.FullLoader)
352/4: config
352/5:
with open('config.yaml', 'r') as file:
    config = yaml.load(file, Loader=yaml.FullLoader)
352/6: config
352/7:
with open('config.yaml', 'r') as file:
    config = yaml.load(file, Loader=yaml.FullLoader)
352/8:
with open('config.yaml', 'r') as file:
    config = yaml.load(file, Loader=yaml.FullLoader)
    config
352/9: config
352/10:
import random

aList = [20, 40, 80, 100, 120]
print ("choosing 3 random items from a list using random.sample() function")
sampled_list = random.sample(aList, 3)
print(sampled_list)
352/11:
import random

aList = [20, 40, 80, 100, 120]
print ("choosing 3 random items from a list using random.sample() function")
sampled_list = random.sample(aList, 3)
print(sampled_list)
352/12:
import random

aList = [20, 40, 80, 100, 120]
print ("choosing 3 random items from a list using random.sample() function")
sampled_list = random.sample(aList, 3)
print(sampled_list)
352/13:
with open('config.yaml', 'r') as file:
    config = yaml.load(file, Loader=yaml.FullLoader)
    config
352/14:
    data_cfg = config['data']
    process_cfg = config['processor']
352/15: ls
352/16: from preprocess import DataProcessor
352/17: processor = DataProcessor(data_cfg, process_cfg)
352/18: print(processor)
352/19: features, labels = processor.get_features_labels()
352/20:
with open('config.yaml', 'r') as file:
    config = yaml.load(file, Loader=yaml.FullLoader)
    config
352/21:
    data_cfg = config['data']
    process_cfg = config['processor']
352/22: features, labels = processor.get_features_labels()
352/23: data_cfg
352/24: features, labels = processor.get_features_labels()
352/25: processor = DataProcessor(data_cfg, process_cfg)
352/26: features, labels = processor.get_features_labels()
352/27: import importlib
352/28: import preprocess
352/29: importlib.reload(preprocess)
352/30: processor = preprocess.DataProcessor(data_cfg, process_cfg)
352/31: features, labels = processor.get_features_labels()
352/32: importlib.reload(preprocess)
352/33: processor = preprocess.DataProcessor(data_cfg, process_cfg)
352/34: features, labels = processor.get_features_labels()
352/35: import utils.bps
352/36: importlib.reload(utils.bps)
352/37: processor = preprocess.DataProcessor(data_cfg, process_cfg)
352/38: features, labels = processor.get_features_labels()
352/39: importlib.reload(preprocess)
352/40: processor = preprocess.DataProcessor(data_cfg, process_cfg)
352/41: features, labels = processor.get_features_labels()
352/42: importlib.reload(preprocess)
352/43: processor = preprocess.DataProcessor(data_cfg, process_cfg)
352/44: features, labels = processor.get_features_labels()
352/45: len(features)
352/46: x_input = np.arange(300, dtype='float64').reshape(5,20,3)
352/47: import numpy as np
352/48: x_input = np.arange(300, dtype='float64').reshape(5,20,3)
352/49: x_input.shape
352/50: x_input[0]
352/51: x_input[0].shape
352/52: np.mean(x_input[0], axis=0)
352/53: np.square(2)
352/54: np.square([2,2])
352/55: np.square([[2,2],[3,3]])
352/56: import trimesh
352/57: mesh_path = "/Users/dustin/CS/consulting/voodoo/mask/src/data/linear_reg/example/FLAME_sample_1592332054_01.ply"
352/58: mesh = trimesh.load(mesh_path, process=False)
352/59: from utils.mesh_utils import get_face_landmarks, get_mask_landmarks_3
352/60: mask_lmks = get_mask_landmarks_3(mesh)
352/61: mask_lmks
352/62: from itertools import chain
352/63: mask_path = list(chain.from_iterable(get_mask_landmarks_3(mesh)))
352/64: len(mask_path)
352/65: mask_path
352/66: mesh
352/67: mask_mlks[0]
352/68: mask_lmks[0]
352/69: mask_lmks[0].verticies
352/70: mask_lmks[0].__dir__()
352/71: mask_lmks[0].data
352/72: mask_lmks[0].ndim
352/73: mask_lmks[0].tolist
352/74: mask_lmks[0].tolist()
352/75: mesh
352/76: mesh.verticies[3121]
352/77: mesh.vertices[3121]
352/78: mesh.vertices[3121, 2094]
352/79: mesh.vertices[[3121, 2094]]
352/80: mesh.vertices[[3121, 2094, 3391]]
352/81: np.array(mesh.vertices[[3121, 2094, 3391]])
352/82: from mesh_utils import average_vertices
352/83: from utils.mesh_utils import average_vertices
352/84: average_vertices(mesh, [3531, 3509])
352/85: np.array(average_vertices(mesh, [3531, 3509]))
352/86: np.array(mesh.vertices[[3121, 2094, 3391]]).append(np.array(average_vertices(mesh, [3531, 3509])) )
352/87: np.append(np.array(mesh.vertices[[3121, 2094, 3391]]), np.array(average_vertices(mesh, [3531, 3509])) )
352/88: np.append(np.array(mesh.vertices[[3121, 2094, 3391]]), np.array(average_vertices(mesh, [3531, 3509])) , axis=0)
352/89: np.append(np.array(mesh.vertices[[3121, 2094, 3391]]), np.array(average_vertices(mesh, [3531, 3509])) , axis=1)
352/90: np.append(np.array(mesh.vertices[[3121, 2094, 3391]]), np.array(average_vertices(mesh, [3531, 3509])).reshape(3,1) , axis=1)
352/91: np.append(np.array(mesh.vertices[[3121, 2094, 3391]]), np.array(average_vertices(mesh, [3531, 3509])).reshape(3,1) , axis=0)
352/92: np.array(average_vertices(mesh, [3531, 3509]))
352/93: np.array(average_vertices(mesh, [3531, 3509])).shape
352/94: np.append(np.array(mesh.vertices[[3121, 2094, 3391]]), np.array(average_vertices(mesh, [3531, 3509])).reshape(3,1) , axis=0).shape
352/95: np.append(np.array(mesh.vertices[[3121, 2094, 3391]]), np.array(average_vertices(mesh, [3531, 3509])).reshape(3,1) , axis=1).shape
352/96: np.append(np.array(mesh.vertices[[3121, 2094, 3391]]), np.array(average_vertices(mesh, [3531, 3509])).reshape(3,1) , axis=0).shape
352/97: np.append(np.array(mesh.vertices[[3121, 2094, 3391]]), np.array(average_vertices(mesh, [3531, 3509])).reshape(1,3) , axis=0).shape
352/98: np.array(average_vertices(mesh, [3531, 3509])).shape
352/99: np.array(average_vertices(mesh, [3531, 3509])).size
352/100: test_arr = np.append(np.array(mesh.vertices[[3121, 2094, 3391]]), np.array(average_vertices(mesh, [3531, 3509])).reshape(1,3) , axis=0)
352/101: test_arr.flatten()
352/102: test_arr.flatten().shape
352/103: test_arr
352/104: flat_arr = test_arr.flatten()
352/105: flat_arr
352/106: flat_arr.reshape(4,3) == test_arr
352/107: np.array(average_vertices(mesh, [3531, 3509])).shape
352/108: np.array(mesh.vertices[[3121, 2094, 3391]]).shape
354/1: arr = np.arange(12).reshape(4,3)
354/2: import numpy as np
354/3: arr = np.arange(12).reshape(4,3)
354/4: arr.shape
354/5: arr.std()
354/6: arr.std(axis=0)
354/7: scan_path = "/Users/dustin/CS/consulting/voodoo/mask/src/data/linear_reg/new_scans/scaled_scans/head3d-scaled.obj"
354/8: import trimesh
354/9: scan_mesh = trimesh.load(scan_path, process=False)
354/10: scan_mesh
354/11: scan_mesh.verticies[3564]
354/12: scan_mesh.vertices[3564]
354/13: scan_mesh.vertices
354/14: len(scan_mesh.vertices)
354/15: scan_mesh.vertices[59737]
354/16: arr = np.arange(9).reshape((3,1,3))
354/17: arr.shape
354/18: np.squeeze(arr).shape
354/19: arr
354/20: arr.reshape(3,3)
354/21: arr.reshape(3,3).shape
354/22: arr = np.arange(180).reshape(5,12,3)
354/23: arr.shape
354/24: arr.sum()
354/25: arr = np.ones((5,12,3))
354/26: arr.shape
354/27: mean=np.arange(15).reshape(5,3)
354/28: max=np.arange(5).reshape(5,)
354/29: max.shape
354/30: arr
354/31: arr = np.ones((5,3,3))
354/32: arr
354/33: arr = np.ones((5,4,3))
354/34: arr
354/35: mean.shape
354/36: max
354/37: arr_max = arr * max
354/38: maxim=np.arange(5).reshape(5,1)
354/39: arr_max = arr * maxim
354/40: maxim=np.arange(5).reshape(5,)
354/41: arr_max = arr.T * maxim
354/42: arr_max = arr_max.T
354/43: arr
354/44: maxim
354/45: arr_max
354/46: maxim=np.arange(5).reshape(5,1)
354/47: arr_max = (arr.T * maxim).T
354/48: maxim=np.arange(5).reshape(5,)
354/49: arr_max = (arr.T * maxim).T
354/50: arr_max
354/51: arr_max.shape
354/52: maxim=np.arange(5).reshape(5,1,1)
354/53: arr_max = (arr * maxim)
354/54: arr_max
354/55: arr_max_sum += mean
354/56: arr_max_sum = arr_max + mean
354/57: mean=np.arange(15).reshape(5,1,3)
354/58: arr_max_sum = arr_max + mean
354/59: mean
354/60: arr_max_sum
354/61: len((4, 28, 3))
354/62: test_arr
354/63: test_arr = np.array([[-0.004047057441008898, -0.04371971512834231, 0.05815955003102621], [-0.0018960582601721399, -0.043225258588790894, 0.058969312657912575], [0.004296212607490209, -0.0438393863538901, 0.05852298314372698], [0.02419377552966277, -0.045657701790332794, 0.04935537030299505], [0.006540982984006405, -0.04385538647572199, 0.058507723112901054], [-0.001489839519005424, -0.04349687943855921, 0.05913414185245832], [-0.006537419278174639, -0.04329786077141762, 0.05860800792773565]])
354/64: test_arr
354/65: from datetime import time
354/66: time.now()
354/67: from datetime import datetime
354/68: dt = datetime.now()
354/69: dt.microsecond
354/70: import time
354/71: time.time()
354/72: int(time.time())
354/73: int(time.time())
354/74: int(time.time())
354/75: int(time.time())
354/76: int(time.time())
354/77: int(time.time())
354/78: int(time.time())
354/79: time.time()
354/80: int(round(time.time() * 1000))
354/81: import os
354/82: os.path.splitext("core/_methods.py")
354/83: import numpy
354/84: import matplotlib.pyplot as plt
354/85: noise = np.random.normal(loc=0.0, scale=1.0, size=(100,))
354/86: plt.plot(noise)
354/87: plt.show()
354/88: plt.hist(noise); plt.show()
354/89: noise = np.random.normal(loc=0.0, scale=1.0, size=(1000,))
354/90: plt.hist(noise); plt.show()
354/91: noise = np.random.normal(loc=0.0, scale=1.0, size=(10000,))
354/92: plt.hist(noise); plt.show()
354/93: noise = np.random.normal(loc=0.0, scale=1.0, size=(100,))
354/94: plt.hist(noise); plt.show()
354/95: noise = np.random.normal(loc=0.0, scale=1.0, size=(1000,))
354/96: plt.hist(noise); plt.show()
354/97: noise = np.random.normal(loc=0.0, scale=1.0, size=(10000,))
354/98: plt.hist(noise); plt.show()
354/99: noise = np.random.normal(loc=0.0, scale=1.0, size=(1000,))
354/100: plt.hist(noise); plt.show()
354/101: noise = np.random.normal(loc=0.0, scale=1.0, size=(1000,))
354/102: plt.hist(noise); plt.show()
354/103: noise = np.random.normal(loc=1.0, scale=1.0, size=(1000,))
354/104: plt.hist(noise); plt.show()
354/105: noise = np.random.normal(loc=0.0, scale=0.5, size=(1000,))
354/106: plt.hist(noise); plt.show()
354/107: noise = np.random.normal(loc=0.0, scale=0.1, size=(1000,))
354/108: plt.hist(noise); plt.show()
354/109: noise = np.random.normal(loc=0.0, scale=0.01, size=(1000,))
354/110: plt.hist(noise); plt.show()
354/111: noise.sum()
354/112: noise = np.random.normal(loc=0.0, scale=1, size=(1000,))
354/113: noise.sum()
354/114: noise.std()
354/115: noise = np.random.normal(loc=0.0, scale=0.0, size=(1000,))
354/116: noise.sum()
354/117: noise = np.random.normal(loc=1.0, scale=0.0, size=(1000,))
354/118: noise.sum()
355/1: train_mesh_path = "/Users/dustin/CS/consulting/voodoo/mask/src/data/linear_reg/test_data/hollowed-FLAME_2020-07-19.ply"
355/2: import trimesh
355/3: mesh = trimesh.load(train_mesh_path, process=False)
355/4: mesh.vertices
355/5: mesh.__dir__()
355/6: mesh.to_dict()
355/7: mesh.to_dict()
355/8: mesh.to_dict().keys()
355/9: type(mesh.to_dict()['vertices'])
355/10: mesh.to_dict()['metadata']
355/11: type(mesh.vertices)
355/12: mesh.vertices
355/13: mesh.vertices[1]
355/14: len(mesh.vertices)
355/15: mesh.vertices[0]
355/16: mesh.vertices[4592]
355/17: mesh.vertices[3122]
355/18: mesh.vertices[3621]
355/19: mesh.vertices[3756]
355/20: mesh.vertices[3539]
355/21: org_mesh = trimesh.load("/Users/dustin/CS/consulting/voodoo/mask/src/data/linear_reg/training_data/2020-06-28/raw_data/FLAME_sample_05.ply", process=False)
355/22: len(org_mesh.vertices)
355/23: len(mesh.vertices)
355/24: mesh.vertices['4693']
355/25: mesh.vertices[4693]
355/26: mesh.to_dict()['vertices'][:5]
355/27: x_set={}
355/28: type(x_set)
355/29: x_set=set()
355/30: type(x_set)
355/31: y_set=set()
355/32: z_set=set()
355/33: z_set.add(3)
355/34: z_set
355/35: z_set=set()
355/36: z_set
355/37: in_hollow=list()
355/38:
for point in mesh.to_dict()['vertices']:
    x_set.add(point[0])
    y_set.add(point[1])
    z_set.add(point[2])
355/39: len(x_set)
355/40: x_set[:5]
355/41: org_mesh.to_dict()['vertices'][:5]
355/42: 0.05452040210366249 in x_set
355/43: -0.006017577368766069 in y_set
355/44: -0.04729744419455528 in z_set
355/45:
for org_idx, point in org_mesh.to_dict()['vertices']:
    if point[0] in x_set:
        if point[1] in y_set:
            if point[2] in z_set:
                in_hollow.append(org_idx)
355/46:
for org_idx, point in enumerate(org_mesh.to_dict()['vertices']):
    if point[0] in x_set:
        if point[1] in y_set:
            if point[2] in z_set:
                in_hollow.append(org_idx)
355/47: len(in_hollow)
355/48: from utils.save_load import save_pickel
355/49: from utils.save_load import save_pickle
355/50: ls
355/51: save_pickle("./hollow_idx.pickle", in_hollow)
355/52: hollow_idx
355/53: in_hollow
355/54: from utils.save_load import load_pickle
355/55: hollow_idx = load_pickle("hollow_idx.pickle")
355/56: len(hollow_idx)
355/57: type(hollow_idx)
355/58: len(org_mesh.vertices)
355/59: len(mesh.vertices)
355/60: mesh.vertices[4593]
356/1: import numpy as np
356/2: import matplotlib.pyplot as plt
356/3: norm_dist = np.random.normal(loc=1.0, scale=0.075)
356/4: plt.hist(norm_dist); plt.show()
356/5: norm_dist = np.random.normal(loc=1.0, scale=0.075, size=1000)
356/6: plt.hist(norm_dist); plt.show()
356/7: plt.hist(norm_dist); plt.xlim(left=0.70, right=1.3); plt.show()
356/8: uni_dist = np.random.uniform(low=0.85, high=1.15, size=1000)
356/9: plt.hist(uni_dist); plt.xlim(left=0.70, right=1.3); plt.show()
356/10: fig, (ax1, ax2) = plt.subplots(2)
356/11: ax1.hist(norm_dist)
356/12: ax2.hist(uni_dist)
356/13: plt.xlim(left=0.75, right=1.25)
356/14: fig, (ax1, ax2) = plt.subplots(2)
356/15: ax1.hist(norm_dist)
356/16: ax2.hist(uni_dist)
356/17: ax1.__dir__()
356/18: fig, (ax1, ax2) = plt.subplots(2)
356/19: ax1.hist(norm_dist, color='r')
356/20: ax2.hist(uni_dist, color='b')
356/21: np.random.normal(1.0, 0.075)
356/22: np.random.normal(1.0, 0.075)
356/23: np.random.normal(1.0, 0.075)
356/24: np.random.normal(1.0, 0.075)
356/25: np.random.normal(1.0, 0.075)
356/26: np.random.normal(1.0, 0.075)
356/27: np.random.normal(1.0, 0.075)
356/28: np.random.normal(1.0, 0.075)
356/29: np.random.normal(1.0, 0.075)
356/30: np.random.normal(1.0, 0.075)
356/31: np.random.normal(1.0, 0.075)
356/32: import scipy
356/33: scipy.stats.halfnorm.rvs(0.0, scale=0.25)
356/34: import scipy.stats
356/35: scipy.stats.halfnorm.rvs(0.0, scale=0.25)
356/36: half_norm_dist=scipy.stats.halfnorm.rvs(1.0, scale=0.075, size=1000)
356/37: fig, (ax1, ax2, ax3) = plt.subplots(3)
356/38: ax1.hist(norm_dist, color='r')
356/39: ax2.hist(uni_dist, color='b')
356/40: ax3.hist(half_norm_dist, color='c')
356/41: half_norm_dist_neg=scipy.stats.halfnorm.rvs(1.0, scale=-0.075, size=1000)
356/42: fig, (ax1, ax2, ax3, ax4, ax5) = plt.subplots(5)
356/43: ax1.hist(norm_dist, color='r')
356/44: ax2.hist(uni_dist, color='b')
356/45: ax3.hist(half_norm_dist, color='c')
356/46: tnorm_pos=scipy.stats.truncnorm.rvs(1.0, 1.15, loc=1.0, scale=0.075)
356/47: ax3.hist(tnorm_pos, color='g')
356/48: ax3.hist(half_norm_dist, color='c')
356/49: ax4.hist(tnorm_pos, color='g')
356/50: fig, (ax1, ax2, ax3, ax4, ax5) = plt.subplots(5)
356/51: ax1.hist(norm_dist, color='r')
356/52: ax2.hist(uni_dist, color='b')
356/53: ax3.hist(half_norm_dist, color='c')
356/54: ax4.hist(tnorm_pos, color='g')
356/55: t_norm_pos.std()
356/56: tnorm_pos.std()
356/57: a, b = 0.1, 2
356/58: from scipy.stats import truncnorm
356/59: mean, var, skew, kurt = truncnorm.stats(a, b, moments='mvsk')
356/60: mean
356/61: var
356/62: r = truncnorm.rvs(a, b, size=1000)
356/63: fig, ax = plt.subplots(1, 1)
356/64: ax.hist(r, normed=True, histtype='stepfilled', alpha=0.2)
356/65:  ax.legend(loc='best', frameon=False)
356/66: tnorm_pos=scipy.stats.truncnorm.rvs(1.0, 1.15, loc=1.0, scale=0.075, size=1000)
356/67: fig, (ax1, ax2, ax3, ax4, ax5) = plt.subplots(5)
356/68: ax1.hist(norm_dist, color='r')
356/69: ax2.hist(uni_dist, color='b')
356/70: ax3.hist(half_norm_dist, color='c')
356/71: ax4.hist(tnorm_pos, color='g')
356/72: tnorm_pos=scipy.stats.truncnorm.rvs(0, 2, loc=1.0, scale=0.075, size=1000)
356/73: fig, (ax1, ax2, ax3, ax4, ax5) = plt.subplots(5)
356/74: ax1.hist(norm_dist, color='r')
356/75: ax2.hist(uni_dist, color='b')
356/76: ax3.hist(half_norm_dist, color='c')
356/77: ax4.hist(tnorm_pos, color='g')
356/78: tnorm_neg=scipy.stats.truncnorm.rvs(-2, 0, loc=1.0, scale=0.075, size=1000)
356/79: ax5.hist(tnorm_neg, color='o')
356/80: ax5.hist(tnorm_neg, color='y')
356/81: fig, (ax1, ax2, ax3, ax4, ax5, ax6) = plt.subplots(6)
356/82: ax1.hist(norm_dist, color='r')
356/83: ax2.hist(uni_dist, color='b')
356/84: ax3.hist(half_norm_dist, color='c')
356/85: ax4.hist(tnorm_pos, color='g')
356/86: ax5.hist(tnorm_neg, color='y')
356/87: tnorm_bound=scipy.stats.truncnorm.rvs(-13, 13, loc=1.0, scale=0.075, size=1000)
356/88: ax6.hist(tnorm_bound, color='r')
356/89: rang = [0, 5]
356/90: rang1=[5,0]
356/91: rang.sort()
356/92: range
356/93: rang
356/94: rang1.sort()
356/95: rang1
356/96: from speech.utils import signal_augment
356/97: value = signal_augment.get_value_from_truncnorm(0, [0, 0.5], [0, 0.8], None)
356/98: import importlib
356/99: importlib.reload(signal_augment)
356/100: value = signal_augment.get_value_from_truncnorm(0, [0, 0.5], [0, 0.8], None)
356/101: value = signal_augment.get_value_from_truncnorm(0, [0, 0.5], [0, 0.8])
356/102: array = [0, 1, 2, 3, 4]
356/103: max(array)
356/104: np.mean(array)
356/105: np.max(array)
356/106: np.mean([10,30])
356/107: np.mean([10,50])
356/108: value
356/109: value = signal_augment.get_value_from_truncnorm(0, [0, 0.5], [0, 0.8], size=1)
356/110: value = signal_augment.get_value_from_truncnorm(0, [0, 0.5], [0, 0.8])
356/111: vale_test=scipy.stats.truncnorm.rvs(-13, 13, loc=1.0, scale=0.075, size=1)
356/112: vale_test
356/113: vale_test=scipy.stats.truncnorm.rvs(-13, 13, loc=1.0, scale=0.075, size=None)
356/114: vale_test
357/1: import torch
357/2: from torch import nn
357/3:
class MLP(nn.Module):
    def __init__(self, num_bps_points):
        super(MLP, self).__init__()
        self.layers = nn.Sequential(
            nn.Linear(num_bps_points, 100),
            nn.ReLU(),
            nn.Linear(100, 3)
        )
        
    def forward(self, x):
        # convert tensor (128, 1, 28, 28) --> (128, 1*28*28)
        x = self.layers(x)
        return x
357/4: mlp = MLP(512)
357/5: print(mlp)
357/6: import numpy as np
357/7: arr = np.arange(5120, dtype="float64").reshape(10, 512)
357/8: arr.shape
357/9: mlp(arr)
357/10: tensor = torch.from_numpy(arr)
357/11: mlp(tensor)
357/12: tensor
357/13: tensor = tensor.type(torch.float32)
357/14: mlp(tensor)
357/15: optimizer = torch.optim.Adam(model.parameters(), lr=0.001)
357/16: optimizer = torch.optim.Adam(mlp.parameters(), lr=0.001)
357/17: loss_fn = torch.nn.MSELoss(size_average=None, reduce=None, reduction='mean')
357/18: mlp.train()
357/19:
labels = torch.ones(10, 3)
for i in range(10):
    print("epoch", i)
    optimizer.zero_grad()
    outputs = mlp(tensor)
    loss = loss_fn(outputs, labels)
    loss.backward()
    optimizer.step()
    print("loss", loss.item())
357/20: import tqdm
357/21: tq = tqdm.tqdm(range(10))
357/22: tq.set_postfix
357/23: tq.__dir__()
357/24: torch
357/25: model = torch.load("./models/torch_test.pth")
357/26: model
358/1: config_path = "./examples/librispeech/ctc_config_ph2.yaml"
358/2: import yaml
359/1: import yaml
359/2: config_path = "./examples/librispeech/ctc_config_ph2.yaml"
359/3:
        with open(config_path, 'r') as config_file:
            config = yaml.load(config_file)
359/4:
        with open(config_path, 'r') as config_file:
            config = yaml.load(config_file)
359/5: config.get("spec_augment_policy")
359/6: config.get("spec_augment_policy")
359/7: config
359/8: config['spec_augment_policy']
359/9: config
359/10: config.get('rnn')
359/11: config['rnn']
359/12: type(config)
359/13: config['model']['rnn']
359/14: config['model']['encoder']['rnn']
359/15: config['preproc']['spec_augment_policy']
359/16: config['optimizer']['dampening']
359/17: type(config['optimizer']['dampening'])
359/18: config_path = "./examples/librispeech/ctc_config_ph2.yaml"
359/19:
        with open(config_path, 'r') as config_file:
            config = yaml.load(config_file)
359/20: config['model']['trained_path']
359/21: type(config['model']['trained_path'])
359/22: type(config['preproc']['tempo_range'])
359/23: type(config['preproc']['tempo_range'])
359/24: import numpy as np
359/25: np.mean(config['preproc']['tempo_range'])
359/26: from speech.utils import signal_augment
359/27: value = signal_augment.get_value_from_truncnorm(np.mean(config['preproc']['tempo_range']), config['preproc']['tempo_range'], config['preproc']['tempo_range'])
359/28: value
359/29: import pandas as pd
359/30: eng_sent_path = "/Users/dustin/CS/consulting/firstlayerai/phoneme_classification/src/awni_speech/speech/examples/tatoeba/eng_sentences.tsv"
359/31: eng_sent_df = pd.read_csv(eng_sent_path)
359/32: eng_sent_df = pd.read_csv(eng_sent_path, delimiter='\t')
359/33: eng_setn_df.head()
359/34: eng_sent_df.head()
359/35: info(pd.read_csv)
359/36: eng_sent_df = pd.read_csv(eng_sent_path, sep='\t', header=None)
359/37: eng_sent_df.head()
359/38: eng_sent_df = pd.read_csv(eng_sent_path, sep='\t', header=None, usecols=['id', 'lang', 'text'])
359/39: eng_sent_df = pd.read_csv(eng_sent_path, sep='\t', header=['id', 'lang', 'text'])
359/40: eng_sent_df = pd.read_csv(eng_sent_path, sep='\t', header=None, names=['id', 'lang', 'text'])
359/41: eng_sent_df.head()
359/42: eng_sent_df.shape
359/43: audio_sent_path = "/Users/dustin/CS/consulting/firstlayerai/phoneme_classification/src/awni_speech/speech/examples/tatoeba/sentences_with_audio.tsv"
359/44: audio_sent_df = pd.read_csv(audio_sent_path, sep='\t', header=None, names=['id', 'user', 'license', 'attr-url'])
359/45: audio_sent_df.shape
359/46: audio_eng_sent_df = pd.merge(eng_sent_df, audio_sent_df, how='inner', on='id', suffixes=('_eng', '_aud'))
359/47: audio_eng_sent_df.head()
359/48: audio_sent_df.head()
359/49: audio_eng_sent_df.shape
359/50: user_lang_path = "/Users/dustin/CS/consulting/firstlayerai/phoneme_classification/src/awni_speech/speech/examples/tatoeba/user_languages.tsv"
359/51: user_lang_df = pd.read_csv(user_lang_path, sep='\t', header=None, names=['lang', 'skill', 'user', 'details'])
359/52: user_land_df.head()
359/53: user_lang_df.head()
359/54: audio_eng_sent_df.head()
359/55: audio_eng_skill_df = pd.merge(audio_eng_sent_df, user_lang_df, how='left', on='user', suffixes=('_m', '_s'))
359/56: audio_eng_skill_df.head()
359/57: audio_eng_skill_df.head()
359/58: audio_eng_skill_df.shape
359/59: user_lang_df.shape()
359/60: user_lang_df.shape
359/61: user_lang_df[user_lang_df['lang']=='eng'].shape
359/62: user_lang_df[user_lang_df['lang']=='eng'].head()
359/63: user_lang_df.head()
359/64: user_lang_df[user_lang_df['lang']=='eng']['lang'].value_counts(sort=True, ascending=False)
359/65: user_lang_df[user_lang_df['lang']=='eng']['user'].value_counts(sort=True, ascending=False)
359/66: user_lang_df[user_lang_df['lang']=='eng']['user'].value_counts(sort=True, ascending=False).shape
359/67: user_lang_df[user_lang_df['lang']=='eng']['user'].shape
359/68: eng_skill_df = user_lang_df[user_lang_df['lang']=='eng']
359/69: eng_skill_df.shape
359/70: audio_eng_skill_df = pd.merge(audio_eng_sent_df, eng_skill_df, how='left', on='user', suffixes=('_m', '_s'))
359/71: audio_eng_skill_df.shape
359/72: audio_eng_sent_df.shape
359/73: audio_eng_skill_df.head()
359/74: audio_eng_skill_df[audio_eng_skill_df['user']=='\N'].shape
359/75: audio_eng_skill_df[audio_eng_skill_df['user']==\N].shape
359/76: audio_eng_skill_df[audio_eng_skill_df['user']==\N]
359/77: audio_eng_skill_df[audio_eng_skill_df['user']=='\N']
359/78: audio_eng_skill_df[audio_eng_skill_df['user']=='\\N']
359/79: audio_eng_skill_df[audio_eng_skill_df['user']=='\\N'].shape
359/80: audio_eng_skill_df[audio_eng_skill_df['user']=='\N'].shape
359/81: audio_eng_skill_df[audio_eng_skill_df['user']=='\\N'].shape
359/82: audio_eng_skill_df.drop_duplicates(subset='id').shape
359/83: audio_eng_sent_df.shape
359/84: audio_eng_sent_df.drop_duplicates(subset='id').shape
359/85: audio_eng_skill_df = audio_eng_skill_df.drop_duplicates(subset='id')
359/86: audio_eng_skill_df.shape
359/87: audio_eng_skill_df[audio_eng_skill_df['user']=='\\N'].shape
359/88: audio_eng_skill_df.value_counts(sort=True, ascending=False)
359/89: audio_eng_skill_df['skill'].value_counts(sort=True, ascending=False)
359/90: eng_skill_df['skill'].value_counts(sort=True, ascending=False)
359/91: audio_eng_skill_df['user'].value_counts(sort=True, ascending=False)
359/92: sent_review_path = "/Users/dustin/CS/consulting/firstlayerai/phoneme_classification/src/awni_speech/speech/examples/tatoeba/users_sentences.tsv"
359/93: sent_review_df = pd.read_csv(sent_review_path, sep='\t', header=None, names=['user', 'lang', 'id', 'review', 'date_add', 'date_mod'])
359/94: sent_review_df['review'].value_counts(sort=True, ascending=False)
359/95: sent_review_df.head()
359/96: sent_review_df = pd.read_csv(sent_review_path, sep='\t', header=None, names=['user', 'id', 'review', 'date_add', 'date_mod'])
359/97: sent_review_df['review'].value_counts(sort=True, ascending=False)
359/98: audio_eng_skill_df.shape
359/99: 494779/498959
360/1: 'hey' * 3
360/2: 'hey ' * 3
360/3: 'hey ' * 3 + "buddy " * 3
361/1: bellus_path = "/Users/dustin/CS/consulting/voodoo/mask/src/data/linear_reg/test_data/bellus-w-lmks/2020-07-27/schwartz-family/lmks/00-david-lmks.obj"
361/2: import trimesh
361/3:  mesh = trimesh.load(beluls_path, process=False)
361/4:  mesh = trimesh.load(bellus_path, process=False)
361/5: len(mesh.vertices)
361/6: mech.verticies[0]
361/7: mesh.verticies[0]
361/8: mesh.vertices[0]
361/9: mesh.vertices[307]
361/10: mesh.vertices[308]
362/1: 1 ==[1]
362/2: 2^2
362/3: 2^1
362/4: 2^5
363/1: s = set()
363/2: s.add([1, 2, 3, 3, 3, 4])
363/3: s.add(set([1, 2, 3, 3, 3, 4]))
363/4: s
363/5: s.add(1)
363/6: s.add(*[1, 2, 3, 3, 3, 4])
363/7: s.update([1, 2, 2, 3, 3, 4, 5])
363/8: s
363/9: s
363/10: s.update([1, 2, 2, 3, 3, 4, 5])
363/11: s
363/12: s.union([1, 2, 2, 3, 3, 4, 5])
364/1: import matplotlib.pyplot as plt
365/1: import matplotlib.pyplot as plt
365/2: from speech.utils.io import read_pickle
365/3: loss_dict = read_pickle("./loss_lr.pickle")
365/4: losses = loss_dict['losses']
365/5: learning_rates = loss_dict['learning_rates']
365/6: losses
365/7: plt.plot(losses, learning_rates)
365/8: plt.plot(learning_rates, losses)
365/9: plt.xscale('log')
365/10: plt.xlabel('learning_rates')
365/11: plt.ylabel('loss')
365/12: plt.save("./plot.svg")
365/13: plt.savefig('./plot.png')
365/14: loss_dict = read_pickle("./loss_lr.pickle")
365/15: loss_dict
366/1: from speech.utils.io import read_pickle
367/1: from speech.utils.io import read_pickle
367/2: import matplotlib.pyplot as plt
367/3: loss_dict = read_pickle("./loss_lr.pickle")
367/4: losses = loss_dict['losses']
367/5: learning_rates = loss_dict['learning_rates']
367/6: len(losses)
367/7: len(learning_rates)
367/8: plt.plot(losses, learning_rates)
367/9: learning_rates[-1]
367/10: plt.plot(learning_rates, losses)
367/11: plt.xscale('log')
367/12: plt.xscale('log')
367/13: plt.xscale('symlog')
367/14: plt.xscale('log')
367/15: plt.xscale('linear')
367/16: plt.xlim(0, 11)
367/17: learning_rates[0]
367/18: learning_rates[1]
367/19: learning_rates[2]
367/20: learning_rates[3]
367/21: learning_rates[4]
367/22: learning_rates[100]
367/23: learning_rates[1000]
367/24: len(learning_rates)
367/25: learning_rates[3000]
367/26: learning_rates[4000]
367/27: learning_rates[5000]
367/28: learning_rates[6000]
367/29: learning_rates[7000]
367/30: learning_rates[8000]
367/31: learning_rates[9000]
367/32: learning_rates[10000]
367/33: learning_rates[20000]
367/34: learning_rates[30000]
367/35: learning_rates[40000]
367/36: learning_rates[50000]
367/37: learning_rates[60000]
367/38: learning_rates[70000]
367/39: learning_rates[80000]
367/40: learning_rates[90000]
367/41: losses[90000]
367/42: losses[20000]
367/43: losses[10000]
367/44: losses[4000]
367/45: losses[3000]
367/46: losses[2000]
367/47: losses[1000]
367/48: losses[100]
367/49: losses[80]
367/50: losses[70]
367/51: losses[60]
367/52: losses[50]
367/53: losses[40]
367/54: losses[30]
367/55: losses[20]
367/56: losses[10]
367/57: losses[0]
367/58: losses[1]
367/59: losses[2]
367/60: losses[3]
367/61: losses[4]
367/62: losses[5]
367/63: losses[6]
367/64: losses[7]
367/65: losses[8]
367/66: losses[9]
368/1: from speech.utils.io import read_pickle
369/1: from speech.utils.io import read_pickle
369/2: loss_dict = read_pickle("loss_lr.pickle")
369/3: loss_dict = read_pickle("./loss_lr.pickle")
369/4: loss_dict = read_pickle("./loss_lr.pickle")
369/5: import matplotlib.pyplot as plt
369/6: plt.plot(loss_dict['learning_rates'], loss_dict['losses'])
369/7: plt.xscale('log')
369/8: plt.xlim(0, 10)
369/9: plt.ylim(0, 10)
369/10: ls
369/11: cd iter-1000_start-1_end-e6/
369/12: loss_dict = read_pickle("./loss_lr.pickle")
369/13: plt.plot(loss_dict['learning_rates'], loss_dict['losses'])
369/14: plt.ylim(0, 20)
369/15: plt.ylim(0, 30)
369/16: cd ../iter-2000_start-1_end-e6/
369/17: loss_dict = read_pickle("./loss_lr.pickle")
369/18: plt.plot(loss_dict['learning_rates'], loss_dict['losses'])
369/19: plt.ylim(2.5, 4)
369/20: plt.ylim(2.5, 5)
369/21: plt.close('all')
369/22: plt.plot(loss_dict['learning_rates'], loss_dict['losses'])
369/23: plt.xscale('log')
369/24: plt.ylim(0, 5)
369/25: plt.ylim(2.5, 5)
369/26: cd ../iter-4000_start-1_end-e6/
369/27: loss_dict = read_pickle("./loss_lr.pickle")
369/28: plt.close('all')
369/29: plt.plot(loss_dict['learning_rates'], loss_dict['losses'])
369/30: plt.xscale('log')
369/31: plt.ylim(0, 200)
369/32: plt.ylim(100, 200)
369/33: cd ../iter-8000_start-1_end-e6/
369/34: loss_dict = read_pickle("./loss_lr.pickle")
369/35: plt.plot(loss_dict['learning_rates'], loss_dict['losses'])
369/36: plt.ylim(0, 200)
369/37: plt.ylim(0, 25)
369/38: plt.ylim(5, 15)
369/39: plt.ylim(9, 12)
369/40: plt.ylim(0, 25)
369/41: plt.ylim(0, 2000)
369/42: plt.ylim(0, 200)
370/1: ls
370/2: from speech.utils.io import load
370/3:
def network_to_half(model):
    """
    Convert model to half precision in a batchnorm-safe way.
    """
    def bn_to_float(module):
        """
        BatchNorm layers need parameters in single precision. Find all layers and convert
        them back to float.
        """
        if isinstance(module, torch.nn.modules.batchnorm._BatchNorm):
            module.float()
        for child in module.children():
            bn_to_float(child)
        return module
    return bn_to_float(model.half())
370/4: import torch
370/5: ls
370/6: model = torch.load("./model_convert/torch_models/large-model_2020-08-10.pth")
370/7: model = torch.load("./model_convert/torch_models/large-model_2020-08-10.pth", map_location='cpu')
370/8: import sys
370/9: sys.getsizeof(model)
370/10: half_model = network_to_half(model)
370/11: sys.getsizeof(half_model)
370/12: torch.save(model, "./model_convert/torch_models/half_large-model_2020-08-10.pth")
369/43: cd ../full-epoch_start-05_end-e7/
369/44: loss_dict = read_pickle("./loss_lr.pickle")
369/45: plt.plot(loss_dict['learning_rates'], loss_dict['losses'])
369/46: plt.xscale('log')
369/47: plt.ylim(0, 10)
369/48: plt.ylim(2, 6)
369/49: plt.ylim(3.0, 4.0)
371/1: import torch
371/2: batch_size=8
371/3: torch.randn(batch_size, 1, 224, 224, requires_grad=True, device=device)
371/4: set_device=""
371/5: device = "cuda" if torch.cuda.is_available() and set_device!='cpu'  else "cpu"
371/6: device = torch.device(device)
371/7: device
371/8: torch.randn(batch_size, 1, 224, 224, requires_grad=True, device=device)
371/9: tensor = torch.randn(batch_size, 1, 224, 224, requires_grad=True, device=device)
371/10: tensor.device
371/11: tensor = torch.randn(batch_size, 1, 224, 224, requires_grad=True, dtype=torch.float32, device=device)
371/12: tensor.dtype
371/13: tensor = torch.randn(batch_size, 1, 224, 224, requires_grad=True, device=device)
371/14: tensor.dtype
371/15: tensor = torch.randn(batch_size, 1, 224, 224, requires_grad=True, dtype=torch.float16, device=device)
371/16: tensor = torch.randn(batch_size, 1, 224, 224, requires_grad=True, dtype=torch.float16)
371/17: tensor = torch.randn(batch_size, 1, 224, 224, requires_grad=True)
371/18: tensor.type(torch.float16)
371/19: tensor = tensor.type(torch.float16)
371/20: tensor.dtype
372/1: speaker_path = "/Users/dustin/CS/consulting/firstlayerai/data/flickr/flickr_audio/wav2spk.txt"
372/2:
with open(speaker_path, 'r') as fid:
    spk_f = fid.readlines()
372/3: print(spk_f[0:4])
372/4: print(spk_f[0].split())
372/5:
spk_set = set()
for line in spk_f:
    filename, spk_id = line.strip().split()
    spk_set.add(spk_id)
372/6: len(spk_set)
372/7:
spk_set = set()
spk_count = dict()
for line in spk_f:
    filename, spk_id = line.strip().split()
    spk_set.add(spk_id)
    spk_count[spk_id] = spk_count.get(spk_id, 0) +1
372/8: len(spk_count)
372/9: spk_count = {k, val for k, val in sorted(spk_count.items(), key=lambda x: x[1])}
372/10: spk_count = {k:val for k, val in sorted(spk_count.items(), key=lambda x: x[1])}
372/11: spk_count
372/12: spk_count = {k:val for k, val in sorted(spk_count.items(), key=lambda x: x[1], reverse=True)}
372/13: spk_count
372/14: import matplotlib.pyplot as plt
372/15: plt.hist(spk_count.values())
372/16: plt.xlabel('utterance count')
372/17: plt.ylabel('number of speakers')
372/18: import numpy as np
372/19: np.std(sk_count.values())
372/20: np.std(spk_count.values())
372/21: np.std(list(spk_count.values()))
372/22: sum(spk_count.values())
372/23: import os
372/24: os.cwd()
372/25: os.getcwd()
372/26: ls
372/27: !conda info -e
372/28: !conda activate awni_env37
373/1:
import coremltools as ct
from coremltools.models.neural_network import quantization_utils
373/2: ls
373/3: cd model_convert/
373/4: coreml_path = "./coreml_models/test-model_single-precision_2020-08-14_model.mlmodel"
373/5: model_fp32 = coremltools.models.MLModel(coreml_path)
373/6:
import coremltools
from coremltools.models.neural_network import quantization_utils
373/7: model_fp32 = coremltools.models.MLModel(coreml_path)
373/8: model_fp16 = quantization_utils.quantize_weights(model_fp32, nbits=16)
373/9: coreml_model.save("./coreml_models/test-model_half-precision_2020-08-14_model.mlmodel")
373/10: model_fp16.save("./coreml_models/test-model_half-precision_2020-08-14_model.mlmodel")
373/11: import numpy as np
373/12: np_11 = np.ones(5,2)
373/13: np_11 = np.ones((5,2))
373/14: np_11.dtype
373/15: np_11 = np_11 *1.1
373/16: np_11
373/17: np_10 = np.ones((5,2))
373/18: np.testing.assert_allclose(np_11, np_10, rtol=0.1)
373/19: np.testing.assert_allclose(np_10, np_11, rtol=0.2)
373/20: np.testing.assert_allclose(np_10, np_11, rtol=0.1)
373/21: np_09 = np_10 *0.9
373/22: np.testing.assert_allclose(np_10, np_09, rtol=0.1)
373/23: np_091 = np_10 *0.91
373/24: np.testing.assert_allclose(np_10, np_091, rtol=0.1)
373/25: np.testing.assert_allclose(np_10, np_09, rtol=0.1)
373/26: np.testing.assert_allclose(np_10, np_09, rtol=0.0, atol=0.1)
373/27: np.testing.assert_allclose(np_10, np_09, rtol=0.0, atol=0.09)
373/28: np.testing.assert_allclose(np_10, np_09, rtol=0.0, atol=0.08)
373/29: np.testing.assert_allclose(np_10, np_09, rtol=0.0, atol=0.05)
373/30: np.testing.assert_allclose(np_10, np_09, rtol=0.0, atol=0.1)
373/31: np.testing.assert_allclose(np_10, np_09, rtol=0.0, atol=0.099999)
373/32: coreml_pred = ((29, 4, 27, 14, 12, 13, 0, 28, 3), 9.73)
373/33: coreml_pred[0]
373/34: all(coreml_pred[0]==coreml_pred[0])
373/35: coreml_pred[0].all(coreml_pred[0])
373/36: coreml_pred[0]==coreml_pred[0]
374/1: csv_path = "/Users/dustin/CS/consulting/firstlayerai/data/speak_test_data/2020-05-27/speak-test-noise-labels_2020-08-178.csv"
374/2: import csv
374/3:
i=0
with open(csv_path, 'r') as fid:
    reader = csv.reader(fid, delimiter=',')
    for row in reader:
        if i < 5:
            print(row)
374/4:
with open(csv_path, 'r') as fid:
    reader = csv.reader(fid, delimiter=',')
    all_labels = list()
    for row in reader:
        labels = row[0].split(',')
        all_labels.extend(labels)
374/5:
with open(csv_path, 'r') as fid:
    reader = csv.reader(fid, delimiter=',')
    all_labels = list()
    for row in reader:
        labels = row.split(',')
        all_labels.extend(labels)
374/6:
with open(csv_path, 'r') as fid:
    reader = csv.reader(fid, delimiter=',')
    all_labels = list()
    for row in reader:
        if len(row):
            labels = row.split(',')
            all_labels.extend(labels)
374/7:
with open(csv_path, 'r') as fid:
    reader = csv.reader(fid, delimiter=',')
    all_labels = list()
    for row in reader:
        if len(row):
            labels = row[0].split(',')
            all_labels.extend(labels)
374/8: all_labels
374/9:
with open(csv_path, 'r') as fid:
    reader = csv.reader(fid, delimiter=',')
    all_labels = list()
    for row in reader:
        if len(row):
            labels = row[0].split(',')
            labels = [label.strip() for label in labels]
            all_labels.extend(labels)
374/10: all_labels
374/11: all_labels[0] = 'motor'
374/12: all_labels
374/13: import Counter
374/14: from collections import Counter
374/15: counter = Counter(all_labels)
374/16: counter
374/17: counter.most_common()
375/1: quarter = True
375/2: half = True
375/3: not(quarter and half)
375/4: (quarter and half)
375/5: not(quarter and half)
375/6: quarter = False
375/7: not(quarter and half)
375/8: not(quarter and half)
375/9: half=False
375/10: not(quarter and half)
375/11: path = "/Users/dustin/CS/consulting/firstlayerai/data/signal_augment_assess/audio/2020-07-21_normal_dist/org"
375/12: import glob
375/13: patthern = ".wav"
375/14: pattern = ".wav"
375/15: glob.glob(os.path.join(path, pattern))
375/16: import os
375/17: glob.glob(os.path.join(path, pattern))
375/18: pattern = "*.wav"
375/19: glob.glob(os.path.join(path, pattern))
375/20: pattern = "*.w*v"
375/21: len(glob.glob(os.path.join(path, pattern)))
375/22: pattern = "*.wav"
375/23: len(glob.glob(os.path.join(path, pattern)))
377/1: path = "test_single-precision_2020-08-21_186_model.pth"
377/2: path.replace(model, "state_dict)
377/3: path.replace(model, "state_dict")
377/4: path.replace("model", "state_dict")
377/5: os.path.exists(path.replace("model", "state_dict") )
377/6: import os
377/7: os.path.exists(path.replace("model", "state_dict") )
377/8: path = "/Users/dustin/CS/consulting/firstlayerai/phoneme_classification/src/awni_speech/speech/model_convert/torch_models/test_single-precision_2020-08-21_186_model.pth'
377/9: path = "/Users/dustin/CS/consulting/firstlayerai/phoneme_classification/src/awni_speech/speech/model_convert/torch_models/test_single-precision_2020-08-21_186_model.pth"
377/10: os.path.exists(path.replace("model", "state_dict") )
377/11: path.replace("model", "state_dict")
377/12: path.reverse()
377/13: reverse(path)
377/14: [0,1,2,3].reverse()
377/15: l = [0,1,2,3]
377/16: l
377/17: l.reverse()
377/18: l
377/19: path.replace("model.pth", "state_dict.pth")
377/20: os.path.exists(path.replace("model.pth", "state_dict.pth") )
378/1: from collections import OrderedDict
379/1: import torch
380/1: import torch
380/2: model_path ="examples/librispeech/models/ctc_models/20200806/ph5/model.pth"
380/3: model = torch.load(model_path)
381/1: import torch
381/2: device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
381/3: device
382/1: dir_path = "examples/librispeech/models/ctc_models/20200806/ph5/20200828/"
382/2: import os
382/3: paths = os.listdir(os.path.join(dir_path, "*.yaml"))
382/4: import glob
382/5: paths = glob.glob(os.path.join(dir_path, "*.yaml"))
382/6: paths
382/7: paths = glob.glob(os.path.join(dir_path, "ctc_config*[.yaml, .json]"))
382/8: paths
382/9: paths = glob.glob(os.path.join(dir_path, "ctc_config*[.yaml, .json]"))
382/10: paths
383/1: test = ["AA", 'BB', 'CC']
383/2: lower_list = lambda x: list(map(lower, x))
383/3: lower_test = lower_list(test)
383/4: "AA".lower()
383/5: lower_list = lambda x: list(map(lower(), x))
383/6: lower_test = lower_list(test)
383/7: lower_list = lambda x: list(map(lower, x))
383/8: lower_test = lower_list(test)
383/9: lower_list = lambda x: list(map(lower(), x))
383/10: lower_test = lower_list(test)
383/11: lower_list = lambda x: list(map(str.lower(), x))
383/12: lower_test = lower_list(test)
383/13: lower_list = lambda x: list(map(str.lower, x))
383/14: lower_test = lower_list(test)
383/15: lower_test
383/16: lower_list = lambda x: list(map(str.lower, x))
383/17: lower_test = lower_list(test)
383/18: lower_test
384/1: import firebase_admin
384/2:
from firebase_admin import credentials
from firebase_admin import firestore
384/3: cred = credentials.ApplicationDefault()
384/4:
firebase_admin.initialize_app(cred, {
  'projectId': 'speak-v2-2a1f1'

})
384/5: db = firestore.client()
384/6: help(db)
385/1: import firebase_admin
385/2:
from firebase_admin import credentials
from firebase_admin import firestore
385/3: cred = credentials.ApplicationDefault()
385/4:
firebase_admin.initialize_app(cred, {
  'projectId': 'speak-v2-2a1f1'
})
385/5: db = firestore.client()
385/6: rec_ref = db.collection(u'recordings')
385/7:
for i, doc in enumerate(rec_ref.stream()):
    if i < 5:
        print(f'{doc.id} => {doc.to_dict()}')
        count += 1
    else:
        count +=1
385/8:
count = 0
for i, doc in enumerate(rec_ref.stream()):
    if i < 3:
        print(f'{doc.id} => {doc.to_dict()}')
        count += 1
    else:
        count +=1
386/1:
import firebase_admin
from firebase_admin import credentials
from firebase_admin import firestore
386/2: import firebase_admin
387/1: import firebase_admin
388/1: import firebase_admin
388/2: !echo $PYTHONPATH
388/3: import editdistance
389/1: !conda activate awni_env36
389/2: !conda info -e
389/3: import editdistance
390/1: import editdistance
390/2:  import firebase_admin
392/1: path = "/Users/dustin/CS/consulting/firstlayerai/data/speak_train/train_data.tsv"
392/2: import csv
392/3:
    lesson_dict = {}
    line_dict = {}
    user_dict ={}
392/4:
    with open(tsv_path, 'r') as tsv_file:
        tsv_reader = csv.reader(tsv_file, delimiter='\t')
        header = next(tsv_reader)
        print(header)
        for row in tsv_reader:
            lesson_id, line_id, user_id = row[2], row[3], row[4]
            update_key(lesson_dict, lesson_id)
            update_key(line_dict, line_id)
            update_key(user_dict, user_id)
392/5: tsv_path = "/Users/dustin/CS/consulting/firstlayerai/data/speak_train/train_data.tsv"
392/6:
    with open(tsv_path, 'r') as tsv_file:
        tsv_reader = csv.reader(tsv_file, delimiter='\t')
        header = next(tsv_reader)
        print(header)
        for row in tsv_reader:
            lesson_id, line_id, user_id = row[2], row[3], row[4]
            update_key(lesson_dict, lesson_id)
            update_key(line_dict, line_id)
            update_key(user_dict, user_id)
392/7:
    def _update_key(in_dict, key):
        in_dict[key] = in_dict.get(key, 0)
392/8:
    with open(tsv_path, 'r') as tsv_file:
        tsv_reader = csv.reader(tsv_file, delimiter='\t')
        header = next(tsv_reader)
        print(header)
        for row in tsv_reader:
            lesson_id, line_id, user_id = row[2], row[3], row[4]
            _update_key(lesson_dict, lesson_id)
            _update_key(line_dict, line_id)
            _update_key(user_dict, user_id)
392/9: import matplotlib.pyplot as plt
393/1: import csv
393/2: import matplotlib.pyplot as plt
393/3: tsv_path = "/Users/dustin/CS/consulting/firstlayerai/data/speak_train/train_data.tsv"
393/4:
    def _update_key(in_dict, key):
        in_dict[key] = in_dict.get(key, 0)
393/5:
    lesson_dict = {}
    line_dict = {}
    user_dict ={}
393/6:

    with open(tsv_path, 'r') as tsv_file:
        tsv_reader = csv.reader(tsv_file, delimiter='\t')
        header = next(tsv_reader)
        print(header)
        for row in tsv_reader:
            lesson_id, line_id, user_id = row[2], row[3], row[4]
            _update_key(lesson_dict, lesson_id)
            _update_key(line_dict, line_id)
            _update_key(user_dict, user_id)
393/7: plt.bar(lesson_dict.keys(), lesson_dict.values(), color='g')
393/8: len(lesson_dict)
393/9: len(line_dict)
393/10: len(user_dict)
393/11: plt.bar(lesson_dict.values(), color='g')
393/12: plt.plot(lesson_dict.values(), color='g')
393/13: plt.plot(range(len(lesson_dict.values())), lesson_dict.values(), color='g')
393/14: len(lesson_dict.values())
393/15: lesson_dict.values()
393/16:
    def _update_key(in_dict, key):
        in_dict[key] = in_dict.get(key, 0) + 1
393/17:
    with open(tsv_path, 'r') as tsv_file:
        tsv_reader = csv.reader(tsv_file, delimiter='\t')
        header = next(tsv_reader)
        print(header)
        for row in tsv_reader:
            lesson_id, line_id, user_id = row[2], row[3], row[4]
            _update_key(lesson_dict, lesson_id)
            _update_key(line_dict, line_id)
            _update_key(user_dict, user_id)
393/18: lesson_dict.values()
393/19: plt.plot(range(len(lesson_dict.values())), lesson_dict.values(), color='g')
393/20: plt.plot(range(len(lesson_dict.values())), list(lesson_dict.values()), color='g')
393/21: plt.plot(range(len(lesson_dict.values())), list(lesson_dict.values()).sort(), color='g')
393/22: plt.plot(range(len(lesson_dict.values())), sorted(list(lesson_dict.values())), color='g')
393/23: plt.plot(range(len(lesson_dict.values())), sorted(list(lesson_dict.values())), color='g')
393/24: plt.plot(range(len(lesson_dict.values())), sorted(list(lesson_dict.values()), reverse=True), color='g')
393/25: plt.plot(range(len(lesson_dict.values())), sorted(list(lesson_dict.values()), reverse=True), color='g')
393/26:
    def plot_count(count_dict:dict):
        plt.plot(range(len(count_dict.values())), sorted(list(count_dict.values()), reverse=True))
393/27: plot_count(user_dict)
393/28: plot_counter(line_dict)
393/29: plot_count(line_dict)
393/30: plot_count(line_dict)
393/31:
    def plot_count(count_dict:dict):
        plt.plot(range(len(count_dict.values())), sorted(list(count_dict.values()), reverse=True))
        plt.show()
393/32: plot_count(line_dict)
393/33: plot_count(line_dict)
393/34:
    def plot_count(count_dict:dict):
        plt.plot(range(len(count_dict.values())), sorted(list(count_dict.values()), reverse=True))
393/35:
    def stats(count_dict:dict):
        mean = round(mean(count_dict.values()), 2)
        std = round(np.std(count_dict.values()), 2)
        max_val = round(max(count_dict.values()), 2)
        min_val = round(min(count_dict.values()), 2)

        print(f"mean: {mean}, std: {std}, max: {max_val}, min: {min_val}")
393/36: stats(lesson_dict)
393/37: import numpy as np
393/38:
    def stats(count_dict:dict):
        mean = round(np.mean(count_dict.values()), 2)
        std = round(np.std(count_dict.values()), 2)
        max_val = round(max(count_dict.values()), 2)
        min_val = round(min(count_dict.values()), 2)
393/39: stats(lesson_dict)
393/40: np.mean(lesson_dict.values())
393/41: np.mean(list(lesson_dict.values()))
393/42:
   
    def stats(count_dict:dict):
        values = list(count_dict.values())
        mean = round(np.mean(values), 2)
        std = round(np.std(values), 2)
        max_val = round(max(values), 2)
        min_val = round(min(values), 2)
393/43: stats(lesson_dict)
393/44:
    def stats(count_dict:dict):
        values = list(count_dict.values())
        mean = round(np.mean(values), 2)
        std = round(np.std(values), 2)
        max_val = round(max(values), 2)
        min_val = round(min(values), 2)
        print(f"mean: {mean}, std: {std}, max: {max_val}, min: {min_val}")
393/45: stats(lesson_dict)
393/46: stats(user_dict)
393/47: stats(lin_dict)
393/48: stats(line_dict)
394/1: l_l = [[1,2,3], [4,5,6], [7,8,9]]
395/1: labels = ["lessons", "lines", "speakers"]
395/2: zip(labels, labels)
395/3: list(zip(labels, labels))
396/1: len(set([1,1,2,3,3]))
396/2: exite
397/1: from itertools import repeat
397/2: list(map(pow, range(10), repeat(2)))
397/3: list(map(pow, range(10), repeat(3)))
398/1: path = "/Users/dustin/Desktop/2020-09-25-per.txt"
398/2: import csv
398/3: import numpy a snp
398/4: import numpy as np
398/5:
with open(path, 'r') as fid:
    reader = csv.reader(fid, delimiter=',')
    per_list = [row[1] for row in reader]
398/6: len(per_list)
398/7: np.histogram(per_list, bins=5, range=(0.0, 1.0))
398/8: np.histogram(np.array(per_list), bins=5, range=(0.0, 1.0))
398/9: hist = np.histogram(np.array(per_list), bins=5, range=(0.0, 1.0))
398/10: per_list[:5]
398/11: per_list = [float(elem) for elem in per_list]
398/12: per_list[:5]
398/13: hist = np.histogram(per_list, bins=5, range=(0.0, 1.0))
398/14: hist
398/15: import matplotib as plt
398/16: import matplotlib as plt
398/17: plt.hist(per_list)
398/18: import matplotlib.pyplot as plt
398/19: plt.hist(per_list)
398/20: plt.hist(per_list, bins=5, range=(0.0, 1.0))
398/21: fig.title("Histogram of PER values")
398/22: plt.title("Histogram of PER values")
398/23: plt.ylabel("count")
398/24: plt.xlabel("PER bins")
398/25: plt.ylabel("# of examples")
398/26: tar_path = "/Users/dustin/Downloads/Speak-test_Review_2020-05-28 - Sheet6.tsv"
398/27: with open(tar_path, 'r') as fid
398/28:
with open(path, 'r') as per_id:
    with open(tar_path, 'r') as tar_fid:
        per_reader = csv.reader(per_id, delimiter=',')
        tar_reader = csv.reader(fid, delimiter='\t')
        tar_dict = {row[0]: (row[1], row[2]) for row in tar_reader}
        per_dict ={row[0]: row[1] for row in per_reader}
398/29:
with open(path, 'r') as per_id:
    with open(tar_path, 'r') as tar_fid:
        per_reader = csv.reader(per_id, delimiter=',')
        tar_reader = csv.reader(tar_fid, delimiter='\t')
        tar_dict = {row[0]: (row[1], row[2]) for row in tar_reader}
        per_dict ={row[0]: row[1] for row in per_reader}
398/30: tar_gues_list = [per for uid, per in per_reader.items() if tar_reader[uid][1] ]
398/31: tar_gues_list = [float(per) for uid, per in per_dict.items() if float(tar_reader[uid][1]) ]
398/32: tar_gues_list = [float(per) for uid, per in per_dict.items() if float(tar_dict[uid][1]) ]
398/33: len(tar_gues_list)
398/34: len(per_dict)
398/35: len(tar_dict)
398/36: list(9tar_dict.values())[:5]
398/37: list(tar_dict.values())[:5]
398/38: tar_gues_list = [float(per) for uid, per in per_dict.items() if float(tar_dict[uid][0]) ]
398/39: len(tar_gues_list)
398/40: tar_gues_list[:5]
398/41: plt.hist(per_list, bins=5, range=(0.0, 1.0))
398/42: plt.hist(tar_gues_list, bins=5, range=(0.0, 1.0), color='o')
398/43: plt.hist(tar_gues_list, bins=5, range=(0.0, 1.0))
398/44: plt.legend(['total', 'tar=guess'])
398/45: plt.legend(['total', 'target=guess'])
398/46: plt.title(Histogram of PER values)
398/47: plt.title("Histogram of PER values")
398/48: plt.ylabel("# of examples")
398/49: plt.xlabel("PER bins")
398/50: per_list
398/51: tar_gues_list
398/52: old_path = "/Users/dustin/Desktop/2020-08-06.txt"
398/53:
with open(old_path, 'r') as fid:
    reader = csv.reader(fid, delimiter=',')
    old_per_list = [row[1] for row in reader]
398/54: plt.hist(old_per_list, bins=5, range=(0.0, 1.0))
398/55: old_per_list[0]
398/56: old_per_list = [float(elem) for elem in old_per_list]
398/57: plt.hist(old_per_list, bins=5, range=(0.0, 1.0))
398/58: plt.ylabels("# of records")
398/59: plt.ylabel("# of records")
398/60: plt.title("histogram of 2020-08-28 model PER values")
398/61: plt.xlabel("PER bins")
398/62: plt.hist(old_per_list, bins=10, range=(0.0, 1.0))
398/63: plt.xlabel("PER bins")
398/64: plt.title("histogram of 2020-08-28 model PER values")
398/65: plt.ylabel("# of records")
398/66: plt.hist(per_list, bins=10, range=(0.0, 1.0))
398/67: plt.hist(tar_gues_list, bins=10, range=(0.0, 1.0))
398/68: plt.ylabel("# of records")
398/69: plt.title("histogram of 2020-09-25 model PER values")
398/70: plt.xlabel("PER bins")
398/71: plt.legend(['total', 'target=guess'])
399/1: path = "/Users/dustin/Downloads/model_state_dict_ckpt.pth"
399/2: import torch
399/3: model = torch.load(path, map_location='cpu')
400/1: l = list()
400/2: f = l.appen
400/3: f = l.append
400/4: f(1)
400/5: l
400/6: f(2)
400/7: l
400/8:
class Timer():
    def __init__(self):
        self.audio_buffer_time = 0.0
        self.audio_buffer_count = 0
        
    def update(self, attr_name, time_interval):
        attr_time = attr_name + "_time"
        setattr(self, attr_time, time_interval)
400/9: time = Timer()
400/10: time.audio_buffer_tiome
400/11: time.audio_buffer_time
400/12: time.update(audio_buffer, 5.0)
400/13: time.update('audio_buffer', 5.0)
400/14: time.audio_buffer_time
400/15:
class Timer():
    def __init__(self):
        self.audio_buffer_time = 0.0
        self.audio_buffer_count = 0
        
    def update(self, attr_name, time_interval):
        attr_time = attr_name + "_time"
        old_time = getattr(self, attr_time)
        new_time = old_time + time_interval
        setattr(self, attr_time, new_time)
400/16: time.update('audio_buffer', 5.0)
400/17: time.audio_buffer_time
400/18: time.update('audio_buffer', 5.0)
400/19: time.audio_buffer_time
400/20: time = Timer()
400/21: time.update('audio_buffer', 5.0)
400/22: time.audio_buffer_time
400/23: time.update('audio_buffer', 5.0)
400/24: time.audio_buffer_time
400/25:
class Timer():
    def __init__(self):
        self.audio_buffer_time = 0.0
        self.audio_buffer_count = 0
        
    def update(self, attr_name, time_interval):
        attr_time = attr_name + "_time"
        old_time = getattr(self, attr_time)
        new_time = old_time + time_interval
        setattr(self, attr_time, new_time)
400/26: time = Timer()
400/27:
class Timer():
    def __init__(self):
        self.audio_buffer_time = 0.0
        self.audio_buffer_count = 0
        
    def update(self, attr_name, time_interval):
        attr_time = attr_name + "_time"
        old_time = getattr(self, attr_time)
        new_time = old_time + time_interval
        setattr(self, attr_time, new_time)
        # increase the count as well
        attr_count = attr_name + "_count"
        old_count = getattr(self, attr_count)
        new_count = old_count + 1
        setattr(self, attr_count, new_count)
400/28: time = Timer()
400/29: time.update('audio_buffer', 5.0)
400/30: time.audio_buffer_time
400/31: time.audio_buffer_count
400/32: time.update('audio_buffer', 5.0)
400/33: time.audio_buffer_time
400/34: time.audio_buffer_count
400/35:
class Timer():
    def __init__(self):
        self.audio_buffer_time = 0.0
        self.audio_buffer_count = 0
        
    def update(self, attr_name, time_interval):
        attr_time = attr_name + "_time"
        old_time = getattr(self, attr_time)
        new_time = old_time + time_interval
        setattr(self, attr_time, new_time)
        # increase the count as well
        attr_count = attr_name + "_count"
        old_count = getattr(self, attr_count)
        new_count = old_count + 1
        setattr(self, attr_count, new_count)
400/36:
class Timer():
    def __init__(self):
        self.audio_buffer_time = 0.0
        self.audio_buffer_count = 0
        
    def update(self, attr_name, time_interval):
        attr_time = attr_name + "_time"
        old_time = getattr(self, attr_time)
        new_time = old_time + time_interval
        setattr(self, attr_time, new_time)
        # increase the count as well
        attr_count = attr_name + "_count"
        old_count = getattr(self, attr_count)
        new_count = old_count + 1
        setattr(self, attr_count, new_count)
    def create_attribute(self, attr_name):
        pass
        
    def print_attributes(self):
        print(f"attributes: {self.__dict__()}")
400/37: timer = Timer()
400/38: setttr(timer, "test_attr", 0)
400/39: setattr(timer, "test_attr", 0)
400/40: timer.test_attr
400/41: timer.print_attributes
400/42: timer.print_attributes()
400/43:
class Timer():
    def __init__(self, attr_names):
        def _set_time_count(attr_name):
            setattr(self, attr_name+"_count", 0) 
            setattr(self, attr_name+"_time", 0.0) 
        
        if isinstance(attr_names, list):
           for attr_name in attr_names:
                _set_time_count(attr_name)
        elif isinstance(attr_names, str):
             _set_time_count(attr_names)
        else:
            raise ValueError(f"attr_names must be of list or str type, not: {type(attr_names)} type") 
            
    def update(self, attr_name, time_interval):
        # update the time value
        attr_time = attr_name + "_time"
        old_time = getattr(self, attr_time)
        new_time = old_time + time_interval
        setattr(self, attr_time, new_time)
        # increase the count as well
        attr_count = attr_name + "_count"
        old_count = getattr(self, attr_count)
        new_count = old_count + 1
        setattr(self, attr_count, new_count)
        
    def print_attributes(self):
        print(f"attributes: {self.__dict__}")
400/44: timer = Timer()
400/45: timer = Timer('audio_buffer')
400/46: timer.print_attributes()
400/47: import numpy as np
400/48: a = np.empty(1, PARAMS['chunk_size'], PARAMS['feature_size'])
400/49: a = np.empty(1, PARAMS['chunk_size'], 257)
400/50: a = np.empty(1, 46, 257)
400/51: a = np.empty((1, 46, 257))
400/52: a.shape
400/53: a = np.concatenate((a,a), axis=0)
400/54: a
400/55: a.shape
400/56: len(a)
400/57:
for row in a:
    print(row.shape)
400/58: a[0, 16:, :].shape
400/59: a[0, :-16, :].shape
400/60: a[0, -16:, :].shape
400/61: import torch
400/62: torch.float32
400/63: split("conv.5.weight")
400/64: os.path.split("conv.5.weight")
400/65: import os
400/66: os.path.split("conv.5.weight")
400/67: str.split("conv.5.weight")
400/68: "conv.5.weight".split('.')
400/69: help(str.split)
401/1: a = np.arange(100)
401/2: import numpy as np
401/3: a = np.arange(100)
401/4: a.reshape(10,10)
401/5: a = a.reshape(10,10)
401/6: a
401/7: a.shape
401/8: b = a.view(20, 5)
401/9: b = a.view((20, 5))
401/10: b = a.view()
401/11: b = b.reshape(20,5)
401/12: b.shape
401/13: a.shape
401/14: a.data
401/15: b.data
401/16: c = a.view()
401/17: c.data
401/18: a.data
401/19: a.data
401/20: pointer, read_only_flag = a.__array_interface__['data']
401/21: pointer
401/22: pointerb, read_only_flag = b.__array_interface__['data']
401/23: pointerb
401/24: pointer
401/25: a.shape
401/26: del b
401/27: b
401/28: b = np.conatenate((b, a), axis=0)
401/29: b = np.concatenate((b, a), axis=0)
401/30: ls
401/31: lc_model_inputs = np.load("test_data/lc_input_2020-09-29_test.npy")
401/32: st_model_inputs = np.load("test_data/st_input_2020-09-29_test.npy")
401/33: lc_model_inputs.shape
401/34: st_model_inputs.shape
401/35: lc_inp = lc_model_inputs
401/36: st_inp = st_model_inputs
401/37: lc_inp == st_inp
401/38: np.sum(lc_inp == st_inp)
401/39: lc_inp.shape
401/40: np.sum(lc[0,:])
401/41: np.sum(lc_inp[0,:])
401/42: np.sum(lc_inp[1,:])
401/43: np.sum(lc_inp[1,:])
401/44: np.sum(lc_inp[0:15,:])
401/45: np.sum(lc_inp[0:16,:])
401/46: np.sum(st_inp[0:15,:])
401/47: np.sum(st_inp[0:16,:])
401/48: np.sum(st_inp[15,:])
401/49: np.sum(st_inp[:15,:])
401/50: np.sum(st_inp[15,:])
401/51: np.sum(lc_inp[15,:])
401/52: np.sum(lc_inp[16,:])
401/53: np.sum(st_inp[16,:])
401/54: import collections
401/55: audio_buffer_size = 2
401/56: audio_ring_buffer = collections.deque(maxlen=audio_buffer_size)
401/57: audio_ring_buffer
401/58: zeros = [0,0,0]
401/59: ones = [1,1,1]
401/60: audio_ring_buffer.append(zeros)
401/61: audio_ring_buffer
401/62: audio_ring_buffer.append(ones)
401/63: audio_ring_buffer
401/64: audio_ring_buffer[0]
401/65: list(audio_ring_buffer)[0]
401/66: list(audio_ring_buffer)[1]
401/67: audio_ring_buffer[1]
401/68: audio_ring_buffer.append(zeros)
401/69: audio_ring_buffer
401/70: np.concatenate(list(audio_ring_buffer))
401/71: a = [*range(46)]
401/72: a[-16:]
401/73: len(a[-16:])
402/1: import torch
402/2: model = torch.load("examples/librispeech/models/ctc_models/20201029/ph7/ckpt_model_state_dict.pth")
402/3: model = torch.load("examples/librispeech/models/ctc_models/20201029/ph7/ckpt_model_state_dict.pth", map_location='cpu')
403/1: import torch
404/1: import torch
406/1: import torch
407/1: import torch
407/2: model = torch.load("examples/librispeech/models/ctc_models/20201029/ph7/ckpt_model_state_dict.pth")
407/3: model = torch.load("examples/librispeech/models/ctc_models/20201029/ph7/ckpt_model_state_dict.pth", map_location='cpu')
407/4: type(model)
408/1: import torch
408/2: torch.__version__
408/3: model = torch.load("examples/librispeech/models/ctc_models/20201029/ph7/best_model_state_dict.pth")
409/1: cd ..
409/2: from speech.utils.io import read_pickle
409/3: preproc = read_pickle("examples/librispeech/models/ctc_models/20201029/ph7/best_preproc.pyc")
409/4: import numpy as np
409/5: preproc = np.load("examples/librispeech/models/ctc_models/20201029/ph7/best_preproc.pyc")
409/6: preproc = np.load("examples/librispeech/models/ctc_models/20201029/ph7/best_preproc.pyc", allow_pickle=True)
409/7: preproc
409/8: print(preroc)
409/9: print(preproc)_
409/10: print(preproc)
411/1: ls
411/2: from speech.models.ctc_model import CTC as CTC_model
411/3: cd model_convert/
411/4: from get_paths import pytorch_onnx_paths
411/5: model_name="2020-11-03_large-model-naren-loss-no-feat-norm_31f_model"
411/6: torch_path, config_path, onnx_path = pytorch_onnx_paths(model_name)
411/7: model_name="2020-11-03_large-model-naren-loss-no-feat-norm_31f"
411/8: torch_path, config_path, onnx_path = pytorch_onnx_paths(model_name)
411/9:
# standard libraries
import argparse
from collections import OrderedDict
import json
import os
# third-party libraries
import onnx
import torch
# project libraries
from get_paths import pytorch_onnx_paths
from get_test_input import generate_test_input
from import_export import torch_load, torch_onnx_export
import speech.loader as loader
from speech.models.ctc_model import CTC as CTC_model
from speech.utils.io import load_config, load_state_dict
411/10:
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        
    config = load_config(config_path)
    model_cfg = config['model']

    freq_dim = 257  #freq dimension out of log_spectrogram 
    vocab_size = 39
411/11: torch_model = CTC_model(freq_dim, vocab_size, model_cfg)
411/12:     state_dict = load_state_dict(torch_path, device=device)
411/13:     torch_model.load_state_dict(state_dict)
411/14:     torch_model.eval()
411/15:     hidden_size = config['model']['encoder']['rnn']['dim']
411/16:     input_tensor = generate_test_input("pytorch", model_name, time_dim, hidden_size)
411/17: time_dim = 31
411/18:     input_tensor = generate_test_input("pytorch", model_name, time_dim, hidden_size)
411/19: x, (h_in, c_in) = input_tensor
411/20: x.shape
411/21: out = model(x, (h, c))
411/22: out = torch_model(x, (h, c))
411/23: out = torch_model(x, (h_in, c_in))
411/24: out.shape
411/25: traced_model = torch.jit.trace(torch_model, (x, (h_in, c_in)))
411/26: print(traced_model)
411/27:     torch_model.eval()
411/28: traced_model = torch.jit.trace(torch_model, (x, (h_in, c_in)))
411/29: print(traced_model)
411/30: traced_out = traced_model(input_tensor)
411/31: traced_out = traced_model(x, (h_in, c_in))
411/32: traced_out, (traced_h, traced_c) = traced_out
411/33: traced_out.shape
411/34: traced_h.shape
411/35: traced_out
411/36: torch_output = torch_model(x, (h_in, c_in))
411/37: torch_out, (torch_h, torch_c) = torch_output
411/38: torch_out
411/39: torch_out == traced_out
411/40: all(torch_out == traced_out)
411/41: all(torch.squeeze(torch_out == traced_out))
411/42: input_tensor = generate_test_input("pytorch", model_name, 46, hidden_size)
411/43: x_46, (h_46, c_46) = input_tensdort
411/44: x_46, (h_46, c_46) = input_tensor
411/45: out_46, (h_out_46, c_out_46) = torch_model(x_46, (h_46, c_46))
411/46: out_46.shape
411/47: traced_out_46, (traced_h_out_46, traced_c_out_46) = traced_model(x_46, (h_46, c_46))
411/48: x_46.shape
411/49: traced_out_46, (traced_h_out_46, traced_c_out_46) = traced_model(x, (h_in, c_in))
411/50: traced_model.graph
411/51: traced_model.code
411/52: print(traced_model.code)
411/53: import torch
411/54: a = torch.arange(200).reshape(2,10,10)
411/55: b = a.view(2, 100)
411/56: b.shape
411/57: a.shape
411/58: a.shape[0]
411/59: c = a.view(:, -1)
411/60: c = a.view(2, -1)
411/61: c.shape
411/62: d = a.view(-1)
411/63: d.shape
411/64: e = a.view(::, -1)
411/65: e = a.view(..., -1)
411/66: a.size
411/67: a.size()
411/68: a.size()[0]
411/69: e = a.view(a.size()[0], -1)
411/70: e.shape
411/71: torch_model = CTC_model(freq_dim, vocab_size, model_cfg)
411/72: traced_model = torch.jit.trace(torch_model, (x, (h_in, c_in)))
411/73: torch_model = CTC_model(freq_dim, vocab_size, model_cfg)
411/74: traced_model = torch.jit.trace(torch_model, (x, (h_in, c_in)))
411/75: traced_model = torch.jit.trace(torch_model, (x, (h_in, c_in)))
411/76: import importlib
411/77: from speech.models import ctc_model
411/78: torch_model = ctc_model.CTC(freq_dim, vocab_size, model_cfg)
411/79: traced_model = torch.jit.trace(torch_model, (x, (h_in, c_in)))
411/80: input_tensor = generate_test_input("pytorch", model_name, 31, hidden_size)
411/81: (x, (h_in, c_in)) = generate_test_input("pytorch", model_name, 31, hidden_size)
411/82: traced_model = torch.jit.trace(torch_model, (x, (h_in, c_in)))
412/1: cd model_convert/
412/2:
import argparse
from collections import OrderedDict
import json
import os
# third-party libraries
import onnx
import torch
# project libraries
from get_paths import pytorch_onnx_paths
from get_test_input import generate_test_input
from import_export import torch_load, torch_onnx_export
import speech.loader as loader
from speech.models.ctc_model import CTC as CTC_model
from speech.utils.io import load_config, load_state_dict
412/3: model_name = "2020-11-03_large-model-naren-loss-no-feat-norm_31f"
412/4: torch_path, config_path, onnx_path = pytorch_onnx_paths(model_name)
412/5:
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        
    config = load_config(config_path)
    model_cfg = config['model']

    freq_dim = 257  #freq dimension out of log_spectrogram 
    vocab_size = 39
    time_dim = num_frames
    
    torch_model = CTC_model(freq_dim, vocab_size, model_cfg)
412/6:
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        
    config = load_config(config_path)
    model_cfg = config['model']

    freq_dim = 257  #freq dimension out of log_spectrogram 
    vocab_size = 39
    time_dim = num_frames
    
    torch_model = CTC_model(freq_dim, vocab_size, model_cfg)
412/7: num_frames = 31
412/8:
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        
    config = load_config(config_path)
    model_cfg = config['model']

    freq_dim = 257  #freq dimension out of log_spectrogram 
    vocab_size = 39
    time_dim = num_frames
    
    torch_model = CTC_model(freq_dim, vocab_size, model_cfg)
412/9:     torch_model.eval()
412/10:
    hidden_size = config['model']['encoder']['rnn']['dim'] 
    input_tensor = generate_test_input("pytorch", model_name, time_dim, hidden_size)
412/11: x, (h_in, c_in) = input_tensor
412/12: traced_model = torch.jit.trace(torch_model, (x, (h_in, c_in)))
412/13: x_46, (h_46, c_46) = generate_test_input("pytorch", model_name, 46, hidden_size)
412/14: out_31, (h_out_31, c_out_31) = traced_model(x, (h_in, c_in))
412/15: out_46, (h_out_46, c_out_46) = traced_model(x_46, (h_46, c_46))
412/16: import importlib
412/17: from speech.utils import ctc_model
412/18: from speech.models import ctc_model
412/19: torch_model = ctc_model.CTC(freq_dim, vocab_size, model_cfg)
412/20: del traced_model
412/21: traced_model = torch.jit.trace(torch_model, (x, (h_in, c_in)))
412/22: importlib.reload(ctc_model)
412/23: torch_model = ctc_model.CTC(freq_dim, vocab_size, model_cfg)
412/24: traced_model = torch.jit.trace(torch_model, (x, (h_in, c_in)))
413/1:
# standard libraries
import argparse
from collections import OrderedDict
import json
import os
# third-party libraries
import onnx
import torch
# project libraries
from get_paths import pytorch_onnx_paths
from get_test_input import generate_test_input
from import_export import torch_load, torch_onnx_export
import speech.loader as loader
from speech.models.ctc_model import CTC as CTC_model
from speech.utils.io import load_config, load_state_dict
413/2: model_name = "2020-11-03_large-model-naren-loss-no-feat-norm_31f"
413/3:
    torch_path, config_path, onnx_path = pytorch_onnx_paths(model_name)

    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        
    config = load_config(config_path)
    model_cfg = config['model']

    freq_dim = 257  #freq dimension out of log_spectrogram 
    vocab_size = 39
    
    torch_model = CTC_model(freq_dim, vocab_size, model_cfg) 

    state_dict = load_state_dict(torch_path, device=device)
413/4:
# standard libraries
import argparse
from collections import OrderedDict
import json
import os
# third-party libraries
import onnx
import torch
# project libraries
from get_paths import pytorch_onnx_paths
from get_test_input import generate_test_input
from import_export import torch_load, torch_onnx_export
import speech.loader as loader
from speech.models.ctc_model import CTC as CTC_model
from speech.utils.io import load_config, load_state_dict
413/5: cd model_convert/
413/6:
# standard libraries
import argparse
from collections import OrderedDict
import json
import os
# third-party libraries
import onnx
import torch
# project libraries
from get_paths import pytorch_onnx_paths
from get_test_input import generate_test_input
from import_export import torch_load, torch_onnx_export
import speech.loader as loader
from speech.models.ctc_model import CTC as CTC_model
from speech.utils.io import load_config, load_state_dict
413/7:
    torch_path, config_path, onnx_path = pytorch_onnx_paths(model_name)

    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        
    config = load_config(config_path)
    model_cfg = config['model']

    freq_dim = 257  #freq dimension out of log_spectrogram 
    vocab_size = 39
    
    torch_model = CTC_model(freq_dim, vocab_size, model_cfg)
413/8:     torch_model.eval()
413/9:
    # create the tracking inputs
    hidden_size = config['model']['encoder']['rnn']['dim'] 
    x, (h_in, c_in) = generate_test_input("pytorch", model_name, 31, hidden_size) 

    traced_model = torch.jit.trace(torch_model, (x, (h_in, c_in)))
413/10:
    x_46, (h_46, c_46) = generate_test_input("pytorch", model_name, 46, hidden_size)

    out_46, (h_out_46, c_out_46) = traced_model(x_46, (h_46, c_46))
415/1:
import firebase_admin
from firebase_admin import credentials
from firebase_admin import firestore
415/2:
    PROJECT_ID = 'speak-v2-2a1f1'
    QUERY_LIMIT = 10000
    NUM_PROC = 100
415/3: CREDENTIAL_PATH = "~/CS/consulting/firstlayerai/phoneme_classification/src/awni_speech/speak-v2-2a1f1-d8fc553a3437.json"
415/4:     assert os.path.exists(CREDENTIAL_PATH), "Credential file does not exist or is in the wrong location."
415/5: import os
415/6:     assert os.path.exists(CREDENTIAL_PATH), "Credential file does not exist or is in the wrong location."
415/7: CREDENTIAL_PATH = "/Users/dustin/CS/consulting/firstlayerai/phoneme_classification/src/awni_speech/speak-v2-2a1f1-d8fc553a3437.json"
415/8:     assert os.path.exists(CREDENTIAL_PATH), "Credential file does not exist or is in the wrong location."
415/9:     assert os.path.exists(CREDENTIAL_PATH), "Credential file does not exist or is in the wrong location."
415/10:     os.putenv("GOOGLE_APPLICATION_CREDENTIALS", CREDENTIAL_PATH)
415/11:
    cred = credentials.ApplicationDefault()
    firebase_admin.initialize_app(cred, {'projectId': PROJECT_ID})
    db = firestore.client()
415/12: CREDENTIAL_PATH = "/Users/dustin/CS/consulting/firstlayerai/phoneme_classification/src/awni_speech/speak-v2-2a1f1-d8fc553a3437.json"
415/13: assert os.path.exists(CREDENTIAL_PATH), "Credential file does not exist or is in the wrong location."
415/14:
    cred = credentials.ApplicationDefault()
    firebase_admin.initialize_app(cred, {'projectId': PROJECT_ID})
    db = firestore.client()
415/15: del cred
415/16: del db
415/17:
    cred = credentials.ApplicationDefault()
    firebase_admin.initialize_app(cred, {'projectId': PROJECT_ID})
    db = firestore.client()
415/18: db
415/19:     firebase_admin.initialize_app(cred, {'projectId': PROJECT_ID})
415/20: db = firestore.client()
415/21: !echo $CREDENTIAL_PATH
415/22: CREDENTIAL_PATH = "/Users/dustin/CS/consulting/firstlayerai/phoneme_classification/src/awni_speech/speak-v2-2a1f1-d8fc553a3437.json"
415/23:     assert os.path.exists(CREDENTIAL_PATH), "Credential file does not exist or is in the wrong location."
416/1:
    PROJECT_ID = 'speak-v2-2a1f1'
    QUERY_LIMIT = 10000
416/2: CREDENTIAL_PATH = "/Users/dustin/CS/consulting/firstlayerai/phoneme_classification/src/awni_speech/speak-v2-2a1f1-d8fc553a3437.json"
416/3:     assert os.path.exists(CREDENTIAL_PATH), "Credential file does not exist or is in the wrong location."
416/4: import os
416/5:     assert os.path.exists(CREDENTIAL_PATH), "Credential file does not exist or is in the wrong location."
416/6:     os.putenv("GOOGLE_APPLICATION_CREDENTIALS", CREDENTIAL_PATH)
416/7: !echo $GOOGLE_APPLICATION_CREDENTIALS
417/1:
    PROJECT_ID = 'speak-v2-2a1f1'
    QUERY_LIMIT = 10000
    NUM_PROC = 100
417/2:     CREDENTIAL_PATH = "/home/dzubke/awni_speech/speak-v2-2a1f1-d8fc553a3437.json"
417/3:     CREDENTIAL_PATH = "/home/dzubke/awni_speech/speak-v2-2a1f1-d8fc553a3437.json"
417/4:     os.putenv("GOOGLE_APPLICATION_CREDENTIALS", CREDENTIAL_PATH)
417/5: import os
417/6:     os.putenv("GOOGLE_APPLICATION_CREDENTIALS", CREDENTIAL_PATH)
417/7:
    cred = credentials.ApplicationDefault()
    firebase_admin.initialize_app(cred, {'projectId': PROJECT_ID})
    db = firestore.client()
417/8:
import firebase_admin
from firebase_admin import credentials
from firebase_admin import firestore
417/9:
    cred = credentials.ApplicationDefault()
    firebase_admin.initialize_app(cred, {'projectId': PROJECT_ID})
    db = firestore.client()
418/1:
import firebase_admin
from firebase_admin import credentials
from firebase_admin import firestore
419/1:
import firebase_admin
from firebase_admin import credentials
from firebase_admin import firestore
421/1:
import firebase_admin
from firebase_admin import credentials
from firebase_admin import firestore
422/1: !conda info -e
423/1: !conda info -e
424/1: import firebase_admin
424/2:
    PROJECT_ID = 'speak-v2-2a1f1'
    QUERY_LIMIT = 10000
    NUM_PROC = 100
424/3: CREDENTIAL_PATH = "/Users/dustin/CS/consulting/firstlayerai/phoneme_classification/src/awni_speech/speak-v2-2a1f1-d8fc553a3437.json"
424/4:     os.putenv("GOOGLE_APPLICATION_CREDENTIALS", CREDENTIAL_PATH)
424/5: import os
424/6:     os.putenv("GOOGLE_APPLICATION_CREDENTIALS", CREDENTIAL_PATH)
424/7:
    cred = credentials.ApplicationDefault()
    firebase_admin.initialize_app(cred, {'projectId': PROJECT_ID})
    db = firestore.client()
424/8:
import firebase_admin
from firebase_admin import credentials
from firebase_admin import firestore
424/9:
    cred = credentials.ApplicationDefault()
    firebase_admin.initialize_app(cred, {'projectId': PROJECT_ID})
    db = firestore.client()
424/10:     rec_ref = db.collection(u'recordings')
424/11: QUERY_LIMIT
424/12: next_query = doc_ref.limit(QUERY_LIMIT)
424/13:     next_query = rec_ref.limit(QUERY_LIMIT)
424/14: docs = list(map(lambda x: x.to_dict(), next_query.stream()))
424/15: docs[0]['info']['date']
424/16: docs[0]['info']['date'].starts_with('2020')
424/17: docs[0]['info']['date'].startswith('2020')
424/18: from collections import Counter
424/19: c = Counter()
424/20: c.add('hello')
424/21: c.__dir__()
424/22: c += 'hello'
424/23: c += ['hello']
424/24: c.__add__('hello')
424/25: c['hello'] += 1
424/26: c
424/27: c['hello'] += 1
424/28: c
424/29: docs[0]
424/30:     iphone_model_count = Counter()
424/31:
    for doc in docs:
        # only select dates in 2020
        date = doc['info']['date']
        if date.startswith('2020'):
            iphone_model = doc['user']['deviceModelIdentifier']
            iphone_model_count['iphone_model'] += 1
424/32: date = docs[0]['info']['date']
424/33: type(date)
424/34: date
424/35: date.startswith('2020')
424/36:
   for doc in docs:
        # only select dates in 2020
        rec_date = doc['info']['date']
        if rec_date.startswith('2020'):
            iphone_model = doc['user']['deviceModelIdentifier']
            iphone_model_count['iphone_model'] += 1
424/37: len(docs)
424/38:
for i in range(4):
    doc = docs[i]
    rec_date = doc['info']['date']
    print(type(rec_date))
    print(rec_date)
424/39:     iphone_model_count = Counter()
424/40:
    for doc in docs:
        # only select dates in 2020
        rec_date = doc['info']['date']
        if isinstance(rec_date, str):
            if rec_date.startswith('2020'):
                iphone_model = doc['user']['deviceModelIdentifier']
                iphone_model_count['iphone_model'] += 1
424/41:
    for doc in docs:
        # only select dates in 2020
        rec_date = doc.get('info', None).get('date', None)
        if isinstance(rec_date, str):
            if rec_date.startswith('2020'):
                iphone_model = doc.get('user', None).get('deviceModelIdentifier', None)
                if not iphone_model:
                    iphone_model_count['iphone_model'] += 1
424/42:
    for doc in docs:
        if doc is not None:
            # only select dates in 2020
            rec_date = doc.get('info', None).get('date', None)
            if isinstance(rec_date, str):
                if rec_date.startswith('2020'):
                    iphone_model = doc.get('user', None).get('deviceModelIdentifier', None)
                    if not iphone_model:
                        iphone_model_count['iphone_model'] += 1
424/43:
    for doc in docs:
        if doc is not None:
            # only select dates in 2020
            rec_date = doc.get('info', None).get('date', None)
            if isinstance(rec_date, str):
                if rec_date.startswith('2020'):
                    iphone_model = doc.get('user', None).get('deviceModelIdentifier', None)
                    if iphone_model is not None:
                        iphone_model_count['iphone_model'] += 1
424/44: len(docs)
424/45:
    for doc in docs:
        if isinstance(doc, dict):
            # only select dates in 2020
            rec_date = doc.get('info', None).get('date', None)
            if isinstance(rec_date, str):
                if rec_date.startswith('2020'):
                    iphone_model = doc.get('user', None).get('deviceModelIdentifier', None)
                    if iphone_model is not None:
                        iphone_model_count['iphone_model'] += 1
424/46: doc
424/47: del doc
424/48:
    for doc in docs:
        if isinstance(doc, dict):
            # only select dates in 2020
            rec_date = doc.get('info', None).get('date', None)
            if isinstance(rec_date, str):
                if rec_date.startswith('2020'):
                    iphone_model = doc.get('user', None).get('deviceModelIdentifier', None)
                    if iphone_model is not None:
                        iphone_model_count['iphone_model'] += 1
424/49:
    for doc in docs:
        if isinstance(doc, dict):
            # only select dates in 2020
            rec_date = doc.get('info', {}).get('date', None)
            if isinstance(rec_date, str):
                if rec_date.startswith('2020'):
                    iphone_model = doc.get('user', {}).get('deviceModelIdentifier', None)
                    if iphone_model is not None:
                        iphone_model_count['iphone_model'] += 1
424/50: iphone_model_count
424/51:
    for doc in docs:
        if isinstance(doc, dict):
            # only select dates in 2020
            rec_date = doc.get('info', {}).get('date', None)
            if isinstance(rec_date, str):
                if rec_date.startswith('2020'):
                    iphone_model = doc.get('user', {}).get('deviceModelIdentifier', None)
                    if iphone_model is not None:
                        iphone_model_count[iphone_model] += 1
424/52:     iphone_model_count = Counter()
424/53: iphone_model_count
424/54:
    for doc in docs:
        if isinstance(doc, dict):
            # only select dates in 2020
            rec_date = doc.get('info', {}).get('date', None)
            if isinstance(rec_date, str):
                if rec_date.startswith('2020'):
                    iphone_model = doc.get('user', {}).get('deviceModelIdentifier', None)
                    if iphone_model is not None:
                        iphone_model_count[iphone_model] += 1
424/55: iphone_model_count
424/56: 'iPad8,2'.split(',')
424/57:     iphone_model_count = Counter()
424/58:
    iphone_model_count = Counter()
    for doc in docs:
        if isinstance(doc, dict):
            # only select dates in 2020
            rec_date = doc.get('info', {}).get('date', None)
            if isinstance(rec_date, str):
                if rec_date.startswith('2020'):
                    iphone_model = doc.get('user', {}).get('deviceModelIdentifier', None)
                    # iphone_model has the formate 'iPad8,2', so splitting off second half
                    iphone_model = iphone_model.split(',')[0]
                    if iphone_model is not None:
                        iphone_model_count[iphone_model] += 1
424/59: iphone_model_count
424/60: sum(iphone_model_count.values())
424/61:
    iphone_model_count = Counter()
    for doc in docs:
        # only select dates in 2020
        rec_date = doc.get('info', {}).get('date', None)
        if isinstance(rec_date, str):
            if rec_date.startswith('2020'):
                iphone_model = doc.get('user', {}).get('deviceModelIdentifier', None)
                # iphone_model has the formate 'iPad8,2', so splitting off second half
                iphone_model = iphone_model.split(',')[0]
                if iphone_model is not None:
                    iphone_model_count[iphone_model] += 1
424/62:
    iphone_model_count = Counter()
    for doc in docs:
        # only select dates in 2020
        rec_date = doc.get('info', {}).get('date', None)
        if rec_date.startswith('2020'):
            iphone_model = doc.get('user', {}).get('deviceModelIdentifier', None)
            # iphone_model has the formate 'iPad8,2', so splitting off second half
            iphone_model = iphone_model.split(',')[0]
            if iphone_model is not None:
                iphone_model_count[iphone_model] += 1
424/63: from speech.utils.io import write_pickle
424/64: help(write_pickle)
424/65: from speech.utils.io import read_pickle
424/66: ls
424/67: cd examples/
424/68: count = read_pickle("iphone_model_count.pickle")
424/69: count
424/70: count = read_pickle("iphone_model_count.pickle")
424/71: count
424/72: sum(count.values())
424/73: sum(count.values()) > 100000
424/74: count = read_pickle("iphone_model_count.pickle")
424/75: count
424/76: count = read_pickle("iphone_model_count.pickle")
424/77: count
424/78: import matplotlib.pyplot as plt
424/79: count.values()
424/80: plt.hist(count.keys(), count.values())
424/81: plt.hist(count.values(), count.keys())
424/82: plt.hist(list(count.values()), list(count.keys()))
424/83: plt.hist(list(count.values()), sorted(list(count.keys())))
424/84: plt.plot(list(count.values()), list(count.keys()))
424/85: counter.most_common()
424/86: count.most_common()
424/87: len(count.most_common())
424/88: len(count)
424/89: count.most_common().values()
424/90: *zip(count.most_common())
424/91: list(zip(count.most_common()))
424/92: count.most_common()
424/93: unzip = [key, val for key, val in count.most_common()]
424/94: unzip = [key, val for key val in count.most_common()]
424/95: unzip = [key for key, val in count.most_common()]
424/96: unzip
424/97: unzip = [val for key, val in count.most_common()]
424/98: unzip
424/99: unzip = list(zip(*count.most_common()))
424/100: unzip
424/101: plt.plot(unzip[1], unzip[0])
424/102: plt.plot(unzip[0], unzip[1])
424/103: help(plt.xticks)
424/104: plt.plot(unzip[0], unzip[1])
424/105: plt.xticks(rotation=30)
424/106: plt.xticks(unzip[0], unzip[0], rotation=30)
424/107: model_names, model_counts = list(zip(*count.most_common()))
424/108: model_names
424/109: unzip[0]
424/110: plt.xticks(unzip[0], unzip[0], rotation=90)
424/111: plt.xticks(unzip[0], unzip[0], rotation=45)
424/112: fig, ax = plt.subplots(constrained_layout=True)
424/113:     model_names, model_counts = list(zip(*iphone_model_count.most_common()))
424/114: ax.plot(model_names, model_counts)
424/115: plt.xticks(rotation=45)
424/116: plt.xticks(model_names, model_names, rotation=45)
424/117: model_counts
424/118: count.most_common()
424/119: iphone_model_count = count
424/120:     model_names, model_counts = list(zip(*iphone_model_count.most_common()))
424/121: fig, ax = plt.subplots(constrained_layout=True)
424/122: ax.plot(model_names, model_counts)
424/123:     plt.xticks(model_names, model_names, rotation=45)
424/124:
def agg2percent(x, total):
    return x/total
424/125: total = sum(model_counts)
424/126: total
424/127:
def agg2percent_forward(x, total):
    return x/total
424/128:
def agg2percent_backward(x, total):
    return x/total
424/129: from functools import partial
424/130: forward_transform = partial(agg2percent_forward, total=total)
424/131: backward_transform = partial(agg2percent_backward, total=total)
424/132:
secax = ax.secondary_yaxis('right', functions=(forward_transform,
                                               backward_transform))
424/133:
def agg2percent_forward(x, total):
    return x/total * 100
424/134:
def agg2percent_backward(x, total):
    return x/total * 100
424/135: forward_transform = partial(agg2percent_forward, total=total)
424/136: backward_transform = partial(agg2percent_backward, total=total)
424/137:
secax = ax.secondary_yaxis('right', functions=(forward_transform,
                                               backward_transform))
424/138: import matplotlib.ticker as mtick
424/139: ax.yaxis.set_major_formatter(mtick.PercentFormatter())
424/140: ax.yaxis.set_minor_formatter(mtick.PercentFormatter())
424/141: fig, ax = plt.subplots(constrained_layout=True)
424/142: ax.plot(model_names, model_counts)
424/143:     plt.xticks(model_names, model_names, rotation=45)
424/144:
secax = ax.secondary_yaxis('right', functions=(forward_transform,
                                               backward_transform
                                               ))
424/145: vals = secax.get_yticks()
425/1: '{:,}'.format(100000)
425/2: '{:%}'.format(100000)
425/3: '{:%}'.format(0.25)
425/4: '{%}'.format(0.25)
425/5: '{:%}'.format(0.25)
425/6: '{:%}'.format(25)
425/7: '{:%}'.format(0.25)
425/8: round('{:%}'.format(0.25))
425/9: '{:%}'.format(0.25).replace('0', '')
425/10: '{:%}'.format(0.25).replace('0', '').replace('.','')
424/146: secax.set_yticklabels(['{:%}'.format(x) for x in vals])
424/147:
    fig, ax = plt.subplots(constrained_layout=True)
    ax.plot(model_names, model_counts)
    plt.xticks(model_names, model_names, rotation=45)
    total = sum(model_counts)
424/148:
    def _agg2percent_forward(x, total):
        return x/total

    def _agg2percent_backward(x, total):
        return x*total
424/149:
    forward_transform = partial(_agg2percent_forward, total=total)
    backward_transform = partial(_agg2percent_backward, total=total)
424/150:
    secaxy = ax.secondary_yaxis('right', functions=(forward_transform,
                                                    backward_transform))
424/151: plt.y_label("Device model count")
424/152: ax.set_ylabel("Device model count")
424/153: secaxy.set_ylabel("Percent of total device count")
424/154: plt.set_xlabel("Device names")
424/155: plt.xlabel("Device names")
424/156:
    fig, ax = plt.subplots()
    ax.bar(model_names, model_counts)
    plt.xticks(model_names, model_names, rotation=45)
    total = sum(model_counts)
    
    # plot the aggregate and percent of total values on both axes
    def _agg2percent_forward(x, total):
        return x/total

    def _agg2percent_backward(x, total):
        return x*total

    # create the forward and backward transforms for the axis
    forward_transform = partial(_agg2percent_forward, total=total)
    backward_transform = partial(_agg2percent_backward, total=total)
    # create the secondary axis
    secaxy = ax.secondary_yaxis('right', functions=(forward_transform,
                                                    backward_transform))

    # add the plot labels for each axis
    ax.set_ylabel("Device model count")
    secaxy.set_ylabel("Percent of total device count")
    plt.xlabel("Device names")
424/157:
    fig, ax = plt.subplots(constrained_layout=True)
    ax.bar(model_names, model_counts)
    plt.xticks(model_names, model_names, rotation=45)
    total = sum(model_counts)
    
    # plot the aggregate and percent of total values on both axes
    def _agg2percent_forward(x, total):
        return x/total

    def _agg2percent_backward(x, total):
        return x*total

    # create the forward and backward transforms for the axis
    forward_transform = partial(_agg2percent_forward, total=total)
    backward_transform = partial(_agg2percent_backward, total=total)
    # create the secondary axis
    secaxy = ax.secondary_yaxis('right', functions=(forward_transform,
                                                    backward_transform))

    # add the plot labels for each axis
    ax.set_ylabel("Device model count")
    secaxy.set_ylabel("Percent of total device count")
    plt.xlabel("Device names")
424/158: xit
   1: %history -g -f ipython_history.txt
