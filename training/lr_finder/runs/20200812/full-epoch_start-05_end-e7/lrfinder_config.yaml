seed: 2017
save_path: /home/dzubke/awni_speech/speech-lfs/training/lr_finder/runs/20200812/
data:
    train_set: /mnt/disks/data_disk/home/dzubke/awni_speech/data/lib-ted-cv/train_lib-ted-cv-v5_2020-07-17_dd.json
    start_and_end: False
    num_workers: 4
logger:
    use_log: False
    log_file: /home/dzubke/awni_speech/speech-lfs/training/lr_finder/2020-08-05-08-10_native-loss_v100.log
preproc:
    preprocessor: log_spectrogram
    window_size: 32
    step_size: 16
    use_feature_normalize: True
    augment_from_normal: False
    tempo_gain_pitch_perturb: False
    tempo_gain_pitch_prob: 0.5
    tempo_range: [0.80, 1.20]
    gain_range: [-5, 5]
    pitch_range: [-250, 250]
    synthetic_gaussian_noise: False
    gauss_noise_prob: 0.5
    gauss_snr_db_range: [15, 50]
    background_noise: False
    background_noise_dir: /mnt/disks/data_disk/home/dzubke/awni_speech/data/noise/feed_to_model/
    background_noise_prob: 0.5
    background_noise_range: [0.0, 0.4]
    spec_augment: False
    spec_augment_prob: 0.5
    spec_augment_policy: 
        0: {W: 20, F: 30, m_F: 1, T: 18, m_T: 1}
        1: {W: 20, F: 15, m_F: 2, T: 9, m_T: 2} 
        2: {W: 20, F: 10, m_F: 3, T: 6, m_T: 3} 
optimizer: 
    max_iterations: 999999999999
    batch_size: 8
    run_state: [0,0]
    best_so_far: 999
    start_epoch: 0
    epochs: 1
    start_lr: 0.5
    end_lr: 0.0000001
model: 
    class: this value is no longer used as train.py is hardcorded to use CTC_train
    multi_gpu: False
    dropout: 0.4
    remove_layers: []
    encoder: 
        conv:
            - [32, 11, 41, 1, 2, 0, 20]
            - [32, 11, 21, 1, 2, 0, 10]
            - [96, 11, 21, 1, 1, 0, 10]
        rnn: 
            type: LSTM
            dim: 512
            bidirectional: False
            layers: 5
